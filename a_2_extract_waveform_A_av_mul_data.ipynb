{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== Dataset Class ==========\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, list_file, wav_dir, label_dir, max_seq_len=250, resample_sr=2000):\n",
    "        self.video_names = self.read_video_list(list_file)\n",
    "        self.wav_dir = Path(wav_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.resample_sr = resample_sr\n",
    "\n",
    "        self.label_files = self.prefetch_label_files()\n",
    "        self.data_size, self.index_to_chunk, self.labels = self.prefetch_and_index()\n",
    "\n",
    "    def read_video_list(self, list_path):\n",
    "        with open(list_path, 'r') as f:\n",
    "            return [line.strip() for line in f]\n",
    "\n",
    "    def prefetch_label_files(self):\n",
    "        name_set = set(self.video_names)\n",
    "        label_files = defaultdict(list)\n",
    "\n",
    "        for label_file in self.label_dir.glob(\"**/*.json\"):\n",
    "            file_name = label_file.stem\n",
    "            annotator_id_index = len(file_name) - file_name[::-1].find(\"_\") - 1\n",
    "            video_name = file_name[:annotator_id_index]\n",
    "\n",
    "            if video_name in name_set:\n",
    "                label_files[video_name].append(label_file)\n",
    "\n",
    "        return label_files\n",
    "\n",
    "    def extract_label(self, video_name):\n",
    "        label_files = self.label_files.get(video_name, [])\n",
    "        labels = []\n",
    "\n",
    "        for label_file in label_files:\n",
    "            with open(label_file, \"r\") as rf:\n",
    "                data = json.load(rf)\n",
    "            video_length = math.ceil(data[\"metadata\"][\"length\"])\n",
    "            annotator_label = np.zeros(video_length)\n",
    "\n",
    "            for timeline in data[\"timelines\"]:\n",
    "                for t in range(timeline[\"start\"], timeline[\"end\"] + 1):\n",
    "                    if t < video_length:\n",
    "                        annotator_label[t] = 1\n",
    "\n",
    "            labels.append(annotator_label)\n",
    "\n",
    "        return np.array(labels)\n",
    "\n",
    "    def prefetch_and_index(self):\n",
    "        index = 0\n",
    "        index_to_chunk = {}\n",
    "        all_labels = {}\n",
    "\n",
    "        for video_name in self.video_names:\n",
    "            labels = self.extract_label(video_name)\n",
    "            if labels is None or len(labels) == 0 or len(labels[0]) == 0:\n",
    "                print(f\"âš ï¸ Skipping {video_name}: no valid labels\")\n",
    "                continue\n",
    "\n",
    "            all_labels[video_name] = labels\n",
    "            chunk_count = math.ceil(len(labels[0]) / self.max_seq_len)\n",
    "\n",
    "            for chunk_index in range(chunk_count):\n",
    "                index_to_chunk[index + chunk_index] = (video_name, chunk_index)\n",
    "\n",
    "            index += chunk_count\n",
    "\n",
    "        return index, index_to_chunk, all_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_name, chunk_index = self.index_to_chunk[index]\n",
    "        start = chunk_index * self.max_seq_len\n",
    "        end = start + self.max_seq_len\n",
    "\n",
    "        labels = self.labels[video_name][:, start:end]\n",
    "\n",
    "        try:\n",
    "            wav_path = self.wav_dir / f\"{video_name}.wav\"\n",
    "            audio_data, sr = torchaudio.load(str(wav_path))\n",
    "        except:\n",
    "            print(f\"ðŸš« Error loading: {wav_path}\")\n",
    "            return None\n",
    "\n",
    "        resampler = T.Resample(sr, self.resample_sr, dtype=audio_data.dtype)\n",
    "        audio_data = resampler(audio_data)\n",
    "        audio_data = torch.mean(audio_data, axis=0).numpy()\n",
    "\n",
    "        # Crop or pad the audio\n",
    "        audio_data = audio_data[start * self.resample_sr : end * self.resample_sr]\n",
    "\n",
    "        total_segments = self.max_seq_len\n",
    "        num_frames_per_segment = len(audio_data) // total_segments\n",
    "        audio_list = []\n",
    "\n",
    "        for i in range(0, len(audio_data) - num_frames_per_segment + 1, num_frames_per_segment):\n",
    "            segment = audio_data[i : i + num_frames_per_segment]\n",
    "\n",
    "            if len(segment) < self.resample_sr:\n",
    "                pad = self.resample_sr - len(segment)\n",
    "                segment = np.pad(segment, (0, pad), mode=\"constant\")\n",
    "            elif len(segment) > self.resample_sr:\n",
    "                segment = segment[:self.resample_sr]\n",
    "\n",
    "            audio_list.append(segment)\n",
    "\n",
    "        audio_array = np.vstack(audio_list)\n",
    "\n",
    "        # Convert labels\n",
    "        labels = torch.from_numpy(labels).squeeze(0)\n",
    "        labels = torch.sum(labels, dim=0)\n",
    "        labels = torch.min(labels, torch.ones(labels.shape[0], device=labels.device))\n",
    "\n",
    "        return video_name, audio_array, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Train dataset size: 401\n",
      "ðŸŽ¬ AV Test dataset size: 140\n",
      "ðŸŽ¬ MUL Test dataset size: 163\n"
     ]
    }
   ],
   "source": [
    "# ========== Dataset ==========\n",
    "sd_train_av = SummaryDataset(\n",
    "    list_file=\"/home/jovyan/EmotionDetection/video_data/av_train.txt\",\n",
    "    wav_dir=\"/home/jovyan/EmotionDetection/audio_data/av_train\",\n",
    "    label_dir=\"/home/jovyan/EmotionDetection/video_data/label\"\n",
    ")\n",
    "\n",
    "sd_test_av = SummaryDataset(\n",
    "    list_file=\"/home/jovyan/EmotionDetection/video_data/av_test.txt\",\n",
    "    wav_dir=\"/home/jovyan/EmotionDetection/audio_data/av_test\",\n",
    "    label_dir=\"/home/jovyan/EmotionDetection/video_data/label\"\n",
    ")\n",
    "\n",
    "sd_test_mul = SummaryDataset(\n",
    "    list_file=\"/home/jovyan/EmotionDetection/video_data/mul_test.txt\",\n",
    "    wav_dir=\"/home/jovyan/EmotionDetection/audio_data/mul_test\",\n",
    "    label_dir=\"/home/jovyan/EmotionDetection/video_data/label\"\n",
    ")\n",
    "\n",
    "# ========== DataLoader ==========\n",
    "# Custom collate function to skip None and unpack correctly\n",
    "def safe_collate(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return tuple(zip(*batch))  # returns (video_names, inputs, labels)\n",
    "\n",
    "dl_train_av = DataLoader(\n",
    "    sd_train_av,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=safe_collate)\n",
    "\n",
    "dl_test_av = DataLoader(\n",
    "    sd_test_av,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=safe_collate)\n",
    "\n",
    "dl_test_mul = DataLoader(\n",
    "    sd_test_mul,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=safe_collate)\n",
    "\n",
    "# ========== Info ==========\n",
    "print(f\"ðŸ“¦ Train dataset size: {len(sd_train_av)}\")\n",
    "print(f\"ðŸŽ¬ AV Test dataset size: {len(sd_test_av)}\")\n",
    "print(f\"ðŸŽ¬ MUL Test dataset size: {len(sd_test_mul)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load\n",
    "# Extract Emotional Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a651d16657414e589bd45b520a45a2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting raw waveforms:   0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 401 waveforms to waveform_without_emotion_av_train.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "all_waveforms = []\n",
    "\n",
    "for batch in tqdm(dl_train_av, desc=\"Extracting raw waveforms\"):\n",
    "    if batch is None:\n",
    "        continue\n",
    "\n",
    "    video_names, inputs, labels = batch  # inputs: (1, [max_seq_len, resample_sr])\n",
    "    audio_array = inputs[0]  # (max_seq_len, resample_sr)\n",
    "    all_waveforms.append(audio_array)\n",
    "\n",
    "# Convert to numpy array with object dtype (due to potential varying lengths)\n",
    "waveform_np = np.array(all_waveforms, dtype=object)\n",
    "\n",
    "# save\n",
    "save_path = \"Features/waveform_without_emotion_av_train.npy\"\n",
    "np.save(save_path, waveform_np)\n",
    "print(f\"âœ… Saved {len(all_waveforms)} waveforms to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bba63fd0cf4443da53d2261271c0fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting raw waveforms:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 140 waveforms to waveform_without_emotion_av_val.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "all_waveforms = []\n",
    "\n",
    "for batch in tqdm(dl_test_av, desc=\"Extracting raw waveforms\"):\n",
    "    if batch is None:\n",
    "        continue\n",
    "\n",
    "    video_names, inputs, labels = batch  # inputs: (1, [max_seq_len, resample_sr])\n",
    "    audio_array = inputs[0]  # (max_seq_len, resample_sr)\n",
    "    all_waveforms.append(audio_array)\n",
    "\n",
    "# Convert to numpy array with object dtype (due to potential varying lengths)\n",
    "waveform_np = np.array(all_waveforms, dtype=object)\n",
    "\n",
    "# save\n",
    "save_path = \"Features/waveform_without_emotion_av_val.npy\"\n",
    "np.save(save_path, waveform_np)\n",
    "print(f\"âœ… Saved {len(all_waveforms)} waveforms to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9398e504da54f4f90f1b3b6e646e84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting raw waveforms:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 163 waveforms to waveform_without_emotion_mul_val.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "all_waveforms = []\n",
    "\n",
    "for batch in tqdm(dl_test_mul, desc=\"Extracting raw waveforms\"):\n",
    "    if batch is None:\n",
    "        continue\n",
    "\n",
    "    video_names, inputs, labels = batch  # inputs: (1, [max_seq_len, resample_sr])\n",
    "    audio_array = inputs[0]  # (max_seq_len, resample_sr)\n",
    "    all_waveforms.append(audio_array)\n",
    "\n",
    "# Convert to numpy array with object dtype (due to potential varying lengths)\n",
    "waveform_np = np.array(all_waveforms, dtype=object)\n",
    "\n",
    "# save\n",
    "save_path = \"Features/waveform_without_emotion_mul_val.npy\"\n",
    "np.save(save_path, waveform_np)\n",
    "print(f\"âœ… Saved {len(all_waveforms)} waveforms to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
