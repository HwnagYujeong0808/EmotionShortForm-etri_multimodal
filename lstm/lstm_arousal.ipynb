{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/EmotionShortForm\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/EmotionShortForm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav 파일의 MFCC Feature와 상태정보를 합친 학습데이터를 불러옵니다.\n",
    "train_df = pd.read_csv('preprocessing_csv/train_mfcc_data.csv')\n",
    "test_df = pd.read_csv('preprocessing_csv/test_mfcc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SegmentId</th>\n",
       "      <th>time</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_23</th>\n",
       "      <th>mfcc_24</th>\n",
       "      <th>mfcc_25</th>\n",
       "      <th>mfcc_26</th>\n",
       "      <th>mfcc_27</th>\n",
       "      <th>mfcc_28</th>\n",
       "      <th>mfcc_29</th>\n",
       "      <th>mfcc_30</th>\n",
       "      <th>mfcc_31</th>\n",
       "      <th>mfcc_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11164</td>\n",
       "      <td>Sess26_script04_User052F_010</td>\n",
       "      <td>7.785990</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-379.964142</td>\n",
       "      <td>109.689522</td>\n",
       "      <td>8.409287</td>\n",
       "      <td>14.947894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390324</td>\n",
       "      <td>2.626545</td>\n",
       "      <td>0.437545</td>\n",
       "      <td>2.089825</td>\n",
       "      <td>0.171152</td>\n",
       "      <td>5.063381</td>\n",
       "      <td>3.834727</td>\n",
       "      <td>1.973997</td>\n",
       "      <td>-3.659752</td>\n",
       "      <td>-1.247434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13031</td>\n",
       "      <td>Sess24_script03_User047F_009</td>\n",
       "      <td>12.188990</td>\n",
       "      <td>happy</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-316.165100</td>\n",
       "      <td>96.198570</td>\n",
       "      <td>-13.874292</td>\n",
       "      <td>23.989883</td>\n",
       "      <td>...</td>\n",
       "      <td>2.732685</td>\n",
       "      <td>5.995924</td>\n",
       "      <td>5.040682</td>\n",
       "      <td>8.586355</td>\n",
       "      <td>6.141405</td>\n",
       "      <td>8.720468</td>\n",
       "      <td>8.573661</td>\n",
       "      <td>10.320451</td>\n",
       "      <td>7.674820</td>\n",
       "      <td>4.114799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1147</td>\n",
       "      <td>Sess09_script01_User018M_030</td>\n",
       "      <td>4.147000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-354.932220</td>\n",
       "      <td>124.622856</td>\n",
       "      <td>13.342740</td>\n",
       "      <td>26.263084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242917</td>\n",
       "      <td>0.397124</td>\n",
       "      <td>4.878927</td>\n",
       "      <td>5.264405</td>\n",
       "      <td>2.994150</td>\n",
       "      <td>2.870259</td>\n",
       "      <td>0.314687</td>\n",
       "      <td>2.809367</td>\n",
       "      <td>-0.628893</td>\n",
       "      <td>3.489642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6435</td>\n",
       "      <td>Sess14_script02_User028M_002</td>\n",
       "      <td>1.556999</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-385.279846</td>\n",
       "      <td>118.912277</td>\n",
       "      <td>13.351930</td>\n",
       "      <td>38.183296</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936629</td>\n",
       "      <td>-0.579203</td>\n",
       "      <td>-1.418086</td>\n",
       "      <td>2.701938</td>\n",
       "      <td>-0.128480</td>\n",
       "      <td>-0.650981</td>\n",
       "      <td>0.222955</td>\n",
       "      <td>1.449542</td>\n",
       "      <td>-1.601669</td>\n",
       "      <td>-0.025312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7053</td>\n",
       "      <td>Sess32_script01_User064F_049</td>\n",
       "      <td>5.443000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-351.163513</td>\n",
       "      <td>109.502029</td>\n",
       "      <td>4.125424</td>\n",
       "      <td>35.244442</td>\n",
       "      <td>...</td>\n",
       "      <td>3.420357</td>\n",
       "      <td>6.290679</td>\n",
       "      <td>3.681709</td>\n",
       "      <td>5.164492</td>\n",
       "      <td>3.685917</td>\n",
       "      <td>7.506263</td>\n",
       "      <td>7.469996</td>\n",
       "      <td>7.370858</td>\n",
       "      <td>6.551222</td>\n",
       "      <td>5.917849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10764</th>\n",
       "      <td>5191</td>\n",
       "      <td>Sess03_script03_User005M_032</td>\n",
       "      <td>3.847000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-307.079376</td>\n",
       "      <td>110.748199</td>\n",
       "      <td>3.071705</td>\n",
       "      <td>26.098019</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.210361</td>\n",
       "      <td>-1.421704</td>\n",
       "      <td>-2.658003</td>\n",
       "      <td>1.054526</td>\n",
       "      <td>-3.188072</td>\n",
       "      <td>0.515963</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.752482</td>\n",
       "      <td>-1.321549</td>\n",
       "      <td>-3.410846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10765</th>\n",
       "      <td>13418</td>\n",
       "      <td>Sess22_script06_User043F_006</td>\n",
       "      <td>5.639000</td>\n",
       "      <td>angry</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>-239.924500</td>\n",
       "      <td>72.984070</td>\n",
       "      <td>-18.580250</td>\n",
       "      <td>-6.845965</td>\n",
       "      <td>...</td>\n",
       "      <td>6.675857</td>\n",
       "      <td>10.649000</td>\n",
       "      <td>7.589522</td>\n",
       "      <td>8.607011</td>\n",
       "      <td>2.216211</td>\n",
       "      <td>7.668085</td>\n",
       "      <td>-0.767406</td>\n",
       "      <td>2.716651</td>\n",
       "      <td>-3.422506</td>\n",
       "      <td>4.849663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>5390</td>\n",
       "      <td>Sess12_script01_User081F_018</td>\n",
       "      <td>3.157000</td>\n",
       "      <td>happy;neutral</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-333.780945</td>\n",
       "      <td>95.989861</td>\n",
       "      <td>-14.832805</td>\n",
       "      <td>15.677377</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.285286</td>\n",
       "      <td>1.564119</td>\n",
       "      <td>6.515084</td>\n",
       "      <td>10.374402</td>\n",
       "      <td>8.091890</td>\n",
       "      <td>14.184700</td>\n",
       "      <td>12.203178</td>\n",
       "      <td>19.104069</td>\n",
       "      <td>6.390728</td>\n",
       "      <td>4.591437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10767</th>\n",
       "      <td>860</td>\n",
       "      <td>Sess19_script02_User037M_002</td>\n",
       "      <td>6.590000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-339.677338</td>\n",
       "      <td>118.469772</td>\n",
       "      <td>10.663605</td>\n",
       "      <td>37.051121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.823455</td>\n",
       "      <td>1.826434</td>\n",
       "      <td>-0.563071</td>\n",
       "      <td>3.543955</td>\n",
       "      <td>-0.973314</td>\n",
       "      <td>3.040696</td>\n",
       "      <td>3.090068</td>\n",
       "      <td>4.243995</td>\n",
       "      <td>0.834076</td>\n",
       "      <td>1.139718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10768</th>\n",
       "      <td>7270</td>\n",
       "      <td>Sess32_script04_User064F_015</td>\n",
       "      <td>4.884996</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-347.986877</td>\n",
       "      <td>119.509628</td>\n",
       "      <td>-8.292249</td>\n",
       "      <td>15.593624</td>\n",
       "      <td>...</td>\n",
       "      <td>5.915211</td>\n",
       "      <td>5.233806</td>\n",
       "      <td>6.230196</td>\n",
       "      <td>8.562304</td>\n",
       "      <td>2.252800</td>\n",
       "      <td>5.576434</td>\n",
       "      <td>2.432891</td>\n",
       "      <td>6.054037</td>\n",
       "      <td>9.382090</td>\n",
       "      <td>3.946812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10769 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                     SegmentId       time        Emotion  \\\n",
       "0           11164  Sess26_script04_User052F_010   7.785990        neutral   \n",
       "1           13031  Sess24_script03_User047F_009  12.188990          happy   \n",
       "2            1147  Sess09_script01_User018M_030   4.147000        neutral   \n",
       "3            6435  Sess14_script02_User028M_002   1.556999        neutral   \n",
       "4            7053  Sess32_script01_User064F_049   5.443000        neutral   \n",
       "...           ...                           ...        ...            ...   \n",
       "10764        5191  Sess03_script03_User005M_032   3.847000        neutral   \n",
       "10765       13418  Sess22_script06_User043F_006   5.639000          angry   \n",
       "10766        5390  Sess12_script01_User081F_018   3.157000  happy;neutral   \n",
       "10767         860  Sess19_script02_User037M_002   6.590000        neutral   \n",
       "10768        7270  Sess32_script04_User064F_015   4.884996        neutral   \n",
       "\n",
       "       Valence  Arousal      mfcc_1      mfcc_2     mfcc_3     mfcc_4  ...  \\\n",
       "0          3.0      3.5 -379.964142  109.689522   8.409287  14.947894  ...   \n",
       "1          3.4      3.6 -316.165100   96.198570 -13.874292  23.989883  ...   \n",
       "2          2.6      3.1 -354.932220  124.622856  13.342740  26.263084  ...   \n",
       "3          3.6      3.1 -385.279846  118.912277  13.351930  38.183296  ...   \n",
       "4          3.0      3.5 -351.163513  109.502029   4.125424  35.244442  ...   \n",
       "...        ...      ...         ...         ...        ...        ...  ...   \n",
       "10764      3.2      3.3 -307.079376  110.748199   3.071705  26.098019  ...   \n",
       "10765      2.6      3.7 -239.924500   72.984070 -18.580250  -6.845965  ...   \n",
       "10766      3.7      3.6 -333.780945   95.989861 -14.832805  15.677377  ...   \n",
       "10767      2.6      3.3 -339.677338  118.469772  10.663605  37.051121  ...   \n",
       "10768      3.1      3.5 -347.986877  119.509628  -8.292249  15.593624  ...   \n",
       "\n",
       "        mfcc_23    mfcc_24   mfcc_25    mfcc_26   mfcc_27    mfcc_28  \\\n",
       "0     -0.390324   2.626545  0.437545   2.089825  0.171152   5.063381   \n",
       "1      2.732685   5.995924  5.040682   8.586355  6.141405   8.720468   \n",
       "2     -0.242917   0.397124  4.878927   5.264405  2.994150   2.870259   \n",
       "3      2.936629  -0.579203 -1.418086   2.701938 -0.128480  -0.650981   \n",
       "4      3.420357   6.290679  3.681709   5.164492  3.685917   7.506263   \n",
       "...         ...        ...       ...        ...       ...        ...   \n",
       "10764 -4.210361  -1.421704 -2.658003   1.054526 -3.188072   0.515963   \n",
       "10765  6.675857  10.649000  7.589522   8.607011  2.216211   7.668085   \n",
       "10766 -3.285286   1.564119  6.515084  10.374402  8.091890  14.184700   \n",
       "10767 -0.823455   1.826434 -0.563071   3.543955 -0.973314   3.040696   \n",
       "10768  5.915211   5.233806  6.230196   8.562304  2.252800   5.576434   \n",
       "\n",
       "         mfcc_29    mfcc_30   mfcc_31   mfcc_32  \n",
       "0       3.834727   1.973997 -3.659752 -1.247434  \n",
       "1       8.573661  10.320451  7.674820  4.114799  \n",
       "2       0.314687   2.809367 -0.628893  3.489642  \n",
       "3       0.222955   1.449542 -1.601669 -0.025312  \n",
       "4       7.469996   7.370858  6.551222  5.917849  \n",
       "...          ...        ...       ...       ...  \n",
       "10764   0.066960   0.752482 -1.321549 -3.410846  \n",
       "10765  -0.767406   2.716651 -3.422506  4.849663  \n",
       "10766  12.203178  19.104069  6.390728  4.591437  \n",
       "10767   3.090068   4.243995  0.834076  1.139718  \n",
       "10768   2.432891   6.054037  9.382090  3.946812  \n",
       "\n",
       "[10769 rows x 38 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SegmentId</th>\n",
       "      <th>time</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_23</th>\n",
       "      <th>mfcc_24</th>\n",
       "      <th>mfcc_25</th>\n",
       "      <th>mfcc_26</th>\n",
       "      <th>mfcc_27</th>\n",
       "      <th>mfcc_28</th>\n",
       "      <th>mfcc_29</th>\n",
       "      <th>mfcc_30</th>\n",
       "      <th>mfcc_31</th>\n",
       "      <th>mfcc_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>Sess13_script01_User026F_029</td>\n",
       "      <td>7.254990</td>\n",
       "      <td>happy</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-336.985138</td>\n",
       "      <td>106.803337</td>\n",
       "      <td>4.534938</td>\n",
       "      <td>10.542789</td>\n",
       "      <td>...</td>\n",
       "      <td>3.775362</td>\n",
       "      <td>8.967966</td>\n",
       "      <td>8.074034</td>\n",
       "      <td>11.653221</td>\n",
       "      <td>11.598641</td>\n",
       "      <td>8.817598</td>\n",
       "      <td>9.400865</td>\n",
       "      <td>8.987887</td>\n",
       "      <td>5.992146</td>\n",
       "      <td>4.622094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5163</td>\n",
       "      <td>Sess03_script03_User005M_021</td>\n",
       "      <td>2.212000</td>\n",
       "      <td>surprise</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-337.904297</td>\n",
       "      <td>105.446899</td>\n",
       "      <td>11.310282</td>\n",
       "      <td>24.076952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827898</td>\n",
       "      <td>-4.050204</td>\n",
       "      <td>-3.444622</td>\n",
       "      <td>-0.045880</td>\n",
       "      <td>-2.624972</td>\n",
       "      <td>-0.375320</td>\n",
       "      <td>-1.709600</td>\n",
       "      <td>-1.171931</td>\n",
       "      <td>-4.099090</td>\n",
       "      <td>-1.587989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7844</td>\n",
       "      <td>Sess23_script02_User046M_022</td>\n",
       "      <td>9.281020</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-419.555511</td>\n",
       "      <td>125.555763</td>\n",
       "      <td>18.391697</td>\n",
       "      <td>30.752876</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.389714</td>\n",
       "      <td>-3.478583</td>\n",
       "      <td>-2.749439</td>\n",
       "      <td>1.098240</td>\n",
       "      <td>1.010692</td>\n",
       "      <td>0.460480</td>\n",
       "      <td>-1.992547</td>\n",
       "      <td>1.084825</td>\n",
       "      <td>-0.576132</td>\n",
       "      <td>0.662649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304</td>\n",
       "      <td>Sess13_script05_User026F_017</td>\n",
       "      <td>6.614991</td>\n",
       "      <td>happy</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>-261.221222</td>\n",
       "      <td>108.892136</td>\n",
       "      <td>-26.196730</td>\n",
       "      <td>9.421447</td>\n",
       "      <td>...</td>\n",
       "      <td>7.680419</td>\n",
       "      <td>13.833956</td>\n",
       "      <td>7.379666</td>\n",
       "      <td>8.362466</td>\n",
       "      <td>1.706965</td>\n",
       "      <td>5.596114</td>\n",
       "      <td>5.339090</td>\n",
       "      <td>5.811155</td>\n",
       "      <td>1.621077</td>\n",
       "      <td>3.745970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5216</td>\n",
       "      <td>Sess03_script04_User005M_008</td>\n",
       "      <td>3.404998</td>\n",
       "      <td>angry</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-346.095032</td>\n",
       "      <td>101.206184</td>\n",
       "      <td>21.618317</td>\n",
       "      <td>20.990105</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.431287</td>\n",
       "      <td>-0.532429</td>\n",
       "      <td>-1.102016</td>\n",
       "      <td>1.010110</td>\n",
       "      <td>-1.844896</td>\n",
       "      <td>-1.788239</td>\n",
       "      <td>0.140551</td>\n",
       "      <td>-2.089751</td>\n",
       "      <td>-1.204731</td>\n",
       "      <td>-1.450962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>8873</td>\n",
       "      <td>Sess39_script05_User078F_007</td>\n",
       "      <td>4.404998</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-452.831024</td>\n",
       "      <td>80.856758</td>\n",
       "      <td>19.427954</td>\n",
       "      <td>32.913322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.777822</td>\n",
       "      <td>1.763288</td>\n",
       "      <td>2.534321</td>\n",
       "      <td>1.811994</td>\n",
       "      <td>4.191569</td>\n",
       "      <td>6.094894</td>\n",
       "      <td>5.072481</td>\n",
       "      <td>7.664765</td>\n",
       "      <td>4.959315</td>\n",
       "      <td>4.205091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>11974</td>\n",
       "      <td>Sess37_script05_User073F_012</td>\n",
       "      <td>6.781000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-450.678375</td>\n",
       "      <td>98.769257</td>\n",
       "      <td>-4.727075</td>\n",
       "      <td>7.584974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.527370</td>\n",
       "      <td>3.983363</td>\n",
       "      <td>4.685560</td>\n",
       "      <td>2.902997</td>\n",
       "      <td>0.235584</td>\n",
       "      <td>2.744654</td>\n",
       "      <td>1.443039</td>\n",
       "      <td>5.997917</td>\n",
       "      <td>3.257085</td>\n",
       "      <td>5.762399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>10559</td>\n",
       "      <td>Sess33_script04_User065M_028</td>\n",
       "      <td>3.066020</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-371.517609</td>\n",
       "      <td>117.556679</td>\n",
       "      <td>5.498871</td>\n",
       "      <td>27.535383</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.194983</td>\n",
       "      <td>-1.782557</td>\n",
       "      <td>-2.510203</td>\n",
       "      <td>-2.095202</td>\n",
       "      <td>-3.341051</td>\n",
       "      <td>-1.580801</td>\n",
       "      <td>-0.980542</td>\n",
       "      <td>0.799156</td>\n",
       "      <td>-2.367372</td>\n",
       "      <td>0.982645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>3076</td>\n",
       "      <td>Sess02_script02_User003F_001</td>\n",
       "      <td>12.123001</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>-375.577759</td>\n",
       "      <td>106.716896</td>\n",
       "      <td>2.852894</td>\n",
       "      <td>26.215828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.205008</td>\n",
       "      <td>4.127676</td>\n",
       "      <td>8.754350</td>\n",
       "      <td>5.358984</td>\n",
       "      <td>4.600071</td>\n",
       "      <td>9.529908</td>\n",
       "      <td>5.941333</td>\n",
       "      <td>14.158734</td>\n",
       "      <td>8.534500</td>\n",
       "      <td>10.391392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>11617</td>\n",
       "      <td>Sess25_script05_User050F_018</td>\n",
       "      <td>2.240010</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-469.148651</td>\n",
       "      <td>96.403061</td>\n",
       "      <td>26.530113</td>\n",
       "      <td>8.035880</td>\n",
       "      <td>...</td>\n",
       "      <td>3.305809</td>\n",
       "      <td>5.588368</td>\n",
       "      <td>4.237176</td>\n",
       "      <td>7.683784</td>\n",
       "      <td>0.811043</td>\n",
       "      <td>1.432141</td>\n",
       "      <td>1.804263</td>\n",
       "      <td>4.709819</td>\n",
       "      <td>1.102094</td>\n",
       "      <td>-0.579098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2693 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                     SegmentId       time   Emotion  Valence  \\\n",
       "0             63  Sess13_script01_User026F_029   7.254990     happy      3.7   \n",
       "1           5163  Sess03_script03_User005M_021   2.212000  surprise      2.9   \n",
       "2           7844  Sess23_script02_User046M_022   9.281020   neutral      2.3   \n",
       "3            304  Sess13_script05_User026F_017   6.614991     happy      4.4   \n",
       "4           5216  Sess03_script04_User005M_008   3.404998     angry      1.6   \n",
       "...          ...                           ...        ...       ...      ...   \n",
       "2688        8873  Sess39_script05_User078F_007   4.404998   neutral      3.0   \n",
       "2689       11974  Sess37_script05_User073F_012   6.781000   neutral      2.6   \n",
       "2690       10559  Sess33_script04_User065M_028   3.066020   neutral      3.4   \n",
       "2691        3076  Sess02_script02_User003F_001  12.123001   neutral      2.4   \n",
       "2692       11617  Sess25_script05_User050F_018   2.240010   neutral      3.2   \n",
       "\n",
       "      Arousal      mfcc_1      mfcc_2     mfcc_3     mfcc_4  ...   mfcc_23  \\\n",
       "0         3.5 -336.985138  106.803337   4.534938  10.542789  ...  3.775362   \n",
       "1         3.4 -337.904297  105.446899  11.310282  24.076952  ...  0.827898   \n",
       "2         3.0 -419.555511  125.555763  18.391697  30.752876  ... -4.389714   \n",
       "3         4.2 -261.221222  108.892136 -26.196730   9.421447  ...  7.680419   \n",
       "4         3.2 -346.095032  101.206184  21.618317  20.990105  ... -1.431287   \n",
       "...       ...         ...         ...        ...        ...  ...       ...   \n",
       "2688      3.0 -452.831024   80.856758  19.427954  32.913322  ... -0.777822   \n",
       "2689      3.2 -450.678375   98.769257  -4.727075   7.584974  ... -0.527370   \n",
       "2690      3.4 -371.517609  117.556679   5.498871  27.535383  ... -3.194983   \n",
       "2691      2.6 -375.577759  106.716896   2.852894  26.215828  ...  1.205008   \n",
       "2692      3.3 -469.148651   96.403061  26.530113   8.035880  ...  3.305809   \n",
       "\n",
       "        mfcc_24   mfcc_25    mfcc_26    mfcc_27   mfcc_28   mfcc_29  \\\n",
       "0      8.967966  8.074034  11.653221  11.598641  8.817598  9.400865   \n",
       "1     -4.050204 -3.444622  -0.045880  -2.624972 -0.375320 -1.709600   \n",
       "2     -3.478583 -2.749439   1.098240   1.010692  0.460480 -1.992547   \n",
       "3     13.833956  7.379666   8.362466   1.706965  5.596114  5.339090   \n",
       "4     -0.532429 -1.102016   1.010110  -1.844896 -1.788239  0.140551   \n",
       "...         ...       ...        ...        ...       ...       ...   \n",
       "2688   1.763288  2.534321   1.811994   4.191569  6.094894  5.072481   \n",
       "2689   3.983363  4.685560   2.902997   0.235584  2.744654  1.443039   \n",
       "2690  -1.782557 -2.510203  -2.095202  -3.341051 -1.580801 -0.980542   \n",
       "2691   4.127676  8.754350   5.358984   4.600071  9.529908  5.941333   \n",
       "2692   5.588368  4.237176   7.683784   0.811043  1.432141  1.804263   \n",
       "\n",
       "        mfcc_30   mfcc_31    mfcc_32  \n",
       "0      8.987887  5.992146   4.622094  \n",
       "1     -1.171931 -4.099090  -1.587989  \n",
       "2      1.084825 -0.576132   0.662649  \n",
       "3      5.811155  1.621077   3.745970  \n",
       "4     -2.089751 -1.204731  -1.450962  \n",
       "...         ...       ...        ...  \n",
       "2688   7.664765  4.959315   4.205091  \n",
       "2689   5.997917  3.257085   5.762399  \n",
       "2690   0.799156 -2.367372   0.982645  \n",
       "2691  14.158734  8.534500  10.391392  \n",
       "2692   4.709819  1.102094  -0.579098  \n",
       "\n",
       "[2693 rows x 38 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터를 모델의 input으로 들어갈 x와 label로 사용할 y로 분할\n",
    "train_x_df = train_df.drop(columns=['Unnamed: 0', 'SegmentId','time','Valence','Arousal','Emotion'])\n",
    "train_y_df = train_df['Arousal']\n",
    "test_x_df = test_df.drop(columns=['Unnamed: 0', 'SegmentId','time','Valence','Arousal','Emotion'])\n",
    "test_y_df = test_df['Arousal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>mfcc_8</th>\n",
       "      <th>mfcc_9</th>\n",
       "      <th>mfcc_10</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_23</th>\n",
       "      <th>mfcc_24</th>\n",
       "      <th>mfcc_25</th>\n",
       "      <th>mfcc_26</th>\n",
       "      <th>mfcc_27</th>\n",
       "      <th>mfcc_28</th>\n",
       "      <th>mfcc_29</th>\n",
       "      <th>mfcc_30</th>\n",
       "      <th>mfcc_31</th>\n",
       "      <th>mfcc_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-379.964142</td>\n",
       "      <td>109.689522</td>\n",
       "      <td>8.409287</td>\n",
       "      <td>14.947894</td>\n",
       "      <td>-10.617918</td>\n",
       "      <td>-1.278166</td>\n",
       "      <td>-9.656478</td>\n",
       "      <td>-3.804506</td>\n",
       "      <td>1.033329</td>\n",
       "      <td>2.386927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390324</td>\n",
       "      <td>2.626545</td>\n",
       "      <td>0.437545</td>\n",
       "      <td>2.089825</td>\n",
       "      <td>0.171152</td>\n",
       "      <td>5.063381</td>\n",
       "      <td>3.834727</td>\n",
       "      <td>1.973997</td>\n",
       "      <td>-3.659752</td>\n",
       "      <td>-1.247434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-316.165100</td>\n",
       "      <td>96.198570</td>\n",
       "      <td>-13.874292</td>\n",
       "      <td>23.989883</td>\n",
       "      <td>-16.052597</td>\n",
       "      <td>6.285239</td>\n",
       "      <td>-17.207815</td>\n",
       "      <td>-9.229585</td>\n",
       "      <td>-1.843399</td>\n",
       "      <td>-1.561611</td>\n",
       "      <td>...</td>\n",
       "      <td>2.732685</td>\n",
       "      <td>5.995924</td>\n",
       "      <td>5.040682</td>\n",
       "      <td>8.586355</td>\n",
       "      <td>6.141405</td>\n",
       "      <td>8.720468</td>\n",
       "      <td>8.573661</td>\n",
       "      <td>10.320451</td>\n",
       "      <td>7.674820</td>\n",
       "      <td>4.114799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-354.932220</td>\n",
       "      <td>124.622856</td>\n",
       "      <td>13.342740</td>\n",
       "      <td>26.263084</td>\n",
       "      <td>-8.358158</td>\n",
       "      <td>12.222456</td>\n",
       "      <td>-5.986515</td>\n",
       "      <td>-3.229876</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>1.045244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242917</td>\n",
       "      <td>0.397124</td>\n",
       "      <td>4.878927</td>\n",
       "      <td>5.264405</td>\n",
       "      <td>2.994150</td>\n",
       "      <td>2.870259</td>\n",
       "      <td>0.314687</td>\n",
       "      <td>2.809367</td>\n",
       "      <td>-0.628893</td>\n",
       "      <td>3.489642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-385.279846</td>\n",
       "      <td>118.912277</td>\n",
       "      <td>13.351930</td>\n",
       "      <td>38.183296</td>\n",
       "      <td>4.666812</td>\n",
       "      <td>5.982921</td>\n",
       "      <td>-3.649076</td>\n",
       "      <td>-1.012486</td>\n",
       "      <td>1.646602</td>\n",
       "      <td>-5.259025</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936629</td>\n",
       "      <td>-0.579203</td>\n",
       "      <td>-1.418086</td>\n",
       "      <td>2.701938</td>\n",
       "      <td>-0.128480</td>\n",
       "      <td>-0.650981</td>\n",
       "      <td>0.222955</td>\n",
       "      <td>1.449542</td>\n",
       "      <td>-1.601669</td>\n",
       "      <td>-0.025312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-351.163513</td>\n",
       "      <td>109.502029</td>\n",
       "      <td>4.125424</td>\n",
       "      <td>35.244442</td>\n",
       "      <td>-8.720855</td>\n",
       "      <td>-9.711296</td>\n",
       "      <td>-7.127413</td>\n",
       "      <td>-2.853542</td>\n",
       "      <td>0.309397</td>\n",
       "      <td>1.081380</td>\n",
       "      <td>...</td>\n",
       "      <td>3.420357</td>\n",
       "      <td>6.290679</td>\n",
       "      <td>3.681709</td>\n",
       "      <td>5.164492</td>\n",
       "      <td>3.685917</td>\n",
       "      <td>7.506263</td>\n",
       "      <td>7.469996</td>\n",
       "      <td>7.370858</td>\n",
       "      <td>6.551222</td>\n",
       "      <td>5.917849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10764</th>\n",
       "      <td>-307.079376</td>\n",
       "      <td>110.748199</td>\n",
       "      <td>3.071705</td>\n",
       "      <td>26.098019</td>\n",
       "      <td>1.365202</td>\n",
       "      <td>-11.161897</td>\n",
       "      <td>-9.812368</td>\n",
       "      <td>2.677070</td>\n",
       "      <td>-12.284978</td>\n",
       "      <td>1.411189</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.210361</td>\n",
       "      <td>-1.421704</td>\n",
       "      <td>-2.658003</td>\n",
       "      <td>1.054526</td>\n",
       "      <td>-3.188072</td>\n",
       "      <td>0.515963</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.752482</td>\n",
       "      <td>-1.321549</td>\n",
       "      <td>-3.410846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10765</th>\n",
       "      <td>-239.924500</td>\n",
       "      <td>72.984070</td>\n",
       "      <td>-18.580250</td>\n",
       "      <td>-6.845965</td>\n",
       "      <td>-45.430508</td>\n",
       "      <td>0.680878</td>\n",
       "      <td>-8.975255</td>\n",
       "      <td>-18.834259</td>\n",
       "      <td>-0.833172</td>\n",
       "      <td>-0.091480</td>\n",
       "      <td>...</td>\n",
       "      <td>6.675857</td>\n",
       "      <td>10.649000</td>\n",
       "      <td>7.589522</td>\n",
       "      <td>8.607011</td>\n",
       "      <td>2.216211</td>\n",
       "      <td>7.668085</td>\n",
       "      <td>-0.767406</td>\n",
       "      <td>2.716651</td>\n",
       "      <td>-3.422506</td>\n",
       "      <td>4.849663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>-333.780945</td>\n",
       "      <td>95.989861</td>\n",
       "      <td>-14.832805</td>\n",
       "      <td>15.677377</td>\n",
       "      <td>-13.445169</td>\n",
       "      <td>-0.555010</td>\n",
       "      <td>-22.605991</td>\n",
       "      <td>-9.910537</td>\n",
       "      <td>-7.518460</td>\n",
       "      <td>-5.561435</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.285286</td>\n",
       "      <td>1.564119</td>\n",
       "      <td>6.515084</td>\n",
       "      <td>10.374402</td>\n",
       "      <td>8.091890</td>\n",
       "      <td>14.184700</td>\n",
       "      <td>12.203178</td>\n",
       "      <td>19.104069</td>\n",
       "      <td>6.390728</td>\n",
       "      <td>4.591437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10767</th>\n",
       "      <td>-339.677338</td>\n",
       "      <td>118.469772</td>\n",
       "      <td>10.663605</td>\n",
       "      <td>37.051121</td>\n",
       "      <td>-10.494557</td>\n",
       "      <td>11.044798</td>\n",
       "      <td>-7.957495</td>\n",
       "      <td>-2.332411</td>\n",
       "      <td>-4.041122</td>\n",
       "      <td>-2.618592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.823455</td>\n",
       "      <td>1.826434</td>\n",
       "      <td>-0.563071</td>\n",
       "      <td>3.543955</td>\n",
       "      <td>-0.973314</td>\n",
       "      <td>3.040696</td>\n",
       "      <td>3.090068</td>\n",
       "      <td>4.243995</td>\n",
       "      <td>0.834076</td>\n",
       "      <td>1.139718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10768</th>\n",
       "      <td>-347.986877</td>\n",
       "      <td>119.509628</td>\n",
       "      <td>-8.292249</td>\n",
       "      <td>15.593624</td>\n",
       "      <td>-9.171785</td>\n",
       "      <td>-2.750766</td>\n",
       "      <td>-7.087870</td>\n",
       "      <td>0.804843</td>\n",
       "      <td>-0.902798</td>\n",
       "      <td>2.116315</td>\n",
       "      <td>...</td>\n",
       "      <td>5.915211</td>\n",
       "      <td>5.233806</td>\n",
       "      <td>6.230196</td>\n",
       "      <td>8.562304</td>\n",
       "      <td>2.252800</td>\n",
       "      <td>5.576434</td>\n",
       "      <td>2.432891</td>\n",
       "      <td>6.054037</td>\n",
       "      <td>9.382090</td>\n",
       "      <td>3.946812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10769 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           mfcc_1      mfcc_2     mfcc_3     mfcc_4     mfcc_5     mfcc_6  \\\n",
       "0     -379.964142  109.689522   8.409287  14.947894 -10.617918  -1.278166   \n",
       "1     -316.165100   96.198570 -13.874292  23.989883 -16.052597   6.285239   \n",
       "2     -354.932220  124.622856  13.342740  26.263084  -8.358158  12.222456   \n",
       "3     -385.279846  118.912277  13.351930  38.183296   4.666812   5.982921   \n",
       "4     -351.163513  109.502029   4.125424  35.244442  -8.720855  -9.711296   \n",
       "...           ...         ...        ...        ...        ...        ...   \n",
       "10764 -307.079376  110.748199   3.071705  26.098019   1.365202 -11.161897   \n",
       "10765 -239.924500   72.984070 -18.580250  -6.845965 -45.430508   0.680878   \n",
       "10766 -333.780945   95.989861 -14.832805  15.677377 -13.445169  -0.555010   \n",
       "10767 -339.677338  118.469772  10.663605  37.051121 -10.494557  11.044798   \n",
       "10768 -347.986877  119.509628  -8.292249  15.593624  -9.171785  -2.750766   \n",
       "\n",
       "          mfcc_7     mfcc_8     mfcc_9   mfcc_10  ...   mfcc_23    mfcc_24  \\\n",
       "0      -9.656478  -3.804506   1.033329  2.386927  ... -0.390324   2.626545   \n",
       "1     -17.207815  -9.229585  -1.843399 -1.561611  ...  2.732685   5.995924   \n",
       "2      -5.986515  -3.229876   0.693578  1.045244  ... -0.242917   0.397124   \n",
       "3      -3.649076  -1.012486   1.646602 -5.259025  ...  2.936629  -0.579203   \n",
       "4      -7.127413  -2.853542   0.309397  1.081380  ...  3.420357   6.290679   \n",
       "...          ...        ...        ...       ...  ...       ...        ...   \n",
       "10764  -9.812368   2.677070 -12.284978  1.411189  ... -4.210361  -1.421704   \n",
       "10765  -8.975255 -18.834259  -0.833172 -0.091480  ...  6.675857  10.649000   \n",
       "10766 -22.605991  -9.910537  -7.518460 -5.561435  ... -3.285286   1.564119   \n",
       "10767  -7.957495  -2.332411  -4.041122 -2.618592  ... -0.823455   1.826434   \n",
       "10768  -7.087870   0.804843  -0.902798  2.116315  ...  5.915211   5.233806   \n",
       "\n",
       "        mfcc_25    mfcc_26   mfcc_27    mfcc_28    mfcc_29    mfcc_30  \\\n",
       "0      0.437545   2.089825  0.171152   5.063381   3.834727   1.973997   \n",
       "1      5.040682   8.586355  6.141405   8.720468   8.573661  10.320451   \n",
       "2      4.878927   5.264405  2.994150   2.870259   0.314687   2.809367   \n",
       "3     -1.418086   2.701938 -0.128480  -0.650981   0.222955   1.449542   \n",
       "4      3.681709   5.164492  3.685917   7.506263   7.469996   7.370858   \n",
       "...         ...        ...       ...        ...        ...        ...   \n",
       "10764 -2.658003   1.054526 -3.188072   0.515963   0.066960   0.752482   \n",
       "10765  7.589522   8.607011  2.216211   7.668085  -0.767406   2.716651   \n",
       "10766  6.515084  10.374402  8.091890  14.184700  12.203178  19.104069   \n",
       "10767 -0.563071   3.543955 -0.973314   3.040696   3.090068   4.243995   \n",
       "10768  6.230196   8.562304  2.252800   5.576434   2.432891   6.054037   \n",
       "\n",
       "        mfcc_31   mfcc_32  \n",
       "0     -3.659752 -1.247434  \n",
       "1      7.674820  4.114799  \n",
       "2     -0.628893  3.489642  \n",
       "3     -1.601669 -0.025312  \n",
       "4      6.551222  5.917849  \n",
       "...         ...       ...  \n",
       "10764 -1.321549 -3.410846  \n",
       "10765 -3.422506  4.849663  \n",
       "10766  6.390728  4.591437  \n",
       "10767  0.834076  1.139718  \n",
       "10768  9.382090  3.946812  \n",
       "\n",
       "[10769 rows x 32 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdaedeadbe0>]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정규화 x\n",
    "train_x = torch.from_numpy(np.array(train_x_df)).float()\n",
    "train_y = torch.from_numpy(train_y_df.to_numpy()).float()\n",
    "\n",
    "test_x = torch.from_numpy(np.array(test_x_df)).float()\n",
    "test_y = torch.from_numpy(test_y_df.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-379.9641,  109.6895,    8.4093,  ...,    1.9740,   -3.6598,\n",
       "            -1.2474],\n",
       "         [-316.1651,   96.1986,  -13.8743,  ...,   10.3205,    7.6748,\n",
       "             4.1148],\n",
       "         [-354.9322,  124.6229,   13.3427,  ...,    2.8094,   -0.6289,\n",
       "             3.4896],\n",
       "         ...,\n",
       "         [-333.7809,   95.9899,  -14.8328,  ...,   19.1041,    6.3907,\n",
       "             4.5914],\n",
       "         [-339.6773,  118.4698,   10.6636,  ...,    4.2440,    0.8341,\n",
       "             1.1397],\n",
       "         [-347.9869,  119.5096,   -8.2922,  ...,    6.0540,    9.3821,\n",
       "             3.9468]]),\n",
       " tensor([[-336.9851,  106.8033,    4.5349,  ...,    8.9879,    5.9921,\n",
       "             4.6221],\n",
       "         [-337.9043,  105.4469,   11.3103,  ...,   -1.1719,   -4.0991,\n",
       "            -1.5880],\n",
       "         [-419.5555,  125.5558,   18.3917,  ...,    1.0848,   -0.5761,\n",
       "             0.6626],\n",
       "         ...,\n",
       "         [-371.5176,  117.5567,    5.4989,  ...,    0.7992,   -2.3674,\n",
       "             0.9826],\n",
       "         [-375.5778,  106.7169,    2.8529,  ...,   14.1587,    8.5345,\n",
       "            10.3914],\n",
       "         [-469.1487,   96.4031,   26.5301,  ...,    4.7098,    1.1021,\n",
       "            -0.5791]]))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7807, 0.6002, 0.5256, ..., 0.3406, 0.2676, 0.265 ],\n",
       "       [0.847 , 0.5264, 0.3587, ..., 0.5966, 0.6359, 0.4704],\n",
       "       [0.8067, 0.682 , 0.5625, ..., 0.3662, 0.3661, 0.4465],\n",
       "       ...,\n",
       "       [0.8287, 0.5253, 0.3515, ..., 0.8661, 0.5941, 0.4887],\n",
       "       [0.8225, 0.6483, 0.5424, ..., 0.4102, 0.4136, 0.3564],\n",
       "       [0.8139, 0.654 , 0.4005, ..., 0.4658, 0.6913, 0.464 ]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.array(train_x_df))\n",
    "# # ### 다 0으로 초기화됨\n",
    "normalized_train_data = scaler.transform(np.array(train_x_df))\n",
    "normalized_test_data = torch.tensor( scaler.transform(np.array(test_x_df))).float()\n",
    "normalized_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정규화 0\n",
    "norm_train_x = torch.from_numpy(normalized_train_data).float()\n",
    "train_y = torch.from_numpy(train_y_df.to_numpy()).float()\n",
    "\n",
    "norm_test_x = torch.from_numpy(normalized_test_data).float()\n",
    "test_y = torch.from_numpy(test_y_df.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "        out, (hidden_state, cell_state) = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(out[:,-1,:])\n",
    "        hidden = self.fc(hidden_state[-1])\n",
    "        return output, hidden,  out[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to calculate CCC\n",
    "def compute_ccc(y_true, y_pred):\n",
    "    mu_true = np.mean(y_true)\n",
    "    mu_pred = np.mean(y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    std_true = np.std(y_true)\n",
    "    std_pred = np.std(y_pred)\n",
    "\n",
    "    rho = np.corrcoef(y_true, y_pred)[0,1]\n",
    "\n",
    "    num = 2 * rho * std_true * std_pred\n",
    "    denom = var_true + var_pred + (mu_true - mu_pred)**2\n",
    "\n",
    "    ccc = num / denom\n",
    "    return ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-7b0ae7840ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3295\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import average_precision_score\n",
    "input_size = 32\n",
    "learning_rate = 0.0001\n",
    "hidden_sizes = [64]\n",
    "num_layerss = [3,4,5]\n",
    "num_epochss = [100,200,300,400]\n",
    "\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds)\n",
    "criterion = nn.MSELoss()\n",
    "results = []\n",
    "min_ccc = 0\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layers in num_layerss:\n",
    "        for num_epochs in num_epochss:\n",
    "            # initialize model and optimizer\n",
    "            model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            # train and evaluate using cross-validation\n",
    "            fold_results = []\n",
    "            for train_idx, val_idx in kf.split(norm_train_x):\n",
    "                train_x_fold, train_y_fold = norm_train_x[train_idx], train_y[train_idx]\n",
    "                val_x_fold, val_y_fold = norm_train_x[val_idx], train_y[val_idx]\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    outputs,_,_ = model(train_x_fold.unsqueeze(1))\n",
    "                    loss = criterion(outputs, train_y_fold)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_outputs,_,_ = model(val_x_fold.unsqueeze(1))\n",
    "                    val_loss = criterion(val_outputs, val_y_fold)\n",
    "                    val_predictions = val_outputs.detach().numpy().flatten()\n",
    "                    val_targets = val_y_fold.detach().numpy().flatten()\n",
    "                    ccc = compute_ccc(val_targets, val_predictions)\n",
    "                    fold_results.append((val_loss.item(), ccc))\n",
    "                    if ccc > min_ccc :\n",
    "                        min_ccc = ccc\n",
    "                        print(f'min_ccc: {min_ccc}') \n",
    "                        torch.save(model.state_dict(), f'./model/lstm_arousal_model_best.pt')\n",
    "            # compute average performance over folds\n",
    "            avg_val_loss = np.mean([x[0] for x in fold_results])\n",
    "            avg_ccc = np.mean([x[1] for x in fold_results])\n",
    "            results.append((hidden_size, num_layers, num_epochs, avg_val_loss, avg_ccc))\n",
    "            # print results for current hyperparameter combination\n",
    "            print(f\"hidden_size={hidden_size}, num_layers={num_layers}, num_epochs={num_epochs}: \"\n",
    "                  f\"val_loss={avg_val_loss:.4f}, ccc={avg_ccc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find hyperparameters with best performance\n",
    "best_results = sorted(results, key=lambda x: x[4], reverse=True)\n",
    "best_hidden_size, best_num_layers, best_num_epochs, _, best_ccc = best_results[0]\n",
    "print(f\"Best hyperparameters: hidden_size={best_hidden_size}, num_layers={best_num_layers}, \"\n",
    "      f\"num_epochs={best_num_epochs}, ccc={best_ccc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32\n",
    "hidden_size = 64 # 64\n",
    "num_layers = 3 # 3\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 400\n",
    "# model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.L1Loss() # maeloss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9840d164d8cb4cdc806fa703f60120fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/400 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Loss: 3.2987\n",
      "min_ccc: 1.563341874252848e-05\n",
      "Epoch [2/400], Loss: 3.2885\n",
      "Epoch [3/400], Loss: 3.2783\n",
      "Epoch [4/400], Loss: 3.2678\n",
      "Epoch [5/400], Loss: 3.2569\n",
      "Epoch [6/400], Loss: 3.2458\n",
      "Epoch [7/400], Loss: 3.2342\n",
      "Epoch [8/400], Loss: 3.2222\n",
      "Epoch [9/400], Loss: 3.2096\n",
      "Epoch [10/400], Loss: 3.1964\n",
      "Epoch [11/400], Loss: 3.1825\n",
      "min_ccc: 1.8239612068025745e-05\n",
      "Epoch [12/400], Loss: 3.1677\n",
      "min_ccc: 2.427377832081409e-05\n",
      "Epoch [13/400], Loss: 3.1520\n",
      "min_ccc: 3.1112620192044384e-05\n",
      "Epoch [14/400], Loss: 3.1352\n",
      "min_ccc: 3.849262068648767e-05\n",
      "Epoch [15/400], Loss: 3.1171\n",
      "min_ccc: 4.6036604604635296e-05\n",
      "Epoch [16/400], Loss: 3.0976\n",
      "min_ccc: 5.313901783663325e-05\n",
      "Epoch [17/400], Loss: 3.0762\n",
      "min_ccc: 5.891318308216262e-05\n",
      "Epoch [18/400], Loss: 3.0529\n",
      "min_ccc: 6.22884431847613e-05\n",
      "Epoch [19/400], Loss: 3.0273\n",
      "Epoch [20/400], Loss: 2.9991\n",
      "Epoch [21/400], Loss: 2.9680\n",
      "Epoch [22/400], Loss: 2.9336\n",
      "Epoch [23/400], Loss: 2.8956\n",
      "Epoch [24/400], Loss: 2.8537\n",
      "Epoch [25/400], Loss: 2.8075\n",
      "Epoch [26/400], Loss: 2.7566\n",
      "Epoch [27/400], Loss: 2.7009\n",
      "min_ccc: 7.268395930898661e-05\n",
      "Epoch [28/400], Loss: 2.6400\n",
      "min_ccc: 0.00015573702816604236\n",
      "Epoch [29/400], Loss: 2.5735\n",
      "min_ccc: 0.0002858498960117536\n",
      "Epoch [30/400], Loss: 2.5011\n",
      "min_ccc: 0.00046897357303782614\n",
      "Epoch [31/400], Loss: 2.4224\n",
      "min_ccc: 0.0007099208957116958\n",
      "Epoch [32/400], Loss: 2.3368\n",
      "min_ccc: 0.0010125021205172338\n",
      "Epoch [33/400], Loss: 2.2441\n",
      "min_ccc: 0.001376753416857606\n",
      "Epoch [34/400], Loss: 2.1438\n",
      "min_ccc: 0.0017966943454596615\n",
      "Epoch [35/400], Loss: 2.0359\n",
      "min_ccc: 0.00225608319835551\n",
      "Epoch [36/400], Loss: 1.9202\n",
      "min_ccc: 0.0027216256360059732\n",
      "Epoch [37/400], Loss: 1.7971\n",
      "min_ccc: 0.003150993567545959\n",
      "Epoch [38/400], Loss: 1.6673\n",
      "min_ccc: 0.003522265755313467\n",
      "Epoch [39/400], Loss: 1.5318\n",
      "min_ccc: 0.003868553595275246\n",
      "Epoch [40/400], Loss: 1.3919\n",
      "min_ccc: 0.0042912877993550955\n",
      "Epoch [41/400], Loss: 1.2489\n",
      "min_ccc: 0.004979328058779442\n",
      "Epoch [42/400], Loss: 1.1042\n",
      "min_ccc: 0.006270988945451519\n",
      "Epoch [43/400], Loss: 0.9589\n",
      "min_ccc: 0.008697360609187623\n",
      "Epoch [44/400], Loss: 0.8140\n",
      "min_ccc: 0.01302870869572211\n",
      "Epoch [45/400], Loss: 0.6710\n",
      "min_ccc: 0.020652681161420026\n",
      "Epoch [46/400], Loss: 0.5342\n",
      "min_ccc: 0.03377124414334057\n",
      "Epoch [47/400], Loss: 0.4125\n",
      "min_ccc: 0.05394160659171689\n",
      "Epoch [48/400], Loss: 0.3175\n",
      "min_ccc: 0.07307971338924178\n",
      "Epoch [49/400], Loss: 0.2653\n",
      "Epoch [50/400], Loss: 0.2627\n",
      "Epoch [51/400], Loss: 0.2980\n",
      "Epoch [52/400], Loss: 0.3481\n",
      "Epoch [53/400], Loss: 0.3933\n",
      "Epoch [54/400], Loss: 0.4241\n",
      "Epoch [55/400], Loss: 0.4382\n",
      "Epoch [56/400], Loss: 0.4367\n",
      "Epoch [57/400], Loss: 0.4223\n",
      "Epoch [58/400], Loss: 0.3986\n",
      "Epoch [59/400], Loss: 0.3696\n",
      "Epoch [60/400], Loss: 0.3387\n",
      "Epoch [61/400], Loss: 0.3100\n",
      "Epoch [62/400], Loss: 0.2855\n",
      "Epoch [63/400], Loss: 0.2689\n",
      "Epoch [64/400], Loss: 0.2590\n",
      "Epoch [65/400], Loss: 0.2573\n",
      "Epoch [66/400], Loss: 0.2604\n",
      "Epoch [67/400], Loss: 0.2682\n",
      "Epoch [68/400], Loss: 0.2754\n",
      "Epoch [69/400], Loss: 0.2829\n",
      "Epoch [70/400], Loss: 0.2883\n",
      "Epoch [71/400], Loss: 0.2908\n",
      "Epoch [72/400], Loss: 0.2902\n",
      "Epoch [73/400], Loss: 0.2869\n",
      "Epoch [74/400], Loss: 0.2817\n",
      "Epoch [75/400], Loss: 0.2757\n",
      "Epoch [76/400], Loss: 0.2708\n",
      "Epoch [77/400], Loss: 0.2654\n",
      "Epoch [78/400], Loss: 0.2604\n",
      "Epoch [79/400], Loss: 0.2574\n",
      "Epoch [80/400], Loss: 0.2573\n",
      "Epoch [81/400], Loss: 0.2576\n",
      "Epoch [82/400], Loss: 0.2580\n",
      "Epoch [83/400], Loss: 0.2594\n",
      "Epoch [84/400], Loss: 0.2611\n",
      "Epoch [85/400], Loss: 0.2626\n",
      "Epoch [86/400], Loss: 0.2635\n",
      "Epoch [87/400], Loss: 0.2634\n",
      "Epoch [88/400], Loss: 0.2623\n",
      "Epoch [89/400], Loss: 0.2610\n",
      "Epoch [90/400], Loss: 0.2596\n",
      "Epoch [91/400], Loss: 0.2585\n",
      "Epoch [92/400], Loss: 0.2578\n",
      "Epoch [93/400], Loss: 0.2577\n",
      "Epoch [94/400], Loss: 0.2575\n",
      "Epoch [95/400], Loss: 0.2574\n",
      "Epoch [96/400], Loss: 0.2573\n",
      "Epoch [97/400], Loss: 0.2572\n",
      "Epoch [98/400], Loss: 0.2571\n",
      "Epoch [99/400], Loss: 0.2573\n",
      "Epoch [100/400], Loss: 0.2578\n",
      "Epoch [101/400], Loss: 0.2580\n",
      "Epoch [102/400], Loss: 0.2581\n",
      "Epoch [103/400], Loss: 0.2579\n",
      "Epoch [104/400], Loss: 0.2575\n",
      "Epoch [105/400], Loss: 0.2572\n",
      "Epoch [106/400], Loss: 0.2571\n",
      "Epoch [107/400], Loss: 0.2571\n",
      "Epoch [108/400], Loss: 0.2572\n",
      "Epoch [109/400], Loss: 0.2572\n",
      "Epoch [110/400], Loss: 0.2573\n",
      "Epoch [111/400], Loss: 0.2573\n",
      "Epoch [112/400], Loss: 0.2573\n",
      "Epoch [113/400], Loss: 0.2573\n",
      "Epoch [114/400], Loss: 0.2573\n",
      "Epoch [115/400], Loss: 0.2574\n",
      "Epoch [116/400], Loss: 0.2574\n",
      "Epoch [117/400], Loss: 0.2574\n",
      "Epoch [118/400], Loss: 0.2574\n",
      "Epoch [119/400], Loss: 0.2573\n",
      "Epoch [120/400], Loss: 0.2573\n",
      "Epoch [121/400], Loss: 0.2573\n",
      "Epoch [122/400], Loss: 0.2573\n",
      "Epoch [123/400], Loss: 0.2573\n",
      "Epoch [124/400], Loss: 0.2573\n",
      "Epoch [125/400], Loss: 0.2572\n",
      "Epoch [126/400], Loss: 0.2572\n",
      "Epoch [127/400], Loss: 0.2572\n",
      "Epoch [128/400], Loss: 0.2571\n",
      "Epoch [129/400], Loss: 0.2571\n",
      "Epoch [130/400], Loss: 0.2571\n",
      "Epoch [131/400], Loss: 0.2571\n",
      "Epoch [132/400], Loss: 0.2570\n",
      "Epoch [133/400], Loss: 0.2570\n",
      "Epoch [134/400], Loss: 0.2570\n",
      "Epoch [135/400], Loss: 0.2571\n",
      "Epoch [136/400], Loss: 0.2571\n",
      "Epoch [137/400], Loss: 0.2570\n",
      "Epoch [138/400], Loss: 0.2570\n",
      "Epoch [139/400], Loss: 0.2570\n",
      "Epoch [140/400], Loss: 0.2570\n",
      "Epoch [141/400], Loss: 0.2570\n",
      "Epoch [142/400], Loss: 0.2570\n",
      "Epoch [143/400], Loss: 0.2570\n",
      "Epoch [144/400], Loss: 0.2570\n",
      "Epoch [145/400], Loss: 0.2570\n",
      "Epoch [146/400], Loss: 0.2570\n",
      "Epoch [147/400], Loss: 0.2570\n",
      "Epoch [148/400], Loss: 0.2570\n",
      "Epoch [149/400], Loss: 0.2570\n",
      "Epoch [150/400], Loss: 0.2570\n",
      "Epoch [151/400], Loss: 0.2570\n",
      "Epoch [152/400], Loss: 0.2570\n",
      "Epoch [153/400], Loss: 0.2570\n",
      "Epoch [154/400], Loss: 0.2570\n",
      "Epoch [155/400], Loss: 0.2570\n",
      "Epoch [156/400], Loss: 0.2570\n",
      "Epoch [157/400], Loss: 0.2570\n",
      "Epoch [158/400], Loss: 0.2570\n",
      "Epoch [159/400], Loss: 0.2570\n",
      "Epoch [160/400], Loss: 0.2570\n",
      "Epoch [161/400], Loss: 0.2570\n",
      "Epoch [162/400], Loss: 0.2570\n",
      "Epoch [163/400], Loss: 0.2570\n",
      "Epoch [164/400], Loss: 0.2570\n",
      "Epoch [165/400], Loss: 0.2570\n",
      "Epoch [166/400], Loss: 0.2570\n",
      "Epoch [167/400], Loss: 0.2570\n",
      "Epoch [168/400], Loss: 0.2570\n",
      "Epoch [169/400], Loss: 0.2570\n",
      "Epoch [170/400], Loss: 0.2570\n",
      "Epoch [171/400], Loss: 0.2570\n",
      "Epoch [172/400], Loss: 0.2570\n",
      "Epoch [173/400], Loss: 0.2570\n",
      "Epoch [174/400], Loss: 0.2570\n",
      "Epoch [175/400], Loss: 0.2570\n",
      "Epoch [176/400], Loss: 0.2570\n",
      "Epoch [177/400], Loss: 0.2570\n",
      "Epoch [178/400], Loss: 0.2570\n",
      "Epoch [179/400], Loss: 0.2570\n",
      "Epoch [180/400], Loss: 0.2570\n",
      "Epoch [181/400], Loss: 0.2570\n",
      "Epoch [182/400], Loss: 0.2570\n",
      "Epoch [183/400], Loss: 0.2570\n",
      "Epoch [184/400], Loss: 0.2570\n",
      "Epoch [185/400], Loss: 0.2570\n",
      "Epoch [186/400], Loss: 0.2570\n",
      "Epoch [187/400], Loss: 0.2570\n",
      "Epoch [188/400], Loss: 0.2570\n",
      "Epoch [189/400], Loss: 0.2570\n",
      "Epoch [190/400], Loss: 0.2570\n",
      "Epoch [191/400], Loss: 0.2570\n",
      "Epoch [192/400], Loss: 0.2570\n",
      "Epoch [193/400], Loss: 0.2570\n",
      "Epoch [194/400], Loss: 0.2570\n",
      "Epoch [195/400], Loss: 0.2570\n",
      "Epoch [196/400], Loss: 0.2570\n",
      "Epoch [197/400], Loss: 0.2570\n",
      "Epoch [198/400], Loss: 0.2570\n",
      "Epoch [199/400], Loss: 0.2570\n",
      "Epoch [200/400], Loss: 0.2570\n",
      "Epoch [201/400], Loss: 0.2570\n",
      "Epoch [202/400], Loss: 0.2570\n",
      "Epoch [203/400], Loss: 0.2570\n",
      "Epoch [204/400], Loss: 0.2570\n",
      "Epoch [205/400], Loss: 0.2570\n",
      "Epoch [206/400], Loss: 0.2570\n",
      "Epoch [207/400], Loss: 0.2570\n",
      "Epoch [208/400], Loss: 0.2570\n",
      "Epoch [209/400], Loss: 0.2570\n",
      "Epoch [210/400], Loss: 0.2570\n",
      "Epoch [211/400], Loss: 0.2570\n",
      "Epoch [212/400], Loss: 0.2570\n",
      "Epoch [213/400], Loss: 0.2570\n",
      "Epoch [214/400], Loss: 0.2570\n",
      "Epoch [215/400], Loss: 0.2570\n",
      "Epoch [216/400], Loss: 0.2570\n",
      "Epoch [217/400], Loss: 0.2569\n",
      "Epoch [218/400], Loss: 0.2570\n",
      "Epoch [219/400], Loss: 0.2569\n",
      "Epoch [220/400], Loss: 0.2570\n",
      "Epoch [221/400], Loss: 0.2570\n",
      "Epoch [222/400], Loss: 0.2570\n",
      "Epoch [223/400], Loss: 0.2570\n",
      "Epoch [224/400], Loss: 0.2570\n",
      "Epoch [225/400], Loss: 0.2569\n",
      "Epoch [226/400], Loss: 0.2570\n",
      "Epoch [227/400], Loss: 0.2569\n",
      "Epoch [228/400], Loss: 0.2569\n",
      "Epoch [229/400], Loss: 0.2569\n",
      "Epoch [230/400], Loss: 0.2569\n",
      "Epoch [231/400], Loss: 0.2569\n",
      "Epoch [232/400], Loss: 0.2569\n",
      "Epoch [233/400], Loss: 0.2569\n",
      "Epoch [234/400], Loss: 0.2569\n",
      "Epoch [235/400], Loss: 0.2569\n",
      "Epoch [236/400], Loss: 0.2569\n",
      "Epoch [237/400], Loss: 0.2569\n",
      "Epoch [238/400], Loss: 0.2569\n",
      "Epoch [239/400], Loss: 0.2569\n",
      "Epoch [240/400], Loss: 0.2569\n",
      "Epoch [241/400], Loss: 0.2569\n",
      "Epoch [242/400], Loss: 0.2569\n",
      "Epoch [243/400], Loss: 0.2569\n",
      "Epoch [244/400], Loss: 0.2569\n",
      "Epoch [245/400], Loss: 0.2569\n",
      "Epoch [246/400], Loss: 0.2569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [247/400], Loss: 0.2569\n",
      "Epoch [248/400], Loss: 0.2570\n",
      "Epoch [249/400], Loss: 0.2569\n",
      "Epoch [250/400], Loss: 0.2569\n",
      "Epoch [251/400], Loss: 0.2569\n",
      "Epoch [252/400], Loss: 0.2569\n",
      "Epoch [253/400], Loss: 0.2569\n",
      "Epoch [254/400], Loss: 0.2569\n",
      "Epoch [255/400], Loss: 0.2569\n",
      "Epoch [256/400], Loss: 0.2569\n",
      "Epoch [257/400], Loss: 0.2569\n",
      "Epoch [258/400], Loss: 0.2569\n",
      "Epoch [259/400], Loss: 0.2569\n",
      "Epoch [260/400], Loss: 0.2569\n",
      "Epoch [261/400], Loss: 0.2569\n",
      "Epoch [262/400], Loss: 0.2569\n",
      "Epoch [263/400], Loss: 0.2569\n",
      "Epoch [264/400], Loss: 0.2569\n",
      "Epoch [265/400], Loss: 0.2569\n",
      "Epoch [266/400], Loss: 0.2569\n",
      "Epoch [267/400], Loss: 0.2569\n",
      "Epoch [268/400], Loss: 0.2569\n",
      "Epoch [269/400], Loss: 0.2569\n",
      "Epoch [270/400], Loss: 0.2569\n",
      "Epoch [271/400], Loss: 0.2569\n",
      "Epoch [272/400], Loss: 0.2569\n",
      "Epoch [273/400], Loss: 0.2569\n",
      "Epoch [274/400], Loss: 0.2569\n",
      "Epoch [275/400], Loss: 0.2569\n",
      "Epoch [276/400], Loss: 0.2569\n",
      "Epoch [277/400], Loss: 0.2569\n",
      "Epoch [278/400], Loss: 0.2569\n",
      "Epoch [279/400], Loss: 0.2569\n",
      "Epoch [280/400], Loss: 0.2569\n",
      "Epoch [281/400], Loss: 0.2569\n",
      "Epoch [282/400], Loss: 0.2569\n",
      "Epoch [283/400], Loss: 0.2569\n",
      "Epoch [284/400], Loss: 0.2569\n",
      "Epoch [285/400], Loss: 0.2569\n",
      "Epoch [286/400], Loss: 0.2569\n",
      "Epoch [287/400], Loss: 0.2569\n",
      "Epoch [288/400], Loss: 0.2569\n",
      "Epoch [289/400], Loss: 0.2569\n",
      "Epoch [290/400], Loss: 0.2569\n",
      "Epoch [291/400], Loss: 0.2569\n",
      "Epoch [292/400], Loss: 0.2569\n",
      "Epoch [293/400], Loss: 0.2569\n",
      "Epoch [294/400], Loss: 0.2569\n",
      "Epoch [295/400], Loss: 0.2569\n",
      "Epoch [296/400], Loss: 0.2569\n",
      "Epoch [297/400], Loss: 0.2569\n",
      "Epoch [298/400], Loss: 0.2569\n",
      "Epoch [299/400], Loss: 0.2569\n",
      "Epoch [300/400], Loss: 0.2569\n",
      "Epoch [301/400], Loss: 0.2569\n",
      "Epoch [302/400], Loss: 0.2569\n",
      "Epoch [303/400], Loss: 0.2569\n",
      "Epoch [304/400], Loss: 0.2569\n",
      "Epoch [305/400], Loss: 0.2569\n",
      "Epoch [306/400], Loss: 0.2569\n",
      "Epoch [307/400], Loss: 0.2569\n",
      "Epoch [308/400], Loss: 0.2570\n",
      "Epoch [309/400], Loss: 0.2569\n",
      "Epoch [310/400], Loss: 0.2569\n",
      "Epoch [311/400], Loss: 0.2569\n",
      "Epoch [312/400], Loss: 0.2569\n",
      "Epoch [313/400], Loss: 0.2569\n",
      "Epoch [314/400], Loss: 0.2569\n",
      "Epoch [315/400], Loss: 0.2569\n",
      "Epoch [316/400], Loss: 0.2569\n",
      "Epoch [317/400], Loss: 0.2569\n",
      "Epoch [318/400], Loss: 0.2569\n",
      "Epoch [319/400], Loss: 0.2569\n",
      "Epoch [320/400], Loss: 0.2569\n",
      "Epoch [321/400], Loss: 0.2569\n",
      "Epoch [322/400], Loss: 0.2569\n",
      "Epoch [323/400], Loss: 0.2569\n",
      "Epoch [324/400], Loss: 0.2569\n",
      "Epoch [325/400], Loss: 0.2569\n",
      "Epoch [326/400], Loss: 0.2569\n",
      "Epoch [327/400], Loss: 0.2569\n",
      "Epoch [328/400], Loss: 0.2569\n",
      "Epoch [329/400], Loss: 0.2569\n",
      "Epoch [330/400], Loss: 0.2569\n",
      "Epoch [331/400], Loss: 0.2569\n",
      "Epoch [332/400], Loss: 0.2569\n",
      "Epoch [333/400], Loss: 0.2569\n",
      "Epoch [334/400], Loss: 0.2569\n",
      "Epoch [335/400], Loss: 0.2569\n",
      "Epoch [336/400], Loss: 0.2569\n",
      "Epoch [337/400], Loss: 0.2569\n",
      "Epoch [338/400], Loss: 0.2569\n",
      "Epoch [339/400], Loss: 0.2569\n",
      "Epoch [340/400], Loss: 0.2569\n",
      "Epoch [341/400], Loss: 0.2569\n",
      "Epoch [342/400], Loss: 0.2569\n",
      "Epoch [343/400], Loss: 0.2569\n",
      "Epoch [344/400], Loss: 0.2569\n",
      "Epoch [345/400], Loss: 0.2569\n",
      "Epoch [346/400], Loss: 0.2569\n",
      "Epoch [347/400], Loss: 0.2569\n",
      "Epoch [348/400], Loss: 0.2569\n",
      "Epoch [349/400], Loss: 0.2569\n",
      "Epoch [350/400], Loss: 0.2569\n",
      "Epoch [351/400], Loss: 0.2569\n",
      "Epoch [352/400], Loss: 0.2569\n",
      "Epoch [353/400], Loss: 0.2569\n",
      "Epoch [354/400], Loss: 0.2569\n",
      "Epoch [355/400], Loss: 0.2569\n",
      "Epoch [356/400], Loss: 0.2569\n",
      "Epoch [357/400], Loss: 0.2569\n",
      "Epoch [358/400], Loss: 0.2569\n",
      "Epoch [359/400], Loss: 0.2569\n",
      "Epoch [360/400], Loss: 0.2569\n",
      "Epoch [361/400], Loss: 0.2569\n",
      "Epoch [362/400], Loss: 0.2569\n",
      "Epoch [363/400], Loss: 0.2569\n",
      "Epoch [364/400], Loss: 0.2569\n",
      "Epoch [365/400], Loss: 0.2569\n",
      "Epoch [366/400], Loss: 0.2569\n",
      "Epoch [367/400], Loss: 0.2570\n",
      "Epoch [368/400], Loss: 0.2569\n",
      "Epoch [369/400], Loss: 0.2569\n",
      "Epoch [370/400], Loss: 0.2569\n",
      "Epoch [371/400], Loss: 0.2569\n",
      "Epoch [372/400], Loss: 0.2569\n",
      "Epoch [373/400], Loss: 0.2569\n",
      "Epoch [374/400], Loss: 0.2569\n",
      "Epoch [375/400], Loss: 0.2569\n",
      "Epoch [376/400], Loss: 0.2569\n",
      "Epoch [377/400], Loss: 0.2569\n",
      "Epoch [378/400], Loss: 0.2569\n",
      "Epoch [379/400], Loss: 0.2569\n",
      "Epoch [380/400], Loss: 0.2569\n",
      "Epoch [381/400], Loss: 0.2569\n",
      "Epoch [382/400], Loss: 0.2569\n",
      "Epoch [383/400], Loss: 0.2569\n",
      "Epoch [384/400], Loss: 0.2569\n",
      "Epoch [385/400], Loss: 0.2569\n",
      "Epoch [386/400], Loss: 0.2569\n",
      "Epoch [387/400], Loss: 0.2570\n",
      "Epoch [388/400], Loss: 0.2569\n",
      "Epoch [389/400], Loss: 0.2569\n",
      "Epoch [390/400], Loss: 0.2569\n",
      "Epoch [391/400], Loss: 0.2569\n",
      "Epoch [392/400], Loss: 0.2569\n",
      "Epoch [393/400], Loss: 0.2569\n",
      "Epoch [394/400], Loss: 0.2569\n",
      "Epoch [395/400], Loss: 0.2569\n",
      "Epoch [396/400], Loss: 0.2569\n",
      "Epoch [397/400], Loss: 0.2569\n",
      "Epoch [398/400], Loss: 0.2569\n",
      "Epoch [399/400], Loss: 0.2569\n",
      "Epoch [400/400], Loss: 0.2569\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "min_ccc =0\n",
    "for epoch in tqdm(range(num_epochs), desc='Training', unit='epoch'):\n",
    "    model.train()\n",
    "    outputs,_,_ = model(train_x.unsqueeze(1))\n",
    "    loss = criterion(outputs, train_y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    tqdm.write('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predict, test_hidden, test_out = model(test_x.unsqueeze(1))\n",
    "        ccc = compute_ccc(test_y.numpy(),test_predict.squeeze(1).numpy())\n",
    "        if ccc > min_ccc :\n",
    "                min_ccc = ccc\n",
    "                print(f'min_ccc: {min_ccc}') \n",
    "                torch.save(model.state_dict(), f'./model/lstm_arousal_model_best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arousal_model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "arousal_model.load_state_dict(torch.load('./model/lstm_arousal_model_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2653\n",
      "Test Loss: 0.2681\n",
      "ccc: 0.0731\n",
      "Predictions:  tensor([[3.2145],\n",
      "        [3.1883],\n",
      "        [3.1634],\n",
      "        ...,\n",
      "        [3.1854],\n",
      "        [3.2164],\n",
      "        [3.1432]])\n",
      "tensor([3.5000, 3.4000, 3.0000,  ..., 3.4000, 2.6000, 3.3000])\n",
      "Extracting 64 sized features: tensor([[ 0.4634,  0.5490, -0.4754,  ..., -0.2010, -0.4905, -0.4120],\n",
      "        [ 0.4592,  0.5451, -0.4693,  ..., -0.1997, -0.4868, -0.4081],\n",
      "        [ 0.4544,  0.5414, -0.4652,  ..., -0.1994, -0.4822, -0.4042],\n",
      "        ...,\n",
      "        [ 0.4592,  0.5447, -0.4682,  ..., -0.1995, -0.4867, -0.4077],\n",
      "        [ 0.4636,  0.5498, -0.4745,  ..., -0.2014, -0.4909, -0.4123],\n",
      "        [ 0.4487,  0.5384, -0.4630,  ..., -0.1989, -0.4776, -0.4022]])\n",
      "shape: torch.Size([2693, 64])\n"
     ]
    }
   ],
   "source": [
    "arousal_model.eval()\n",
    "with torch.no_grad():\n",
    "    train_predict, train_hidden, train_out  = arousal_model(train_x.unsqueeze(1))\n",
    "    test_predict, test_hidden, test_out = arousal_model(test_x.unsqueeze(1))\n",
    "    train_loss = criterion(train_predict, train_y)\n",
    "    test_loss = criterion(test_predict, test_y)\n",
    "    \n",
    "    ccc = compute_ccc(test_y.numpy(),test_predict.squeeze(1).numpy())\n",
    "\n",
    "    print('Train Loss: {:.4f}'.format(train_loss.item()))\n",
    "    print('Test Loss: {:.4f}'.format(test_loss.item()))\n",
    "    print('ccc: {:.4f}'.format(ccc))\n",
    "    print('Predictions: ', test_predict)\n",
    "    print(test_y)\n",
    "    print(f'Extracting 64 sized features: {test_out}') # 64 sized feature 뽑아냄\n",
    "    print(f'shape: {test_out.shape}')\n",
    "    \n",
    "    '''\n",
    "    epoch: 700\n",
    "    lr = 0.00005\n",
    "    Train Loss: 0.1392\n",
    "Test Loss: 0.1392\n",
    "ccc: -0.0818\n",
    "\n",
    "\n",
    "    2) MAE\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 100\n",
    "    Train Loss: 0.2581\n",
    "    Test Loss: 0.2581\n",
    "    ccc: 0.0448\n",
    "    \n",
    "    3) MAE LOSS\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 400\n",
    "    Train Loss: 0.2593\n",
    "    Test Loss: 0.2593\n",
    "    ccc: 0.0308\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2569015324115753, 0.2569015324115753)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss.item(), test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/EmotionShortForm'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# filename = './model/lstm_arousal_model.pkl'\n",
    "# pickle.dump(model, open(filename,'wb'))\n",
    "\n",
    "\n",
    "# Save model\n",
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), './model/lstm_arousal_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "        out, (hidden_state, cell_state) = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(out[:,-1,:])\n",
    "        hidden = self.fc(hidden_state[-1])\n",
    "        return output, hidden,  out[:,-1,:]\n",
    "input_size = 32\n",
    "hidden_size = 64 # 64\n",
    "num_layers = 3 # 3\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 250\n",
    "\n",
    "arousal_model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "arousal_model.load_state_dict(torch.load('./model/lstm_arousal_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.7877\n",
      "Test Loss: 7.7877\n",
      "ccc: -0.0001\n",
      "Predictions:  tensor([[0.4719],\n",
      "        [0.4625],\n",
      "        [0.4720],\n",
      "        ...,\n",
      "        [0.4845],\n",
      "        [0.4664],\n",
      "        [0.4603]])\n",
      "Extracting 64 sized features: tensor([[ 0.0734, -0.0814,  0.0662,  ..., -0.0451,  0.0446, -0.0772],\n",
      "        [ 0.0720, -0.0796,  0.0650,  ..., -0.0442,  0.0437, -0.0755],\n",
      "        [ 0.0734, -0.0815,  0.0664,  ..., -0.0459,  0.0447, -0.0770],\n",
      "        ...,\n",
      "        [ 0.0754, -0.0828,  0.0683,  ..., -0.0469,  0.0477, -0.0790],\n",
      "        [ 0.0726, -0.0804,  0.0656,  ..., -0.0451,  0.0437, -0.0760],\n",
      "        [ 0.0718, -0.0795,  0.0648,  ..., -0.0444,  0.0433, -0.0753]])\n",
      "shape: torch.Size([10769, 64])\n"
     ]
    }
   ],
   "source": [
    "arousal_model.eval()\n",
    "with torch.no_grad():\n",
    "    train_predict, train_hidden, train_out  = arousal_model(train_x.unsqueeze(1))\n",
    "    test_predict, test_hidden, test_out = arousal_model(test_x.unsqueeze(1))\n",
    "    train_loss = criterion(train_predict, train_y)\n",
    "    test_loss = criterion(test_predict, test_y)\n",
    "    ccc = compute_ccc(test_y.numpy(),test_predict.squeeze(1).numpy())\n",
    "\n",
    "    print('Train Loss: {:.4f}'.format(train_loss.item()))\n",
    "    print('Test Loss: {:.4f}'.format(test_loss.item()))\n",
    "    print('ccc: {:.4f}'.format(ccc))\n",
    "    print('Predictions: ', test_predict)\n",
    "    print(f'Extracting 64 sized features: {test_out}') # 64 sized feature 뽑아냄\n",
    "    print(f'shape: {test_out.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
