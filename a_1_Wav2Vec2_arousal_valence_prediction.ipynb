{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: cp949 -*-\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import argparse\n",
    "from audtorch.metrics.functional import concordance_cc\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type=int, default=19)\n",
    "# parser.add_argument('--dataset_dir', type=str, default='./KEMDy19/')\n",
    "# parser.add_argument('--ckpt', type=str, default='0', help='checkpoint')\n",
    "# parser.add_argument('--num_fold', type=int, default=5)\n",
    "# parser.add_argument('--lr', type=float, default=1e-5, help='learning_rate')\n",
    "# parser.add_argument('--batch_size', type=int, default=32, help='batch_size')\n",
    "# parser.add_argument('--gpus', type=str, default='0', help='gpu numbers')\n",
    "# parser.add_argument('--epochs', type=int, default=5, help='epochs')\n",
    "# parser.add_argument('--max_seq_len', type=int, default=5, help='max sequence length of speech')\n",
    "# parser.add_argument('--num_labels', type=int, default=7, help='num_labels')\n",
    "# parser.add_argument('--regress', type=int, default=1, help='regression (0-1)')\n",
    "# parser.add_argument('--seed', type=int, default=1234, help='seed')\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터 노트북에서 명령행 인자 대신 변수를 직접 설정합니다.\n",
    "# python speech_regression.py --batch_size 32 --lr 1e-5 --num_fold 1\n",
    "args = argparse.Namespace()\n",
    "args.dataset = 20\n",
    "args.dataset_dir = './data_audio/'\n",
    "args.ckpt = '0'\n",
    "args.num_fold = 1\n",
    "args.lr = 1e-5\n",
    "args.batch_size = 8\n",
    "args.gpus = '0'\n",
    "args.epochs = 5\n",
    "args.max_seq_len = 5\n",
    "args.num_labels = 7\n",
    "args.regress = 1\n",
    "args.seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed\n",
    "if args.seed is not None:\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, path_list, max_seq_len):\n",
    "        super(SpectrogramDataset, self).__init__()\n",
    "        # [0] = wav, [1] = txt, [2] = emo_label, [3] = valence, [4] = arousal\n",
    "        self.wav_list = path_list[0]\n",
    "        self.txt_list = path_list[1]\n",
    "        self.label_list = path_list[2]\n",
    "        self.valence_list = path_list[3]\n",
    "        self.arousal_list = path_list[4]\n",
    "        self.size = len(self.wav_list)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        try:        \n",
    "            wav_data, based_sr = torchaudio.load(self.wav_list[index])            \n",
    "        except:\n",
    "            print('data {} has problem, can not reading '.format(self.wav_list[index]))\n",
    "            return None\n",
    "        \n",
    "        if wav_data.size(-1) > self.max_seq_len:\n",
    "            wav_data = wav_data[:, :self.max_seq_len]\n",
    "        \n",
    "        #input_dict = tokenizer(self.txt_list[index], padding = 'max_length', max_length = args.max_text_len, return_tensors = 'pt', return_attention_mask = False)\n",
    "        #output_text = torch.cat([input_dict['input_ids'], input_dict['token_type_ids'], ~(input_dict['input_ids']==0)], dim=0)\n",
    "        \n",
    "        return wav_data, self.valence_list[index], self.arousal_list[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate_fn(batch):    \n",
    "    batches = list(filter(lambda x: x is not None, batch))\n",
    "    batch = sorted(batches, key=lambda sample: sample[0].size(1), reverse=True)\n",
    "    \n",
    "    seq_lengths = [s[0].size(1) for s in batch]\n",
    "    max_seq_size = max(seq_lengths)\n",
    "    \n",
    "    seqs = torch.zeros(len(batch), max_seq_size)\n",
    "    valences = torch.zeros(len(batch), 1)\n",
    "    arousals = torch.zeros(len(batch), 1)\n",
    "        \n",
    "    for x in range(len(batch)):\n",
    "        sample = batch[x]\n",
    "        tensor = sample[0]\n",
    "        valence = torch.FloatTensor([sample[1]])\n",
    "        arousal = torch.FloatTensor([sample[2]])\n",
    "        seq_length = tensor.size(1)\n",
    "        seqs[x].narrow(0, 0, seq_length).copy_(tensor.squeeze())\n",
    "        valences[x].narrow(0, 0, len(valence)).copy_(valence)\n",
    "        arousals[x].narrow(0, 0, len(arousal)).copy_(arousal)\n",
    "    \n",
    "    return seqs, valences, arousals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "wav_dir = os.path.join(args.dataset_dir, 'wav')\n",
    "label_df = sorted(glob(os.path.join(args.dataset_dir, 'annotation', '*')))\n",
    "\n",
    "all_wav = []\n",
    "all_txt = []\n",
    "all_emotion = []\n",
    "all_valence = []\n",
    "all_arousal = []\n",
    "bad_data_for_20 = 0\n",
    "bad_data_for_19 = 0\n",
    "\n",
    "emotion_dict = {\"angry\": 0, \"disqust\": 1, \"fear\": 2, \"happy\": 3, \"neutral\": 4, \"sad\": 5, \"surprise\": 6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad for 20 0\n",
      "bad for 19 0\n"
     ]
    }
   ],
   "source": [
    "for d in range(len(label_df)):    \n",
    "    _, file_tmp = os.path.split(label_df[d])\n",
    "    file_num = file_tmp.split('_')[0]\n",
    "    \n",
    "    usecols_element = [3, 4, 5, 6] if args.dataset == 20 else [9, 10, 11, 12]\n",
    "    df = pd.read_csv(label_df[d], usecols=usecols_element, skiprows=[0])\n",
    "    \n",
    "    val_list = df.values.tolist()\n",
    "    \n",
    "    for i in range(len(val_list)):\n",
    "    \n",
    "        if args.dataset == 20:\n",
    "            first_folder = val_list[i][0].split('_')[0]\n",
    "            if file_num != first_folder and file_num == 'Sess17':\n",
    "                first_folder = 'Sess17'\n",
    "                first_folder_tmp = first_folder[:-2]+'ion'+first_folder[-2:]\n",
    "                direc = os.path.join(wav_dir, first_folder_tmp)\n",
    "                    \n",
    "                wav_file = os.path.join(direc, first_folder+val_list[i][0][6:]+'.wav')\n",
    "                txt_file = os.path.join(direc, first_folder+val_list[i][0][6:]+'.txt')\n",
    "                   \n",
    "            else:\n",
    "                first_folder_tmp = first_folder[:-2]+'ion'+first_folder[-2:]\n",
    "                direc = os.path.join(wav_dir, first_folder_tmp)\n",
    "                wav_file = os.path.join(direc, val_list[i][0]+'.wav')\n",
    "                txt_file = os.path.join(direc, val_list[i][0]+'.txt')\n",
    "        \n",
    "            if os.path.isfile(wav_file) and os.path.isfile(txt_file):\n",
    "            \n",
    "                emotion = val_list[i][1]\n",
    "                val = val_list[i][2]\n",
    "                aro = val_list[i][-1]\n",
    "                \n",
    "                all_wav.append(wav_file)\n",
    "                with open(txt_file, 'r', encoding='cp949') as f:\n",
    "                    infor = f.readline()\n",
    "                    infor = infor.split('\\n')[0]\n",
    "                all_txt.append(infor)\n",
    "                all_emotion.append(emotion)\n",
    "                all_valence.append(val)\n",
    "                all_arousal.append(aro)\n",
    "            else:\n",
    "                bad_data_for_20 += 1\n",
    "    \n",
    "    \n",
    "        elif args.dataset == 19:\n",
    "            first_folder = val_list[i][0].split('_')[0]\n",
    "            first_folder_tmp = first_folder[:-2]+'ion'+first_folder[-2:]\n",
    "            direc = os.path.join(wav_dir, first_folder_tmp)\n",
    "            speaker_num = val_list[i][0][:-5]\n",
    "\n",
    "            wav_file = os.path.join(direc, speaker_num, val_list[i][0]+'.wav')\n",
    "            txt_file = os.path.join(direc, speaker_num, val_list[i][0]+'.txt')\n",
    "            \n",
    "            if os.path.isfile(wav_file) and os.path.isfile(txt_file):\n",
    "                \n",
    "                emotion = val_list[i][1]\n",
    "                val = val_list[i][2]\n",
    "                aro = val_list[i][-1]\n",
    "                all_wav.append(wav_file)\n",
    "                \n",
    "                with open(txt_file, 'r') as f:\n",
    "                    infor = f.readline()\n",
    "                    infor = infor.split('\\n')[0]\n",
    "                all_txt.append(infor)\n",
    "                all_emotion.append(emotion)\n",
    "                all_valence.append(val)\n",
    "                all_arousal.append(aro)\n",
    "            else:\n",
    "                bad_data_for_19 +=1\n",
    "\n",
    "print('bad for 20', bad_data_for_20)\n",
    "print('bad for 19', bad_data_for_19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13462 13462 13462 13462 13462\n"
     ]
    }
   ],
   "source": [
    "#making label as multi-label classification\n",
    "make_label = torch.FloatTensor(len(all_emotion), 7).random_(1)\n",
    "for k in range(len(all_emotion)):\n",
    "    data_tmp = all_emotion[k]\n",
    "    if len(data_tmp.split(';')) == 1:\n",
    "        make_label[k][emotion_dict.get(data_tmp)] = 1.\n",
    "        \n",
    "    else:\n",
    "        for j in range(len(data_tmp.split(';'))):\n",
    "            make_label[k][emotion_dict.get(data_tmp.split(';')[j])] = 1.\n",
    "            \n",
    "print(len(all_wav), len(all_txt), len(make_label), len(all_valence), len(all_arousal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_fold 5 wav 10770 emo 10770\n",
      "fold = 1, training wav 10770 txt 10770 emo 10770 val 10770 aro 10770\n",
      "test wav 1 txt 2692 emo 2692 val 2692 aro 2692\n"
     ]
    }
   ],
   "source": [
    "## cut as k-fold\n",
    "if args.num_fold == 1:    \n",
    "    train_wav = all_wav[int(len(all_wav)*0.2):]\n",
    "    train_txt = all_txt[int(len(all_wav)*0.2):]\n",
    "    train_emo_label = make_label[int(len(all_wav)*0.2):]\n",
    "    train_valence_label = all_valence[int(len(all_wav)*0.2):]\n",
    "    train_arousal_label = all_arousal[int(len(all_wav)*0.2):]\n",
    "    \n",
    "    test_wav = all_wav[:int(len(all_wav)*0.2)]\n",
    "    test_txt = all_txt[:int(len(all_wav)*0.2)]\n",
    "    test_emo_label = make_label[:int(len(all_wav)*0.2)]\n",
    "    test_valence_label = all_valence[:int(len(all_wav)*0.2)]\n",
    "    test_arousal_label = all_arousal[:int(len(all_wav)*0.2)]\n",
    "    \n",
    "    \n",
    "elif args.num_fold == 2:\n",
    "    train_wav = all_wav[:int(len(all_wav)*0.2)] + all_wav[int(len(all_wav)*0.4):]\n",
    "    train_txt = all_txt[:int(len(all_wav)*0.2)] + all_txt[int(len(all_wav)*0.4):]\n",
    "    train_emo_label = torch.cat((make_label[:int(len(all_wav)*0.2)],make_label[int(len(all_wav)*0.4):]))\n",
    "    train_valence_label = all_valence[:int(len(all_wav)*0.2)] + all_valence[int(len(all_wav)*0.4):]\n",
    "    train_arousal_label = all_arousal[:int(len(all_wav)*0.2)] + all_arousal[int(len(all_wav)*0.4):]\n",
    "    \n",
    "    test_wav = all_wav[int(len(all_wav)*0.2):int(len(all_wav)*0.4)]\n",
    "    test_txt = all_txt[int(len(all_wav)*0.2):int(len(all_wav)*0.4)]\n",
    "    test_emo_label = make_label[int(len(all_wav)*0.2):int(len(all_wav)*0.4)]\n",
    "    test_valence_label = all_valence[int(len(all_wav)*0.2):int(len(all_wav)*0.4)]\n",
    "    test_arousal_label = all_arousal[int(len(all_wav)*0.2):int(len(all_wav)*0.4)]\n",
    "    \n",
    "elif args.num_fold == 3:\n",
    "    train_wav = all_wav[:int(len(all_wav)*0.4)] + all_wav[int(len(all_wav)*0.6):]\n",
    "    train_txt = all_txt[:int(len(all_wav)*0.4)] + all_txt[int(len(all_wav)*0.6):]\n",
    "    train_emo_label = torch.cat((make_label[:int(len(all_wav)*0.4)],make_label[int(len(all_wav)*0.6):]))\n",
    "    train_valence_label = all_valence[:int(len(all_wav)*0.4)] + all_valence[int(len(all_wav)*0.6):]\n",
    "    train_arousal_label = all_arousal[:int(len(all_wav)*0.4)] + all_arousal[int(len(all_wav)*0.6):]\n",
    "    \n",
    "    test_wav = all_wav[int(len(all_wav)*0.4):int(len(all_wav)*0.6)]\n",
    "    test_txt = all_txt[int(len(all_wav)*0.4):int(len(all_wav)*0.6)]\n",
    "    test_emo_label = make_label[int(len(all_wav)*0.4):int(len(all_wav)*0.6)]\n",
    "    test_valence_label = all_valence[int(len(all_wav)*0.4):int(len(all_wav)*0.6)]\n",
    "    test_arousal_label = all_arousal[int(len(all_wav)*0.4):int(len(all_wav)*0.6)]\n",
    "\n",
    "elif args.num_fold == 4:\n",
    "    train_wav = all_wav[:int(len(all_wav)*0.6)] + all_wav[int(len(all_wav)*0.8):]\n",
    "    train_txt = all_txt[:int(len(all_wav)*0.6)] + all_txt[int(len(all_wav)*0.8):]\n",
    "    train_emo_label = torch.cat((make_label[:int(len(all_wav)*0.6)],make_label[int(len(all_wav)*0.8):]))\n",
    "    train_valence_label = all_valence[:int(len(all_wav)*0.6)] + all_valence[int(len(all_wav)*0.8):]\n",
    "    train_arousal_label = all_arousal[:int(len(all_wav)*0.6)] + all_arousal[int(len(all_wav)*0.8):]\n",
    "    \n",
    "    test_wav = all_wav[int(len(all_wav)*0.6):int(len(all_wav)*0.8)]\n",
    "    test_txt = all_txt[int(len(all_wav)*0.6):int(len(all_wav)*0.8)]\n",
    "    test_emo_label = make_label[int(len(all_wav)*0.6):int(len(all_wav)*0.8)]\n",
    "    test_valence_label = all_valence[int(len(all_wav)*0.6):int(len(all_wav)*0.8)]\n",
    "    test_arousal_label = all_arousal[int(len(all_wav)*0.6):int(len(all_wav)*0.8)]\n",
    "\n",
    "elif args.num_fold == 5:\n",
    "    train_wav = all_wav[:int(len(all_wav)*0.8)]\n",
    "    train_txt = all_txt[:int(len(all_wav)*0.8)]\n",
    "    train_emo_label = make_label[:int(len(all_wav)*0.8)]\n",
    "    train_valence_label = all_valence[:int(len(all_wav)*0.8)]\n",
    "    train_arousal_label = all_arousal[:int(len(all_wav)*0.8)]\n",
    "    \n",
    "    test_wav = all_wav[int(len(all_wav)*0.8):]\n",
    "    test_txt = all_txt[int(len(all_wav)*0.8):]\n",
    "    test_emo_label = make_label[int(len(all_wav)*0.8):]\n",
    "    test_valence_label = all_valence[int(len(all_wav)*0.8):]\n",
    "    test_arousal_label = all_arousal[int(len(all_wav)*0.8):]\n",
    "train_fold = (train_wav, train_txt, train_emo_label, train_valence_label, train_arousal_label)\n",
    "test_fold = (test_wav, test_txt, test_emo_label, test_valence_label, test_arousal_label)\n",
    "print('len of train_fold {} wav {} emo {}'.format(len(train_fold), len(train_wav), len(train_emo_label)))\n",
    "\n",
    "print('fold = {}, training wav {} txt {} emo {} val {} aro {}'.format(args.num_fold, len(train_wav), len(train_txt), len(train_emo_label), len(train_valence_label), len(train_arousal_label)))\n",
    "print('test wav {} txt {} emo {} val {} aro {}'.format(args.num_fold, len(test_wav), len(test_txt), len(test_emo_label), len(test_valence_label), len(test_arousal_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wav2vec_classifier(\n",
       "  (extractor): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (valence_classifier): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (arousal_classifier): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "class wav2vec_classifier(nn.Module):\n",
    "    def __init__(self, extractor, num_labels, dropout_prob=0.1):\n",
    "        super(wav2vec_classifier, self).__init__()\n",
    "\n",
    "        self.extractor = extractor\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.nu_labels = num_labels\n",
    "        self.valence_classifier = nn.Linear(512, num_labels)\n",
    "        self.arousal_classifier = nn.Linear(512, num_labels)\n",
    "\n",
    "    def forward(self, wav):\n",
    "        extracted_wav = self.extractor(wav)\n",
    "        \n",
    "        #last_hidden_states = extracted_wav.last_hidden_state\n",
    "        last_hidden_states = extracted_wav.extract_features # B, seq, 512\n",
    "        last_hidden_states = self.dropout(last_hidden_states) #B, seq, 512\n",
    "        \n",
    "        output_valence = self.valence_classifier(last_hidden_states) # B, Seq, 1\n",
    "        output_arousal = self.arousal_classifier(last_hidden_states) # B, Seq, 1\n",
    "                \n",
    "        return output_valence[:, -1], output_arousal[:, -1]\n",
    "\n",
    "\n",
    "extractor = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "ser_model = wav2vec_classifier(extractor, args.regress)\n",
    "ser_model.cuda()\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train 1347 test 337\n"
     ]
    }
   ],
   "source": [
    "#data_loader\n",
    "train_dataset = SpectrogramDataset(path_list=train_fold, max_seq_len=int(args.max_seq_len*16000))\n",
    "test_dataset = SpectrogramDataset(path_list=test_fold, max_seq_len=int(args.max_seq_len*16000))\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "train_dataloader = AudioDataLoader(train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None), num_workers=8, pin_memory=True, sampler=train_sampler)\n",
    "test_dataloader = AudioDataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "print('len of train {} test {}'.format(len(train_dataloader), len(test_dataloader)))\n",
    "optimizer = torch.optim.AdamW(ser_model.parameters(), lr = args.lr,  eps = 1e-8)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한번만 학습했을때가 가장 ccc가 높아서 맨 아래 트레이닝 하기! -> 뭔가 이상함 아래처럼 결과가 동일하게 안나오는중\n",
    "\n",
    "```epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
    "======== Epoch 1 / 5 ========\n",
    "Training...\n",
    "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]\n",
    "  Batch   100  of  1,347.\n",
    "  Batch   200  of  1,347.\n",
    "  Batch   300  of  1,347.\n",
    "  Batch   400  of  1,347.\n",
    "  Batch   500  of  1,347.\n",
    "  Batch   600  of  1,347.\n",
    "  Batch   700  of  1,347.\n",
    "  Batch   800  of  1,347.\n",
    "  Batch   900  of  1,347.\n",
    "  Batch 1,000  of  1,347.\n",
    "  Batch 1,100  of  1,347.\n",
    "  Batch 1,200  of  1,347.\n",
    "  Batch 1,300  of  1,347.\n",
    "  Average training loss: 0.31\n",
    "Running evaluation...\n",
    "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]\n",
    "  val pearsonr: 14.4450\n",
    "  aro pearsonr: 41.1482\n",
    "  val concordance_cc: 4.4356\n",
    "  aro concordance_cc: 31.3617\n",
    "======== Epoch 2 / 5 ========\n",
    "Training...\n",
    "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]\n",
    "  Batch   100  of  1,347.\n",
    "  Batch   200  of  1,347.\n",
    "  Batch   300  of  1,347.\n",
    "  Batch   400  of  1,347.\n",
    "  Batch   500  of  1,347.\n",
    "  Batch   600  of  1,347.\n",
    "  Batch   700  of  1,347.\n",
    "  Batch   800  of  1,347.\n",
    "  Batch   900  of  1,347.\n",
    "  Batch 1,000  of  1,347.\n",
    "  Batch 1,100  of  1,347.\n",
    "  Batch 1,200  of  1,347.\n",
    "  Batch 1,300  of  1,347.\n",
    "  Average training loss: 0.30\n",
    "Running evaluation...\n",
    "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]\n",
    "  val pearsonr: 20.8708\n",
    "  aro pearsonr: 43.4785\n",
    "  val concordance_cc: 7.6908\n",
    "  aro concordance_cc: 27.8480\n",
    "======== Epoch 3 / 5 ========\n",
    "Training...\n",
    "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]\n",
    "  Batch   100  of  1,347.\n",
    "  Batch   200  of  1,347.\n",
    "  Batch   300  of  1,347.\n",
    "  Batch   400  of  1,347.\n",
    "  Batch   500  of  1,347.\n",
    "  Batch   600  of  1,347.\n",
    "  Batch   700  of  1,347.\n",
    "  Batch   800  of  1,347.\n",
    "  Batch   900  of  1,347.\n",
    "  Batch 1,000  of  1,347.\n",
    "  Batch 1,100  of  1,347.\n",
    "  Batch 1,200  of  1,347.\n",
    "  Batch 1,300  of  1,347.\n",
    "  Average training loss: 0.30\n",
    "Running evaluation...\n",
    "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]\n",
    "  val pearsonr: 18.6313\n",
    "  aro pearsonr: 43.7204\n",
    "  val concordance_cc: 8.0243\n",
    "  aro concordance_cc: 19.7746\n",
    "======== Epoch 4 / 5 ========\n",
    "Training...\n",
    "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]\n",
    "  Batch   100  of  1,347.\n",
    "  Batch   200  of  1,347.\n",
    "  Batch   300  of  1,347.\n",
    "  Batch   400  of  1,347.\n",
    "  Batch   500  of  1,347.\n",
    "  Batch   600  of  1,347.\n",
    "  Batch   700  of  1,347.\n",
    "  Batch   800  of  1,347.\n",
    "  Batch   900  of  1,347.\n",
    "  Batch 1,000  of  1,347.\n",
    "  Batch 1,100  of  1,347.\n",
    "  Batch 1,200  of  1,347.\n",
    "  Batch 1,300  of  1,347.\n",
    "  Average training loss: 0.29\n",
    "Running evaluation...\n",
    "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]\n",
    "  val pearsonr: 21.4230\n",
    "  aro pearsonr: 43.5920\n",
    "  val concordance_cc: 10.1175\n",
    "  aro concordance_cc: 22.9818\n",
    "======== Epoch 5 / 5 ========\n",
    "Training...\n",
    "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]\n",
    "  Batch   100  of  1,347.\n",
    "  Batch   200  of  1,347.\n",
    "  Batch   300  of  1,347.\n",
    "  Batch   400  of  1,347.\n",
    "  Batch   500  of  1,347.\n",
    "  Batch   600  of  1,347.\n",
    "  Batch   700  of  1,347.\n",
    "  Batch   800  of  1,347.\n",
    "  Batch   900  of  1,347.\n",
    "  Batch 1,000  of  1,347.\n",
    "  Batch 1,100  of  1,347.\n",
    "  Batch 1,200  of  1,347.\n",
    "  Batch 1,300  of  1,347.\n",
    "  Average training loss: 0.28\n",
    "Running evaluation...\n",
    "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s] ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614a6082db6c440091219bd92169aac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a3c640df614b1aa6076eef3deb61a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,347.\n",
      "  Batch   200  of  1,347.\n",
      "  Batch   300  of  1,347.\n",
      "  Batch   400  of  1,347.\n",
      "  Batch   500  of  1,347.\n",
      "  Batch   600  of  1,347.\n",
      "  Batch   700  of  1,347.\n",
      "  Batch   800  of  1,347.\n",
      "  Batch   900  of  1,347.\n",
      "  Batch 1,000  of  1,347.\n",
      "  Batch 1,100  of  1,347.\n",
      "  Batch 1,200  of  1,347.\n",
      "  Batch 1,300  of  1,347.\n",
      "  Average training loss: 0.70\n",
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14d8985b7b74e6597fc060adaf6eeac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val pearsonr: 2.4684\n",
      "  aro pearsonr: 29.8224\n",
      "  val concordance_cc: 0.5839\n",
      "  aro concordance_cc: 16.4933\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580228e1e24a46949cd91bc341a52bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,347.\n",
      "  Batch   200  of  1,347.\n",
      "  Batch   300  of  1,347.\n",
      "  Batch   400  of  1,347.\n",
      "  Batch   500  of  1,347.\n",
      "  Batch   600  of  1,347.\n",
      "  Batch   700  of  1,347.\n",
      "  Batch   800  of  1,347.\n",
      "  Batch   900  of  1,347.\n",
      "  Batch 1,000  of  1,347.\n",
      "  Batch 1,100  of  1,347.\n",
      "  Batch 1,200  of  1,347.\n",
      "  Batch 1,300  of  1,347.\n",
      "  Average training loss: 0.33\n",
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befc613866e244198f96b2c1504c7bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val pearsonr: 11.4271\n",
      "  aro pearsonr: 34.2324\n",
      "  val concordance_cc: 3.5796\n",
      "  aro concordance_cc: 16.4099\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b37de7b923640199a0b66bc7aef9f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,347.\n",
      "  Batch   200  of  1,347.\n",
      "  Batch   300  of  1,347.\n",
      "  Batch   400  of  1,347.\n",
      "  Batch   500  of  1,347.\n",
      "  Batch   600  of  1,347.\n",
      "  Batch   700  of  1,347.\n",
      "  Batch   800  of  1,347.\n",
      "  Batch   900  of  1,347.\n",
      "  Batch 1,000  of  1,347.\n",
      "  Batch 1,100  of  1,347.\n",
      "  Batch 1,200  of  1,347.\n",
      "  Batch 1,300  of  1,347.\n",
      "  Average training loss: 0.32\n",
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be8d05885534e5598c1c62213350e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val pearsonr: 16.1630\n",
      "  aro pearsonr: 42.5671\n",
      "  val concordance_cc: 4.8128\n",
      "  aro concordance_cc: 29.3892\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc34ff4b5a524a69bd5e93356b620293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,347.\n",
      "  Batch   200  of  1,347.\n",
      "  Batch   300  of  1,347.\n",
      "  Batch   400  of  1,347.\n",
      "  Batch   500  of  1,347.\n",
      "  Batch   600  of  1,347.\n",
      "  Batch   700  of  1,347.\n",
      "  Batch   800  of  1,347.\n",
      "  Batch   900  of  1,347.\n",
      "  Batch 1,000  of  1,347.\n",
      "  Batch 1,100  of  1,347.\n",
      "  Batch 1,200  of  1,347.\n",
      "  Batch 1,300  of  1,347.\n",
      "  Average training loss: 0.30\n",
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4688c14fa04f128d4b30c9c258b362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val pearsonr: 18.5952\n",
      "  aro pearsonr: 45.0299\n",
      "  val concordance_cc: 6.6173\n",
      "  aro concordance_cc: 33.1662\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4506e9ad6557490eab137fdee6740fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_dataloader:   0%|          | 0/1347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,347.\n",
      "  Batch   200  of  1,347.\n",
      "  Batch   300  of  1,347.\n",
      "  Batch   400  of  1,347.\n",
      "  Batch   500  of  1,347.\n",
      "  Batch   600  of  1,347.\n",
      "  Batch   700  of  1,347.\n",
      "  Batch   800  of  1,347.\n",
      "  Batch   900  of  1,347.\n",
      "  Batch 1,000  of  1,347.\n",
      "  Batch 1,100  of  1,347.\n",
      "  Batch 1,200  of  1,347.\n",
      "  Batch 1,300  of  1,347.\n",
      "  Average training loss: 0.30\n",
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9fbae1c315409bafc461d263c40359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_dataloader:   0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val pearsonr: 16.5983\n",
      "  aro pearsonr: 45.3229\n",
      "  val concordance_cc: 6.2806\n",
      "  aro concordance_cc: 24.0727\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch_i in tqdm(range(args.epochs), desc = 'epoch', total = args.epochs):\n",
    "\n",
    "    #train\n",
    "    \n",
    "    ser_model.train()\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, args.epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    total_loss = 0\n",
    "    train_acc_sum = 0\n",
    "    train_loss = []\n",
    "    for step, (data, val, aro) in tqdm(enumerate(train_dataloader), desc = 'train_dataloader', total = len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        speech = data.cuda()\n",
    "        vals = val.cuda()\n",
    "        aros = aro.cuda()\n",
    "\n",
    "        val_outputs, aro_outputs = ser_model(speech)\n",
    "        loss_val = criterion(val_outputs.squeeze(dim=-1), vals.squeeze(dim=-1))\n",
    "        loss_aro = criterion(aro_outputs.squeeze(dim=-1), aros.squeeze(dim=-1))\n",
    "        loss = loss_val + loss_aro\n",
    "        total_loss += loss.item()\n",
    "        train_loss.append(total_loss/(step+1))        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'  Average training loss: {avg_train_loss:.2f}')\n",
    "\n",
    "    #validation\n",
    "    with torch.no_grad():\n",
    "        ser_model.eval()\n",
    "        print('Running evaluation...')\n",
    "\n",
    "        val_acc_sum = 0\n",
    "        \n",
    "        val_targets_list = []\n",
    "        aro_targets_list = []\n",
    "        \n",
    "        val_preds_list = []\n",
    "        aro_preds_list = []\n",
    "        for data, val, aro in tqdm(test_dataloader, desc = 'test_dataloader', total = len(test_dataloader)):\n",
    "\n",
    "            speech = data.cuda()\n",
    "            vals = val.cuda()\n",
    "            aros = aro.cuda()\n",
    "            val_outputs, aro_outputs = ser_model(speech)\n",
    "            \n",
    "            val_targets = vals.detach().cpu().numpy()\n",
    "            val_targets = np.squeeze(val_targets)\n",
    "            val_preds = val_outputs.detach().cpu().numpy()\n",
    "            val_preds = np.squeeze(val_preds)\n",
    "            val_targets_list.append(val_targets)\n",
    "            val_preds_list.append(val_preds)\n",
    "            \n",
    "            \n",
    "            aro_targets = aros.detach().cpu().numpy()\n",
    "            aro_targets = np.squeeze(aro_targets)\n",
    "            aro_preds = aro_outputs.detach().cpu().numpy()\n",
    "            aro_preds = np.squeeze(aro_preds)\n",
    "            aro_targets_list.append(aro_targets)\n",
    "            aro_preds_list.append(aro_preds)\n",
    "            \n",
    "\n",
    "        val_targets_list = np.concatenate(val_targets_list, axis = 0)\n",
    "        val_preds_list = np.concatenate(val_preds_list, axis = 0)\n",
    "        \n",
    "        val_pearsonrs = pearsonr(val_preds_list, val_targets_list)[0] * 100.0\n",
    "        \n",
    "        t_val_targets_list = torch.tensor(val_targets_list)\n",
    "        t_val_preds_list = torch.tensor(val_preds_list)\n",
    "        val_concordance_cc = concordance_cc(t_val_preds_list, t_val_targets_list)[0] * 100.0\n",
    "\n",
    "        aro_targets_list = np.concatenate(aro_targets_list, axis = 0)\n",
    "        aro_preds_list = np.concatenate(aro_preds_list, axis = 0)\n",
    "        aro_pearsonrs = pearsonr(aro_preds_list, aro_targets_list)[0] * 100.0\n",
    "        \n",
    "        t_aro_targets_list = torch.tensor(aro_targets_list)\n",
    "        t_aro_preds_list = torch.tensor(aro_preds_list)\n",
    "        aro_concordance_cc = concordance_cc(t_aro_preds_list, t_aro_targets_list)[0] * 100.0\n",
    "\n",
    "    print(f'  val pearsonr: {val_pearsonrs:.4f}')\n",
    "    print(f'  aro pearsonr: {aro_pearsonrs:.4f}')\n",
    "    print(f'  val concordance_cc: {val_concordance_cc:.4f}')\n",
    "    print(f'  aro concordance_cc: {aro_concordance_cc:.4f}')\n",
    "    \n",
    "    PATH = f'./model/wac2vec_arousal_valence_model_epoch_{epoch_i+1}.pt'\n",
    "    torch.save(ser_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
