{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== Dataset Class ==========\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, list_file, wav_dir, label_dir, max_seq_len=250, resample_sr=2000):\n",
    "        self.video_names = self.read_video_list(list_file)\n",
    "        self.wav_dir = Path(wav_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.resample_sr = resample_sr\n",
    "\n",
    "        self.label_files = self.prefetch_label_files()\n",
    "        self.data_size, self.index_to_chunk, self.labels = self.prefetch_and_index()\n",
    "\n",
    "    def read_video_list(self, list_path):\n",
    "        with open(list_path, 'r') as f:\n",
    "            return [line.strip() for line in f]\n",
    "\n",
    "    def prefetch_label_files(self):\n",
    "        name_set = set(self.video_names)\n",
    "        label_files = defaultdict(list)\n",
    "\n",
    "        for label_file in self.label_dir.glob(\"**/*.json\"):\n",
    "            file_name = label_file.stem\n",
    "            annotator_id_index = len(file_name) - file_name[::-1].find(\"_\") - 1\n",
    "            video_name = file_name[:annotator_id_index]\n",
    "\n",
    "            if video_name in name_set:\n",
    "                label_files[video_name].append(label_file)\n",
    "\n",
    "        return label_files\n",
    "\n",
    "    def extract_label(self, video_name):\n",
    "        label_files = self.label_files.get(video_name, [])\n",
    "        labels = []\n",
    "\n",
    "        for label_file in label_files:\n",
    "            with open(label_file, \"r\") as rf:\n",
    "                data = json.load(rf)\n",
    "            video_length = math.ceil(data[\"metadata\"][\"length\"])\n",
    "            annotator_label = np.zeros(video_length)\n",
    "\n",
    "            for timeline in data[\"timelines\"]:\n",
    "                for t in range(timeline[\"start\"], timeline[\"end\"] + 1):\n",
    "                    if t < video_length:\n",
    "                        annotator_label[t] = 1\n",
    "\n",
    "            labels.append(annotator_label)\n",
    "\n",
    "        return np.array(labels)\n",
    "\n",
    "    def prefetch_and_index(self):\n",
    "        index = 0\n",
    "        index_to_chunk = {}\n",
    "        all_labels = {}\n",
    "\n",
    "        for video_name in self.video_names:\n",
    "            labels = self.extract_label(video_name)\n",
    "            if labels is None or len(labels) == 0 or len(labels[0]) == 0:\n",
    "                print(f\"⚠️ Skipping {video_name}: no valid labels\")\n",
    "                continue\n",
    "\n",
    "            all_labels[video_name] = labels\n",
    "            chunk_count = math.ceil(len(labels[0]) / self.max_seq_len)\n",
    "\n",
    "            for chunk_index in range(chunk_count):\n",
    "                index_to_chunk[index + chunk_index] = (video_name, chunk_index)\n",
    "\n",
    "            index += chunk_count\n",
    "\n",
    "        return index, index_to_chunk, all_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_name, chunk_index = self.index_to_chunk[index]\n",
    "        start = chunk_index * self.max_seq_len\n",
    "        end = start + self.max_seq_len\n",
    "\n",
    "        labels = self.labels[video_name][:, start:end]\n",
    "\n",
    "        try:\n",
    "            wav_path = self.wav_dir / f\"{video_name}.wav\"\n",
    "            audio_data, sr = torchaudio.load(str(wav_path))\n",
    "        except:\n",
    "            print(f\"🚫 Error loading: {wav_path}\")\n",
    "            return None\n",
    "\n",
    "        resampler = T.Resample(sr, self.resample_sr, dtype=audio_data.dtype)\n",
    "        audio_data = resampler(audio_data)\n",
    "        audio_data = torch.mean(audio_data, axis=0).numpy()\n",
    "\n",
    "        # Crop or pad the audio\n",
    "        audio_data = audio_data[start * self.resample_sr : end * self.resample_sr]\n",
    "\n",
    "        total_segments = self.max_seq_len\n",
    "        num_frames_per_segment = len(audio_data) // total_segments\n",
    "        audio_list = []\n",
    "\n",
    "        for i in range(0, len(audio_data) - num_frames_per_segment + 1, num_frames_per_segment):\n",
    "            segment = audio_data[i : i + num_frames_per_segment]\n",
    "\n",
    "            if len(segment) < self.resample_sr:\n",
    "                pad = self.resample_sr - len(segment)\n",
    "                segment = np.pad(segment, (0, pad), mode=\"constant\")\n",
    "            elif len(segment) > self.resample_sr:\n",
    "                segment = segment[:self.resample_sr]\n",
    "\n",
    "            audio_list.append(segment)\n",
    "\n",
    "        audio_array = np.vstack(audio_list)\n",
    "\n",
    "        # Convert labels\n",
    "        labels = torch.from_numpy(labels).squeeze(0)\n",
    "        labels = torch.sum(labels, dim=0)\n",
    "        labels = torch.min(labels, torch.ones(labels.shape[0], device=labels.device))\n",
    "\n",
    "        return video_name, audio_array, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Train dataset size: 401\n",
      "🎬 AV Test dataset size: 140\n",
      "🎬 MUL Test dataset size: 163\n"
     ]
    }
   ],
   "source": [
    "# ========== Dataset ==========\n",
    "sd_train_av = SummaryDataset(\n",
    "    list_file=\"/home/jovyan/EmotionDetection/video_data/av_train.txt\",\n",
    "    wav_dir=\"/home/jovyan/EmotionDetection/audio_data/av_train\",\n",
    "    label_dir=\"/home/jovyan/EmotionDetection/video_data/label\"\n",
    ")\n",
    "\n",
    "sd_test_av = SummaryDataset(\n",
    "    list_file=\"/home/jovyan/EmotionDetection/video_data/av_test.txt\",\n",
    "    wav_dir=\"/home/jovyan/EmotionDetection/audio_data/av_test\",\n",
    "    label_dir=\"/home/jovyan/EmotionDetection/video_data/label\"\n",
    ")\n",
    "\n",
    "sd_test_mul = SummaryDataset(\n",
    "    list_file=\"/home/jovyan/EmotionDetection/video_data/mul_test.txt\",\n",
    "    wav_dir=\"/home/jovyan/EmotionDetection/audio_data/mul_test\",\n",
    "    label_dir=\"/home/jovyan/EmotionDetection/video_data/label\"\n",
    ")\n",
    "\n",
    "# ========== DataLoader ==========\n",
    "# Custom collate function to skip None and unpack correctly\n",
    "def safe_collate(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return tuple(zip(*batch))  # returns (video_names, inputs, labels)\n",
    "\n",
    "dl_train_av = DataLoader(\n",
    "    sd_train,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=safe_collate)\n",
    "\n",
    "dl_test_av = DataLoader(\n",
    "    sd_test_av,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=safe_collate)\n",
    "\n",
    "dl_test_mul = DataLoader(\n",
    "    sd_test_mul,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=safe_collate)\n",
    "\n",
    "# ========== Info ==========\n",
    "print(f\"📦 Train dataset size: {len(sd_train_av)}\")\n",
    "print(f\"🎬 AV Test dataset size: {len(sd_test_av)}\")\n",
    "print(f\"🎬 MUL Test dataset size: {len(sd_test_mul)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load\n",
    "# Extract Emotional Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 07:36:19.909488: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-13 07:36:20.262358: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-13 07:36:20.262437: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-13 07:36:20.262499: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-13 07:36:20.288485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-13 07:36:23.399324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: cp949 -*-\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import argparse \n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터 노트북에서 명령행 인자 대신 변수를 직접 설정합니다.\n",
    "args = argparse.Namespace()\n",
    "args.num_labels = 7\n",
    "args.seed = 1234\n",
    "args.lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b793dcc07a24a2bb672be474cfe4d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca9ed49362a49d09a0a2f238afca184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wav2vec_classifier(\n",
       "  (extractor): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class wav2vec_classifier(nn.Module):\n",
    "    def __init__(self, extractor, num_labels, dropout_prob=0.1):\n",
    "        super(wav2vec_classifier, self).__init__()\n",
    "\n",
    "        self.extractor = extractor\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.nu_labels = num_labels\n",
    "        self.classifier = nn.Linear(512, num_labels)\n",
    "        #self.softmax = F.softmax()\n",
    "\n",
    "    def forward(self, wav):\n",
    "        extracted_wav = self.extractor(wav)\n",
    "        \n",
    "        #last_hidden_states = extracted_wav.last_hidden_state\n",
    "        last_hidden_states = extracted_wav.extract_features\n",
    "        last_hidden_states = self.dropout(last_hidden_states)\n",
    "        output = self.classifier(last_hidden_states)\n",
    "           \n",
    "#         hidden = extracted_wav.hidden_states[-1]\n",
    "        out_last = last_hidden_states[:, -1, :]\n",
    "        \n",
    "        return F.softmax(output[:, -1], dim=-1), last_hidden_states, out_last\n",
    "\n",
    "\n",
    "extractor = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "emotion_model = wav2vec_classifier(extractor, args.num_labels)\n",
    "emotion_model.load_state_dict(torch.load('wav2vec_affective_audio/wac2vec_emotion_classification_model.pt'))\n",
    "emotion_model.cuda()\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(emotion_model.parameters(), lr = args.lr,  eps = 1e-8)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd7d6ee50f048e2bcbdeb218b39e404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 2000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([251, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([251, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([260, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([251, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "# train emotion data\n",
    "emotion_waveform_av_train = []\n",
    "\n",
    "for batch in tqdm_notebook(dl_train_av, total=len(dl_train_av), desc='Processing dataset'):\n",
    "    if batch is None:\n",
    "        continue\n",
    "\n",
    "    video_names, inputs, labels = batch\n",
    "    video_name = video_names[0]\n",
    "    input_array = inputs[0]  # numpy array\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emotion_model.eval()\n",
    "        input_tensor = torch.tensor(input_array, dtype=torch.float32).to(device)  # 변환\n",
    "        print(input_tensor.shape)\n",
    "\n",
    "        outputs, last_hidden_states, out_last = emotion_model(input_tensor)\n",
    "        emotion_waveform_av_train.append(out_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Move all tensors to CPU and convert to numpy\n",
    "cpu_array = [t.cpu().numpy() for t in emotion_waveform_av_train]\n",
    "\n",
    "# Save to .npy as object (since shapes might vary)\n",
    "np.save(\"waveform_emotion_av_train.npy\", np.array(cpu_array, dtype=object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: torch.Size([250, 512])\n",
      "1: torch.Size([250, 512])\n",
      "2: torch.Size([250, 512])\n",
      "3: torch.Size([250, 512])\n",
      "4: torch.Size([250, 512])\n",
      "5: torch.Size([250, 512])\n",
      "6: torch.Size([250, 512])\n",
      "7: torch.Size([250, 512])\n",
      "8: torch.Size([250, 512])\n",
      "9: torch.Size([250, 512])\n",
      "10: torch.Size([250, 512])\n",
      "11: torch.Size([250, 512])\n",
      "12: torch.Size([250, 512])\n",
      "13: torch.Size([250, 512])\n",
      "14: torch.Size([250, 512])\n",
      "15: torch.Size([250, 512])\n",
      "16: torch.Size([250, 512])\n",
      "17: torch.Size([250, 512])\n",
      "18: torch.Size([250, 512])\n",
      "19: torch.Size([250, 512])\n",
      "20: torch.Size([250, 512])\n",
      "21: torch.Size([250, 512])\n",
      "22: torch.Size([250, 512])\n",
      "23: torch.Size([250, 512])\n",
      "24: torch.Size([250, 512])\n",
      "25: torch.Size([250, 512])\n",
      "26: torch.Size([250, 512])\n",
      "27: torch.Size([250, 512])\n",
      "28: torch.Size([250, 512])\n",
      "29: torch.Size([250, 512])\n",
      "30: torch.Size([250, 512])\n",
      "31: torch.Size([250, 512])\n",
      "32: torch.Size([250, 512])\n",
      "33: torch.Size([250, 512])\n",
      "34: torch.Size([250, 512])\n",
      "35: torch.Size([250, 512])\n",
      "36: torch.Size([250, 512])\n",
      "37: torch.Size([250, 512])\n",
      "38: torch.Size([250, 512])\n",
      "39: torch.Size([250, 512])\n",
      "40: torch.Size([250, 512])\n",
      "41: torch.Size([250, 512])\n",
      "42: torch.Size([250, 512])\n",
      "43: torch.Size([250, 512])\n",
      "44: torch.Size([250, 512])\n",
      "45: torch.Size([250, 512])\n",
      "46: torch.Size([250, 512])\n",
      "47: torch.Size([250, 512])\n",
      "48: torch.Size([250, 512])\n",
      "49: torch.Size([250, 512])\n",
      "50: torch.Size([250, 512])\n",
      "51: torch.Size([250, 512])\n",
      "52: torch.Size([250, 512])\n",
      "53: torch.Size([250, 512])\n",
      "54: torch.Size([250, 512])\n",
      "55: torch.Size([250, 512])\n",
      "56: torch.Size([250, 512])\n",
      "57: torch.Size([250, 512])\n",
      "58: torch.Size([250, 512])\n",
      "59: torch.Size([250, 512])\n",
      "60: torch.Size([250, 512])\n",
      "61: torch.Size([250, 512])\n",
      "62: torch.Size([250, 512])\n",
      "63: torch.Size([250, 512])\n",
      "64: torch.Size([250, 512])\n",
      "65: torch.Size([250, 512])\n",
      "66: torch.Size([250, 512])\n",
      "67: torch.Size([250, 512])\n",
      "68: torch.Size([250, 512])\n",
      "69: torch.Size([250, 512])\n",
      "70: torch.Size([250, 512])\n",
      "71: torch.Size([250, 512])\n",
      "72: torch.Size([250, 512])\n",
      "73: torch.Size([250, 512])\n",
      "74: torch.Size([250, 512])\n",
      "75: torch.Size([250, 512])\n",
      "76: torch.Size([250, 512])\n",
      "77: torch.Size([250, 512])\n",
      "78: torch.Size([250, 512])\n",
      "79: torch.Size([250, 512])\n",
      "80: torch.Size([250, 512])\n",
      "81: torch.Size([250, 512])\n",
      "82: torch.Size([250, 512])\n",
      "83: torch.Size([250, 512])\n",
      "84: torch.Size([250, 512])\n",
      "85: torch.Size([250, 512])\n",
      "86: torch.Size([250, 512])\n",
      "87: torch.Size([250, 512])\n",
      "88: torch.Size([250, 512])\n",
      "89: torch.Size([250, 512])\n",
      "90: torch.Size([250, 512])\n",
      "91: torch.Size([250, 512])\n",
      "92: torch.Size([250, 512])\n",
      "93: torch.Size([250, 512])\n",
      "94: torch.Size([250, 512])\n",
      "95: torch.Size([250, 512])\n",
      "96: torch.Size([250, 512])\n",
      "97: torch.Size([250, 512])\n",
      "98: torch.Size([250, 512])\n",
      "99: torch.Size([250, 512])\n",
      "100: torch.Size([250, 512])\n",
      "101: torch.Size([250, 512])\n",
      "102: torch.Size([250, 512])\n",
      "103: torch.Size([250, 512])\n",
      "104: torch.Size([250, 512])\n",
      "105: torch.Size([250, 512])\n",
      "106: torch.Size([250, 512])\n",
      "107: torch.Size([250, 512])\n",
      "108: torch.Size([250, 512])\n",
      "109: torch.Size([250, 512])\n",
      "110: torch.Size([250, 512])\n",
      "111: torch.Size([250, 512])\n",
      "112: torch.Size([250, 512])\n",
      "113: torch.Size([250, 512])\n",
      "114: torch.Size([250, 512])\n",
      "115: torch.Size([250, 512])\n",
      "116: torch.Size([250, 512])\n",
      "117: torch.Size([250, 512])\n",
      "118: torch.Size([250, 512])\n",
      "119: torch.Size([250, 512])\n",
      "120: torch.Size([250, 512])\n",
      "121: torch.Size([250, 512])\n",
      "122: torch.Size([250, 512])\n",
      "123: torch.Size([250, 512])\n",
      "124: torch.Size([250, 512])\n",
      "125: torch.Size([250, 512])\n",
      "126: torch.Size([251, 512])\n",
      "127: torch.Size([250, 512])\n",
      "128: torch.Size([250, 512])\n",
      "129: torch.Size([250, 512])\n",
      "130: torch.Size([250, 512])\n",
      "131: torch.Size([250, 512])\n",
      "132: torch.Size([250, 512])\n",
      "133: torch.Size([250, 512])\n",
      "134: torch.Size([250, 512])\n",
      "135: torch.Size([250, 512])\n",
      "136: torch.Size([250, 512])\n",
      "137: torch.Size([250, 512])\n",
      "138: torch.Size([250, 512])\n",
      "139: torch.Size([250, 512])\n",
      "140: torch.Size([250, 512])\n",
      "141: torch.Size([250, 512])\n",
      "142: torch.Size([250, 512])\n",
      "143: torch.Size([250, 512])\n",
      "144: torch.Size([250, 512])\n",
      "145: torch.Size([250, 512])\n",
      "146: torch.Size([250, 512])\n",
      "147: torch.Size([251, 512])\n",
      "148: torch.Size([250, 512])\n",
      "149: torch.Size([250, 512])\n",
      "150: torch.Size([250, 512])\n",
      "151: torch.Size([250, 512])\n",
      "152: torch.Size([250, 512])\n",
      "153: torch.Size([250, 512])\n",
      "154: torch.Size([250, 512])\n",
      "155: torch.Size([250, 512])\n",
      "156: torch.Size([250, 512])\n",
      "157: torch.Size([250, 512])\n",
      "158: torch.Size([250, 512])\n",
      "159: torch.Size([250, 512])\n",
      "160: torch.Size([250, 512])\n",
      "161: torch.Size([250, 512])\n",
      "162: torch.Size([250, 512])\n",
      "163: torch.Size([250, 512])\n",
      "164: torch.Size([250, 512])\n",
      "165: torch.Size([250, 512])\n",
      "166: torch.Size([250, 512])\n",
      "167: torch.Size([250, 512])\n",
      "168: torch.Size([250, 512])\n",
      "169: torch.Size([250, 512])\n",
      "170: torch.Size([250, 512])\n",
      "171: torch.Size([250, 512])\n",
      "172: torch.Size([250, 512])\n",
      "173: torch.Size([250, 512])\n",
      "174: torch.Size([250, 512])\n",
      "175: torch.Size([250, 512])\n",
      "176: torch.Size([250, 512])\n",
      "177: torch.Size([250, 512])\n",
      "178: torch.Size([250, 512])\n",
      "179: torch.Size([250, 512])\n",
      "180: torch.Size([250, 512])\n",
      "181: torch.Size([250, 512])\n",
      "182: torch.Size([250, 512])\n",
      "183: torch.Size([250, 512])\n",
      "184: torch.Size([250, 512])\n",
      "185: torch.Size([250, 512])\n",
      "186: torch.Size([250, 512])\n",
      "187: torch.Size([250, 512])\n",
      "188: torch.Size([250, 512])\n",
      "189: torch.Size([250, 512])\n",
      "190: torch.Size([250, 512])\n",
      "191: torch.Size([250, 512])\n",
      "192: torch.Size([250, 512])\n",
      "193: torch.Size([250, 512])\n",
      "194: torch.Size([250, 512])\n",
      "195: torch.Size([250, 512])\n",
      "196: torch.Size([250, 512])\n",
      "197: torch.Size([250, 512])\n",
      "198: torch.Size([250, 512])\n",
      "199: torch.Size([250, 512])\n",
      "200: torch.Size([250, 512])\n",
      "201: torch.Size([250, 512])\n",
      "202: torch.Size([250, 512])\n",
      "203: torch.Size([250, 512])\n",
      "204: torch.Size([260, 512])\n",
      "205: torch.Size([250, 512])\n",
      "206: torch.Size([250, 512])\n",
      "207: torch.Size([250, 512])\n",
      "208: torch.Size([250, 512])\n",
      "209: torch.Size([250, 512])\n",
      "210: torch.Size([250, 512])\n",
      "211: torch.Size([250, 512])\n",
      "212: torch.Size([250, 512])\n",
      "213: torch.Size([250, 512])\n",
      "214: torch.Size([250, 512])\n",
      "215: torch.Size([250, 512])\n",
      "216: torch.Size([250, 512])\n",
      "217: torch.Size([250, 512])\n",
      "218: torch.Size([250, 512])\n",
      "219: torch.Size([250, 512])\n",
      "220: torch.Size([250, 512])\n",
      "221: torch.Size([250, 512])\n",
      "222: torch.Size([250, 512])\n",
      "223: torch.Size([250, 512])\n",
      "224: torch.Size([250, 512])\n",
      "225: torch.Size([250, 512])\n",
      "226: torch.Size([250, 512])\n",
      "227: torch.Size([250, 512])\n",
      "228: torch.Size([250, 512])\n",
      "229: torch.Size([250, 512])\n",
      "230: torch.Size([250, 512])\n",
      "231: torch.Size([250, 512])\n",
      "232: torch.Size([250, 512])\n",
      "233: torch.Size([250, 512])\n",
      "234: torch.Size([250, 512])\n",
      "235: torch.Size([250, 512])\n",
      "236: torch.Size([250, 512])\n",
      "237: torch.Size([250, 512])\n",
      "238: torch.Size([250, 512])\n",
      "239: torch.Size([250, 512])\n",
      "240: torch.Size([250, 512])\n",
      "241: torch.Size([250, 512])\n",
      "242: torch.Size([250, 512])\n",
      "243: torch.Size([250, 512])\n",
      "244: torch.Size([250, 512])\n",
      "245: torch.Size([250, 512])\n",
      "246: torch.Size([250, 512])\n",
      "247: torch.Size([250, 512])\n",
      "248: torch.Size([250, 512])\n",
      "249: torch.Size([250, 512])\n",
      "250: torch.Size([250, 512])\n",
      "251: torch.Size([250, 512])\n",
      "252: torch.Size([250, 512])\n",
      "253: torch.Size([250, 512])\n",
      "254: torch.Size([250, 512])\n",
      "255: torch.Size([250, 512])\n",
      "256: torch.Size([250, 512])\n",
      "257: torch.Size([250, 512])\n",
      "258: torch.Size([250, 512])\n",
      "259: torch.Size([250, 512])\n",
      "260: torch.Size([250, 512])\n",
      "261: torch.Size([250, 512])\n",
      "262: torch.Size([250, 512])\n",
      "263: torch.Size([250, 512])\n",
      "264: torch.Size([250, 512])\n",
      "265: torch.Size([250, 512])\n",
      "266: torch.Size([250, 512])\n",
      "267: torch.Size([250, 512])\n",
      "268: torch.Size([250, 512])\n",
      "269: torch.Size([250, 512])\n",
      "270: torch.Size([250, 512])\n",
      "271: torch.Size([250, 512])\n",
      "272: torch.Size([250, 512])\n",
      "273: torch.Size([250, 512])\n",
      "274: torch.Size([250, 512])\n",
      "275: torch.Size([250, 512])\n",
      "276: torch.Size([250, 512])\n",
      "277: torch.Size([250, 512])\n",
      "278: torch.Size([250, 512])\n",
      "279: torch.Size([250, 512])\n",
      "280: torch.Size([250, 512])\n",
      "281: torch.Size([250, 512])\n",
      "282: torch.Size([250, 512])\n",
      "283: torch.Size([250, 512])\n",
      "284: torch.Size([250, 512])\n",
      "285: torch.Size([250, 512])\n",
      "286: torch.Size([250, 512])\n",
      "287: torch.Size([250, 512])\n",
      "288: torch.Size([250, 512])\n",
      "289: torch.Size([250, 512])\n",
      "290: torch.Size([250, 512])\n",
      "291: torch.Size([250, 512])\n",
      "292: torch.Size([250, 512])\n",
      "293: torch.Size([250, 512])\n",
      "294: torch.Size([250, 512])\n",
      "295: torch.Size([250, 512])\n",
      "296: torch.Size([250, 512])\n",
      "297: torch.Size([250, 512])\n",
      "298: torch.Size([250, 512])\n",
      "299: torch.Size([250, 512])\n",
      "300: torch.Size([250, 512])\n",
      "301: torch.Size([250, 512])\n",
      "302: torch.Size([250, 512])\n",
      "303: torch.Size([250, 512])\n",
      "304: torch.Size([250, 512])\n",
      "305: torch.Size([250, 512])\n",
      "306: torch.Size([250, 512])\n",
      "307: torch.Size([250, 512])\n",
      "308: torch.Size([250, 512])\n",
      "309: torch.Size([250, 512])\n",
      "310: torch.Size([250, 512])\n",
      "311: torch.Size([250, 512])\n",
      "312: torch.Size([250, 512])\n",
      "313: torch.Size([250, 512])\n",
      "314: torch.Size([250, 512])\n",
      "315: torch.Size([250, 512])\n",
      "316: torch.Size([250, 512])\n",
      "317: torch.Size([250, 512])\n",
      "318: torch.Size([250, 512])\n",
      "319: torch.Size([250, 512])\n",
      "320: torch.Size([250, 512])\n",
      "321: torch.Size([250, 512])\n",
      "322: torch.Size([250, 512])\n",
      "323: torch.Size([250, 512])\n",
      "324: torch.Size([250, 512])\n",
      "325: torch.Size([250, 512])\n",
      "326: torch.Size([250, 512])\n",
      "327: torch.Size([250, 512])\n",
      "328: torch.Size([250, 512])\n",
      "329: torch.Size([250, 512])\n",
      "330: torch.Size([250, 512])\n",
      "331: torch.Size([250, 512])\n",
      "332: torch.Size([250, 512])\n",
      "333: torch.Size([250, 512])\n",
      "334: torch.Size([250, 512])\n",
      "335: torch.Size([250, 512])\n",
      "336: torch.Size([250, 512])\n",
      "337: torch.Size([250, 512])\n",
      "338: torch.Size([250, 512])\n",
      "339: torch.Size([250, 512])\n",
      "340: torch.Size([250, 512])\n",
      "341: torch.Size([250, 512])\n",
      "342: torch.Size([250, 512])\n",
      "343: torch.Size([250, 512])\n",
      "344: torch.Size([250, 512])\n",
      "345: torch.Size([250, 512])\n",
      "346: torch.Size([250, 512])\n",
      "347: torch.Size([250, 512])\n",
      "348: torch.Size([250, 512])\n",
      "349: torch.Size([250, 512])\n",
      "350: torch.Size([250, 512])\n",
      "351: torch.Size([250, 512])\n",
      "352: torch.Size([250, 512])\n",
      "353: torch.Size([250, 512])\n",
      "354: torch.Size([250, 512])\n",
      "355: torch.Size([250, 512])\n",
      "356: torch.Size([250, 512])\n",
      "357: torch.Size([250, 512])\n",
      "358: torch.Size([250, 512])\n",
      "359: torch.Size([250, 512])\n",
      "360: torch.Size([250, 512])\n",
      "361: torch.Size([250, 512])\n",
      "362: torch.Size([250, 512])\n",
      "363: torch.Size([250, 512])\n",
      "364: torch.Size([250, 512])\n",
      "365: torch.Size([250, 512])\n",
      "366: torch.Size([250, 512])\n",
      "367: torch.Size([251, 512])\n",
      "368: torch.Size([250, 512])\n",
      "369: torch.Size([250, 512])\n",
      "370: torch.Size([250, 512])\n",
      "371: torch.Size([250, 512])\n",
      "372: torch.Size([250, 512])\n",
      "373: torch.Size([250, 512])\n",
      "374: torch.Size([250, 512])\n",
      "375: torch.Size([250, 512])\n",
      "376: torch.Size([250, 512])\n",
      "377: torch.Size([250, 512])\n",
      "378: torch.Size([250, 512])\n",
      "379: torch.Size([250, 512])\n",
      "380: torch.Size([250, 512])\n",
      "381: torch.Size([250, 512])\n",
      "382: torch.Size([250, 512])\n",
      "383: torch.Size([250, 512])\n",
      "384: torch.Size([250, 512])\n",
      "385: torch.Size([250, 512])\n",
      "386: torch.Size([250, 512])\n",
      "387: torch.Size([250, 512])\n",
      "388: torch.Size([250, 512])\n",
      "389: torch.Size([250, 512])\n",
      "390: torch.Size([250, 512])\n",
      "391: torch.Size([250, 512])\n",
      "392: torch.Size([250, 512])\n",
      "393: torch.Size([250, 512])\n",
      "394: torch.Size([250, 512])\n",
      "395: torch.Size([250, 512])\n",
      "396: torch.Size([250, 512])\n",
      "397: torch.Size([250, 512])\n",
      "398: torch.Size([250, 512])\n",
      "399: torch.Size([250, 512])\n",
      "400: torch.Size([250, 512])\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(emotion_waveform_av_train):\n",
    "    print(f\"{i}: {t.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea1ffd044d0435a93bec0b28b43da48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([254, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([253, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([254, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "# train emotion data\n",
    "waveform_emotion_av_test = []\n",
    "\n",
    "for batch in tqdm_notebook(dl_test_av, total=len(dl_test_av), desc='Processing dataset'):\n",
    "    if batch is None:\n",
    "        continue\n",
    "\n",
    "    video_names, inputs, labels = batch\n",
    "    video_name = video_names[0]\n",
    "    input_array = inputs[0]  # numpy array\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emotion_model.eval()\n",
    "        input_tensor = torch.tensor(input_array, dtype=torch.float32).to(device)  # 변환\n",
    "        print(input_tensor.shape)\n",
    "\n",
    "        outputs, last_hidden_states, out_last = emotion_model(input_tensor)\n",
    "        waveform_emotion_av_test.append(out_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8432ed78f64629b449edb96c5fce02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([253, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([251, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "# train emotion data\n",
    "waveform_emotion_mul_test = []\n",
    "\n",
    "for batch in tqdm_notebook(dl_test_mul, total=len(dl_test_mul), desc='Processing dataset'):\n",
    "    if batch is None:\n",
    "        continue\n",
    "\n",
    "    video_names, inputs, labels = batch\n",
    "    video_name = video_names[0]\n",
    "    input_array = inputs[0]  # numpy array\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emotion_model.eval()\n",
    "        input_tensor = torch.tensor(input_array, dtype=torch.float32).to(device)  # 변환\n",
    "        print(input_tensor.shape)\n",
    "\n",
    "        outputs, last_hidden_states, out_last = emotion_model(input_tensor)\n",
    "        waveform_emotion_mul_test.append(out_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250, 512]), 140)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform_emotion_av_test[0].shape, len(waveform_emotion_av_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250, 512]), 163)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform_emotion_mul_test[0].shape, len(waveform_emotion_mul_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Move all tensors to CPU and convert to numpy\n",
    "cpu_array = [t.cpu().numpy() for t in waveform_emotion_av_test]\n",
    "\n",
    "# Save to .npy as object (since shapes might vary)\n",
    "np.save(\"waveform_emotion_av_test.npy\", np.array(cpu_array, dtype=object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Move all tensors to CPU and convert to numpy\n",
    "cpu_array = [t.cpu().numpy() for t in waveform_emotion_mul_test]\n",
    "\n",
    "# Save to .npy as object (since shapes might vary)\n",
    "np.save(\"waveform_emotion_mul_test.npy\", np.array(cpu_array, dtype=object))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Arousal, Valence Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터 노트북에서 명령행 인자 대신 변수를 직접 설정합니다.\n",
    "args = argparse.Namespace()\n",
    "args.seed = 1234\n",
    "args.regress = 1\n",
    "args.lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wav2vec_classifier(\n",
       "  (extractor): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (valence_classifier): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (arousal_classifier): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class wav2vec_classifier(nn.Module):\n",
    "    def __init__(self, extractor, num_labels, dropout_prob=0.1):\n",
    "        super(wav2vec_classifier, self).__init__()\n",
    "\n",
    "        self.extractor = extractor\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.nu_labels = num_labels\n",
    "        self.valence_classifier = nn.Linear(512, num_labels)\n",
    "        self.arousal_classifier = nn.Linear(512, num_labels)\n",
    "\n",
    "    def forward(self, wav):\n",
    "        extracted_wav = self.extractor(wav)\n",
    "        \n",
    "        #last_hidden_states = extracted_wav.last_hidden_state\n",
    "        last_hidden_states = extracted_wav.extract_features # B, seq, 512\n",
    "        last_hidden_states = self.dropout(last_hidden_states) #B, seq, 512\n",
    "        \n",
    "        output_valence = self.valence_classifier(last_hidden_states) # B, Seq, 1\n",
    "        output_arousal = self.arousal_classifier(last_hidden_states) # B, Seq, 1\n",
    "                \n",
    "        out_last = last_hidden_states[:, -1, :]\n",
    "        return output_valence[:, -1], output_arousal[:, -1], out_last\n",
    "\n",
    "extractor = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "arousal_valence_model = wav2vec_classifier(extractor,args.regress)\n",
    "arousal_valence_model.load_state_dict(torch.load('wav2vec_affective_audio/wac2vec_arousal_valence_model_epoch_4.pt'))\n",
    "arousal_valence_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(arousal_valence_model.parameters(), lr = args.lr,  eps = 1e-8)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 13 16:04:07 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000    Off  | 00000000:65:00.0 Off |                  Off |\n",
      "| 41%   35C    P2    34W / 140W |   6788MiB / 16117MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2381      G   /opt/conda/bin/python               4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d548604d53b042239d05da0f7400abbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([251, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([260, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([251, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([251, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "waveform_arousal_valence_av_train = []\n",
    "train_labels = []\n",
    "\n",
    "for batch in tqdm_notebook(dl_train_av, total=len(dl_train_av), desc='Processing dataset'):\n",
    "    if batch is None:\n",
    "        continue\n",
    "    video_names, inputs, labels = batch\n",
    "    video_name = video_names[0]\n",
    "    input_array = inputs[0]  # Unpack the tuple to get the actual array\n",
    "\n",
    "    with torch.no_grad():\n",
    "        arousal_valence_model.eval()\n",
    "        input_tensor = torch.tensor(input_array, dtype=torch.float32).to(device)\n",
    "        print(input_tensor.shape)\n",
    "        outputs_valence, outputs_arousal, out_last = arousal_valence_model(input_tensor)\n",
    "        waveform_arousal_valence_av_train.append(out_last.cpu())  # Move to CPU for later save\n",
    "        train_labels.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Move all tensors to CPU and convert to numpy\n",
    "cpu_array = [t.cpu().numpy() for t in waveform_arousal_valence_av_train]\n",
    "\n",
    "# Save to .npy as object (since shapes might vary)\n",
    "np.save(\"Features/waveform_arousal_valence_av_train.npy\", np.array(cpu_array, dtype=object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baeb1f6f88342f3ab07994212b7b747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([254, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([253, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([254, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "waveform_arousal_valence_av_test = []\n",
    "av_test_labels = []\n",
    "\n",
    "for batch in tqdm_notebook(dl_test_av, total=len(dl_test_av), desc='Processing dataset'):\n",
    "    if batch is None:\n",
    "        continue\n",
    "    video_names, inputs, labels = batch\n",
    "    video_name = video_names[0]\n",
    "    input_array = inputs[0]  # Unpack the tuple to get the actual array\n",
    "\n",
    "    with torch.no_grad():\n",
    "        arousal_valence_model.eval()\n",
    "        input_tensor = torch.tensor(input_array, dtype=torch.float32).to(device)\n",
    "        print(input_tensor.shape)\n",
    "        outputs_valence, outputs_arousal, out_last = arousal_valence_model(input_tensor)\n",
    "        waveform_arousal_valence_av_test.append(out_last.cpu())  # Move to CPU for later save\n",
    "        av_test_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d76e86370e54aa9ac60fa0248c845b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([253, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([251, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n",
      "torch.Size([250, 2000])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "waveform_arousal_valence_mul_test = []\n",
    "mul_test_labels = []\n",
    "\n",
    "for batch in tqdm_notebook(dl_test_mul, total=len(dl_test_mul), desc='Processing dataset'):\n",
    "    if batch is None:\n",
    "        continue\n",
    "    video_names, inputs, labels = batch\n",
    "    video_name = video_names[0]\n",
    "    input_array = inputs[0]  # Unpack the tuple to get the actual array\n",
    "\n",
    "    with torch.no_grad():\n",
    "        arousal_valence_model.eval()\n",
    "        input_tensor = torch.tensor(input_array, dtype=torch.float32).to(device)\n",
    "        print(input_tensor.shape)\n",
    "        outputs_valence, outputs_arousal, out_last = arousal_valence_model(input_tensor)\n",
    "        waveform_arousal_valence_mul_test.append(out_last.cpu())  # Move to CPU for later save\n",
    "        mul_test_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Move all tensors to CPU and convert to numpy\n",
    "cpu_array = [t.cpu().numpy() for t in waveform_arousal_valence_mul_test]\n",
    "\n",
    "# Save to .npy as object (since shapes might vary)\n",
    "np.save(\"Features/waveform_arousal_valence_mul_test.npy\", np.array(cpu_array, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Move all tensors to CPU and convert to numpy\n",
    "cpu_array = [t.cpu().numpy() for t in waveform_arousal_valence_mul_test]\n",
    "\n",
    "# Save to .npy as object (since shapes might vary)\n",
    "np.save(\"Features/waveform_arousal_valence_mul_test.npy\", np.array(cpu_array, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401, 401, (250, 512), (250, 512))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_train = np.load('Features/waveform_emotion_av_train.npy',allow_pickle=True)\n",
    "arousal_valence_train = np.load('Features/waveform_arousal_valence_av_train.npy',allow_pickle=True)\n",
    "len(emotion_train), len(arousal_valence_train),emotion_train[0].shape ,arousal_valence_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250, 512]), 140)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform_emotion_av_train[0].shape, len(waveform_emotion_av_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250, 512]), 140)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform_emotion_av_test[0].shape, len(waveform_emotion_av_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250, 512]), torch.Size([250, 512]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform_emotion_mul_test[0].shape, waveform_emotion_mul_test[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Concatenate (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_concatenate_train = []\n",
    "\n",
    "for av_feature, emotion_feature in zip(arousal_valence_train, emotion_train):\n",
    "    av_tensor = torch.tensor(av_feature)\n",
    "    emo_tensor = torch.tensor(emotion_feature)\n",
    "\n",
    "    min_len = min(av_tensor.shape[0], emo_tensor.shape[0])\n",
    "    av_tensor = av_tensor[:min_len]\n",
    "    emo_tensor = emo_tensor[:min_len]\n",
    "\n",
    "    concated_feature = torch.cat((av_tensor, emo_tensor), dim=1)\n",
    "    features_concatenate_train.append(concated_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([250, 1024]), 401)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_concatenate_train[0].shape, len(features_concatenate_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.array([t.cpu().numpy() for t in features_concatenate_train])\n",
    "\n",
    "# 저장 경로와 파일 이름 지정\n",
    "file_path = 'Features/concatenate_emotion_waveform_av_train.npy'\n",
    "\n",
    "# 넘파이 배열을 npy 파일로 저장\n",
    "np.save(file_path, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Unpack tensors from tuple and convert to numpy\n",
    "new_np_labels_train = [l[0].cpu().numpy().squeeze() for l in train_labels]\n",
    "\n",
    "# Convert to numpy array (may still be ragged depending on shape)\n",
    "labels_train = np.array(new_np_labels_train, dtype=object)\n",
    "\n",
    "# Save\n",
    "np.save('Features/labels_av_train.npy', labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all label arrays!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# AV TEST\n",
    "np_av_test_labels = [l[0].cpu().numpy().squeeze() for l in av_test_labels]\n",
    "np_av_test_labels = np.array(np_av_test_labels, dtype=object)\n",
    "np.save('Features/labels_av_test.npy', np_av_test_labels)\n",
    "\n",
    "# MUL TEST\n",
    "np_mul_test_labels = [l[0].cpu().numpy().squeeze() for l in mul_test_labels]\n",
    "np_mul_test_labels = np.array(np_mul_test_labels, dtype=object)\n",
    "np.save('Features/labels_mul_test.npy', np_mul_test_labels)\n",
    "\n",
    "print(\"✅ Saved all label arrays!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293, 48, (250,), (250,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_train), len(labels_val), labels_train[0].shape, labels_val[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef2f94aa5f146a19ea78adb57bd7940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check1: (246, 1024), (246,)\n",
      "check1: (106, 1024), (106,)\n",
      "check1: (133, 1024), (133,)\n",
      "check1: (102, 1024), (102,)\n",
      "check1: (124, 1024), (124,)\n",
      "check1: (28, 1024), (28,)\n",
      "check1: (95, 1024), (95,)\n",
      "check1: (69, 1024), (69,)\n",
      "check1: (18, 1024), (18,)\n",
      "check1: (86, 1024), (86,)\n",
      "check1: (219, 1024), (219,)\n",
      "check1: (79, 1024), (79,)\n",
      "check1: (136, 1024), (136,)\n",
      "check1: (118, 1024), (118,)\n",
      "check1: (83, 1024), (83,)\n",
      "check1: (159, 1024), (159,)\n",
      "check1: (106, 1024), (106,)\n",
      "check1: (196, 1024), (196,)\n",
      "check1: (47, 1024), (47,)\n",
      "check1: (201, 1024), (201,)\n",
      "check1: (67, 1024), (67,)\n",
      "check1: (102, 1024), (102,)\n",
      "check1: (246, 1024), (246,)\n",
      "check1: (134, 1024), (134,)\n",
      "check1: (59, 1024), (59,)\n",
      "check1: (163, 1024), (163,)\n",
      "check1: (241, 1024), (241,)\n",
      "check1: (71, 1024), (71,)\n",
      "check1: (30, 1024), (30,)\n",
      "check1: (231, 1024), (231,)\n",
      "check1: (111, 1024), (111,)\n",
      "check1: (93, 1024), (93,)\n",
      "check1: (94, 1024), (94,)\n",
      "check1: (105, 1024), (105,)\n",
      "check1: (158, 1024), (158,)\n",
      "check1: (109, 1024), (109,)\n",
      "check1: (43, 1024), (43,)\n",
      "check1: (130, 1024), (130,)\n",
      "check1: (35, 1024), (35,)\n",
      "check1: (52, 1024), (52,)\n",
      "check1: (142, 1024), (142,)\n",
      "check1: (183, 1024), (183,)\n",
      "check1: (196, 1024), (196,)\n",
      "check1: (51, 1024), (51,)\n",
      "check1: (50, 1024), (50,)\n",
      "check1: (43, 1024), (43,)\n",
      "check1: (64, 1024), (64,)\n",
      "check1: (101, 1024), (101,)\n",
      "check1: (186, 1024), (186,)\n",
      "check1: (3, 1024), (3,)\n",
      "check1: (54, 1024), (54,)\n",
      "check1: (167, 1024), (167,)\n",
      "check1: (124, 1024), (124,)\n",
      "check1: (248, 1024), (248,)\n",
      "check1: (155, 1024), (155,)\n",
      "check1: (105, 1024), (105,)\n",
      "check1: (67, 1024), (67,)\n",
      "check1: (114, 1024), (114,)\n",
      "check1: (129, 1024), (129,)\n",
      "check1: (20, 1024), (20,)\n",
      "check1: (71, 1024), (71,)\n",
      "check1: (191, 1024), (191,)\n",
      "check1: (236, 1024), (236,)\n",
      "check1: (117, 1024), (117,)\n",
      "check1: (121, 1024), (121,)\n",
      "check1: (67, 1024), (67,)\n",
      "check1: (101, 1024), (101,)\n",
      "check1: (128, 1024), (128,)\n",
      "check1: (136, 1024), (136,)\n",
      "check1: (179, 1024), (179,)\n",
      "check1: (51, 1024), (51,)\n",
      "check1: (243, 1024), (243,)\n",
      "check1: (18, 1024), (18,)\n",
      "check1: (19, 1024), (19,)\n",
      "check1: (208, 1024), (208,)\n",
      "check1: (129, 1024), (129,)\n",
      "check1: (208, 1024), (208,)\n",
      "check1: (49, 1024), (49,)\n",
      "check1: (96, 1024), (96,)\n",
      "check1: (106, 1024), (106,)\n",
      "check1: (156, 1024), (156,)\n",
      "check1: (57, 1024), (57,)\n",
      "check1: (177, 1024), (177,)\n",
      "check1: (65, 1024), (65,)\n",
      "check1: (55, 1024), (55,)\n",
      "check1: (224, 1024), (224,)\n",
      "check1: (107, 1024), (107,)\n",
      "check1: (97, 1024), (97,)\n",
      "check1: (102, 1024), (102,)\n",
      "check1: (86, 1024), (86,)\n",
      "check1: (26, 1024), (26,)\n",
      "check1: (189, 1024), (189,)\n",
      "check1: (97, 1024), (97,)\n",
      "check1: (201, 1024), (201,)\n",
      "check1: (98, 1024), (98,)\n",
      "check1: (248, 1024), (248,)\n",
      "check1: (212, 1024), (212,)\n",
      "check1: (121, 1024), (121,)\n",
      "check1: (76, 1024), (76,)\n",
      "check1: (97, 1024), (97,)\n",
      "check1: (148, 1024), (148,)\n",
      "check1: (95, 1024), (95,)\n",
      "check1: (241, 1024), (241,)\n",
      "check1: (96, 1024), (96,)\n",
      "check1: (156, 1024), (156,)\n",
      "check1: (97, 1024), (97,)\n",
      "check1: (174, 1024), (174,)\n",
      "check1: (99, 1024), (99,)\n",
      "check1: (147, 1024), (147,)\n",
      "check1: (135, 1024), (135,)\n",
      "check1: (78, 1024), (78,)\n",
      "check1: (103, 1024), (103,)\n",
      "check1: (245, 1024), (245,)\n",
      "check1: (145, 1024), (145,)\n",
      "check1: (153, 1024), (153,)\n",
      "check1: (206, 1024), (206,)\n",
      "check1: (54, 1024), (54,)\n",
      "check1: (40, 1024), (40,)\n",
      "check1: (41, 1024), (41,)\n",
      "check1: (142, 1024), (142,)\n",
      "check1: (112, 1024), (112,)\n",
      "check1: (111, 1024), (111,)\n",
      "check1: (150, 1024), (150,)\n",
      "check1: (43, 1024), (43,)\n",
      "check1: (20, 1024), (20,)\n",
      "check1: (90, 1024), (90,)\n",
      "check1: (216, 1024), (216,)\n",
      "check1: (148, 1024), (148,)\n",
      "check1: (100, 1024), (100,)\n",
      "check1: (94, 1024), (94,)\n",
      "check1: (132, 1024), (132,)\n",
      "check1: (96, 1024), (96,)\n",
      "check1: (78, 1024), (78,)\n",
      "check1: (3, 1024), (3,)\n",
      "check1: (119, 1024), (119,)\n",
      "check1: (218, 1024), (218,)\n",
      "check1: (117, 1024), (117,)\n",
      "check1: (72, 1024), (72,)\n",
      "check1: (67, 1024), (67,)\n",
      "check1: (91, 1024), (91,)\n",
      "check1: (98, 1024), (98,)\n",
      "check1: (68, 1024), (68,)\n",
      "check1: (102, 1024), (102,)\n",
      "check1: (208, 1024), (208,)\n",
      "check1: (99, 1024), (99,)\n",
      "check1: (242, 1024), (242,)\n",
      "check1: (169, 1024), (169,)\n",
      "check1: (15, 1024), (15,)\n",
      "check1: (96, 1024), (96,)\n",
      "check1: (82, 1024), (82,)\n",
      "check1: (175, 1024), (175,)\n",
      "check1: (118, 1024), (118,)\n",
      "check1: (72, 1024), (72,)\n",
      "check1: (46, 1024), (46,)\n",
      "check1: (232, 1024), (232,)\n",
      "check1: (94, 1024), (94,)\n",
      "check1: (112, 1024), (112,)\n",
      "check1: (207, 1024), (207,)\n",
      "check1: (195, 1024), (195,)\n",
      "check1: (167, 1024), (167,)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# label과 feature index 개수 맞추기\n",
    "# feature에서 임의로 0으로 padding 해놨던 구간 인덱스 개수 맞추기\n",
    "new_features_concatenate_list_train = []\n",
    "new_labels_train = []\n",
    "\n",
    "for concat, label in tqdm(zip(data_train, labels_train), total=len(labels_train), desc=\"Processing\"):\n",
    "    #print(concat.shape, label.shape)\n",
    "    if label.shape == ():\n",
    "         # concat 길이 줄이기\n",
    "        target_length = 1 \n",
    "        new_concat = concat[0:target_length,:]\n",
    "        new_features_concatenate_list_train.append(new_concat)\n",
    "        new_labels_train.append(label)\n",
    "        print(f'scalar check1: {new_concat.shape}, {label.shape}')\n",
    "        continue  # or any other logic you want to apply for scalar tensors\n",
    "    \n",
    "    if len(concat) == len(label):\n",
    "        new_features_concatenate_list_train.append(concat)\n",
    "        new_labels_train.append(label)\n",
    "    elif len(concat) > len(label):\n",
    "        # concat 길이 줄이기\n",
    "        target_length = len(label)  \n",
    "        new_concat = concat[0:target_length,:]\n",
    "        new_features_concatenate_list_train.append(new_concat)\n",
    "        new_labels_train.append(label)\n",
    "        print(f'check1: {new_concat.shape}, {label.shape}')\n",
    "    elif len(concat) < len(label):\n",
    "        # concat 길이 줄이기\n",
    "        target_length = len(concat.cuda())  \n",
    "        new_label = label[0:target_length]\n",
    "        new_labels_train.append(new_label)\n",
    "        new_features_concatenate_list_train.append(concat)\n",
    "        print(f'check2: {concat.shape}, {new_label.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401, (250,))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_labels_train), new_labels_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved with dtype=object\n"
     ]
    }
   ],
   "source": [
    "data_train = np.array([t for t in new_features_concatenate_list_train], dtype=object)\n",
    "file_path = 'Features/concatenate/concatenate_waveform_av_train.npy'\n",
    "np.save(file_path, data_train)\n",
    "print(\"✅ Saved with dtype=object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved with dtype=object\n"
     ]
    }
   ],
   "source": [
    "labels_train = np.array([t for t in new_labels_train], dtype=object)\n",
    "label_path = 'Features/labels_av_train.npy'\n",
    "np.save(label_path, labels_train)\n",
    "print(\"✅ Saved with dtype=object\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Concatenate (av_test,mul_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: Features/concatenate/concatenate_waveform_av_test.npy\n",
      "✅ Saved: Features/concatenate/concatenate_waveform_mul_test.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Utility Function to Concatenate Feature Pairs Safely\n",
    "def concatenate_features(av_list, emo_list):\n",
    "    concat_list = []\n",
    "    for av_feature, emotion_feature in zip(av_list, emo_list):\n",
    "        av_tensor = av_feature.cpu() if av_feature.is_cuda else av_feature\n",
    "        emo_tensor = emotion_feature.cpu() if emotion_feature.is_cuda else emotion_feature\n",
    "        concat_tensor = torch.cat((av_tensor, emo_tensor), dim=1)\n",
    "        concat_list.append(concat_tensor)\n",
    "    return concat_list\n",
    "\n",
    "# Save function\n",
    "def save_npy(data_list, file_path):\n",
    "    np_array = np.array([t.cpu().numpy() for t in data_list], dtype=object)\n",
    "    np.save(file_path, np_array)\n",
    "    print(f\"✅ Saved: {file_path}\")\n",
    "\n",
    "# Output directory\n",
    "Path(\"Features/concatenate\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. AV TEST\n",
    "features_concatenate_av_test = concatenate_features(\n",
    "    waveform_arousal_valence_av_test, waveform_emotion_av_test\n",
    ")\n",
    "save_npy(features_concatenate_av_test, \"Features/concatenate/concatenate_waveform_av_test.npy\")\n",
    "\n",
    "# 2. MUL TEST\n",
    "features_concatenate_mul_test = concatenate_features(\n",
    "    waveform_arousal_valence_mul_test, waveform_emotion_mul_test\n",
    ")\n",
    "save_npy(features_concatenate_mul_test, \"Features/concatenate/concatenate_waveform_mul_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "def align_features_labels(features, labels, verbose=True):\n",
    "    aligned_features = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for concat, label in tqdm(zip(features, labels), total=len(labels), desc=\"Aligning\"):\n",
    "        # ⚠️ Unpack if label is a tuple like (tensor,)\n",
    "        if isinstance(label, tuple) and len(label) == 1 and isinstance(label[0], torch.Tensor):\n",
    "            label = label[0]\n",
    "        elif isinstance(label, tuple):\n",
    "            if verbose:\n",
    "                print(f\"⚠️ Skipping invalid label (tuple of unexpected format): {label}\")\n",
    "            continue\n",
    "\n",
    "        # Check scalar\n",
    "        if hasattr(label, \"ndim\") and label.ndim == 0:\n",
    "            if verbose:\n",
    "                print(f\"⚠️ Skipping scalar label: {label}\")\n",
    "            continue\n",
    "\n",
    "        # Tensor to numpy\n",
    "        concat = concat.cpu().numpy() if hasattr(concat, \"cpu\") else concat\n",
    "        label = label.cpu().numpy() if hasattr(label, \"cpu\") else label\n",
    "\n",
    "        # Length match logic\n",
    "        if len(concat) == len(label):\n",
    "            aligned_features.append(concat)\n",
    "            aligned_labels.append(label)\n",
    "        elif len(concat) > len(label):\n",
    "            aligned_features.append(concat[:len(label)])\n",
    "            aligned_labels.append(label)\n",
    "            if verbose:\n",
    "                print(f\"✂️ Trimmed feature: {len(concat)} → {len(label)}\")\n",
    "        elif len(concat) < len(label):\n",
    "            aligned_features.append(concat)\n",
    "            aligned_labels.append(label[:len(concat)])\n",
    "            if verbose:\n",
    "                print(f\"✂️ Trimmed label: {len(label)} → {len(concat)}\")\n",
    "\n",
    "    return aligned_features, aligned_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a301ff806941cfa8b6f2f08cd564aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Trimmed feature: 250 → 97\n",
      "✂️ Trimmed feature: 250 → 94\n",
      "✂️ Trimmed feature: 250 → 103\n",
      "✂️ Trimmed feature: 250 → 106\n",
      "✂️ Trimmed feature: 250 → 82\n",
      "✂️ Trimmed feature: 250 → 105\n",
      "✂️ Trimmed feature: 250 → 168\n",
      "✂️ Trimmed feature: 250 → 115\n",
      "✂️ Trimmed feature: 250 → 170\n",
      "✂️ Trimmed feature: 250 → 93\n",
      "✂️ Trimmed feature: 250 → 102\n",
      "✂️ Trimmed feature: 250 → 104\n",
      "✂️ Trimmed feature: 250 → 95\n",
      "✂️ Trimmed feature: 250 → 111\n",
      "✂️ Trimmed feature: 250 → 191\n",
      "✂️ Trimmed feature: 250 → 90\n",
      "✂️ Trimmed feature: 250 → 117\n",
      "✂️ Trimmed feature: 250 → 169\n",
      "✂️ Trimmed feature: 250 → 179\n",
      "✂️ Trimmed feature: 250 → 102\n",
      "✂️ Trimmed feature: 250 → 151\n",
      "✂️ Trimmed feature: 250 → 97\n",
      "✂️ Trimmed feature: 250 → 81\n",
      "✂️ Trimmed feature: 250 → 226\n",
      "✂️ Trimmed feature: 250 → 108\n",
      "✂️ Trimmed feature: 250 → 191\n",
      "✂️ Trimmed feature: 250 → 124\n",
      "✂️ Trimmed feature: 254 → 8\n",
      "✂️ Trimmed feature: 250 → 75\n",
      "✂️ Trimmed feature: 250 → 161\n",
      "✂️ Trimmed feature: 250 → 98\n",
      "✂️ Trimmed feature: 250 → 213\n",
      "✂️ Trimmed feature: 250 → 72\n",
      "✂️ Trimmed feature: 250 → 55\n",
      "✂️ Trimmed feature: 250 → 136\n",
      "✂️ Trimmed feature: 253 → 9\n",
      "✂️ Trimmed feature: 250 → 113\n",
      "✂️ Trimmed feature: 250 → 179\n",
      "✂️ Trimmed feature: 250 → 159\n",
      "✂️ Trimmed feature: 250 → 104\n",
      "✂️ Trimmed feature: 254 → 4\n",
      "✂️ Trimmed feature: 250 → 237\n",
      "✂️ Trimmed feature: 250 → 205\n",
      "✂️ Trimmed feature: 250 → 145\n",
      "✂️ Trimmed feature: 250 → 141\n",
      "✂️ Trimmed feature: 250 → 239\n",
      "✂️ Trimmed feature: 250 → 239\n",
      "✂️ Trimmed feature: 250 → 68\n",
      "✂️ Trimmed feature: 250 → 60\n",
      "✂️ Trimmed feature: 250 → 176\n",
      "✂️ Trimmed feature: 250 → 56\n",
      "✂️ Trimmed feature: 250 → 50\n",
      "✂️ Trimmed feature: 250 → 75\n",
      "✂️ Trimmed feature: 250 → 240\n",
      "✂️ Trimmed feature: 250 → 196\n",
      "✂️ Trimmed feature: 250 → 96\n",
      "✂️ Trimmed feature: 250 → 182\n",
      "✂️ Trimmed feature: 250 → 144\n",
      "✂️ Trimmed feature: 250 → 55\n",
      "✂️ Trimmed feature: 250 → 26\n",
      "✂️ Trimmed feature: 250 → 52\n",
      "✂️ Trimmed feature: 250 → 223\n",
      "✂️ Trimmed feature: 250 → 35\n",
      "✂️ Trimmed feature: 250 → 132\n"
     ]
    }
   ],
   "source": [
    "# 예시: AV-TEST 셋에 대해 저장\n",
    "features_concatenate_val_aligned, labels_val_aligned = align_features_labels(\n",
    "    features_concatenate_av_test,  # ex: waveform_arousal_valence_av_test + emotion_av_test concat\n",
    "    av_test_labels\n",
    ")\n",
    "\n",
    "# numpy 배열 저장\n",
    "np.save(\"Features/concatenate/concatenate_waveform_av_test.npy\", np.array(features_concatenate_val_aligned, dtype=object))\n",
    "np.save(\"Features/labels_av_test.npy\", np.array(labels_val_aligned, dtype=object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07e41aaf9d147ada32d8f1be62108cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Trimmed feature: 250 → 96\n",
      "✂️ Trimmed feature: 250 → 107\n",
      "✂️ Trimmed feature: 250 → 94\n",
      "✂️ Trimmed feature: 250 → 87\n",
      "✂️ Trimmed feature: 250 → 84\n",
      "✂️ Trimmed feature: 250 → 125\n",
      "✂️ Trimmed feature: 250 → 106\n",
      "✂️ Trimmed feature: 250 → 237\n",
      "✂️ Trimmed feature: 250 → 87\n",
      "✂️ Trimmed feature: 250 → 162\n",
      "✂️ Trimmed feature: 250 → 94\n",
      "✂️ Trimmed feature: 250 → 90\n",
      "✂️ Trimmed feature: 250 → 106\n",
      "✂️ Trimmed feature: 250 → 177\n",
      "✂️ Trimmed feature: 250 → 106\n",
      "✂️ Trimmed feature: 250 → 95\n",
      "✂️ Trimmed feature: 250 → 214\n",
      "✂️ Trimmed feature: 250 → 231\n",
      "✂️ Trimmed feature: 250 → 156\n",
      "✂️ Trimmed feature: 250 → 108\n",
      "✂️ Trimmed feature: 250 → 99\n",
      "✂️ Trimmed feature: 250 → 185\n",
      "✂️ Trimmed feature: 250 → 208\n",
      "✂️ Trimmed feature: 250 → 54\n",
      "✂️ Trimmed feature: 250 → 18\n",
      "✂️ Trimmed feature: 250 → 181\n",
      "✂️ Trimmed feature: 250 → 192\n",
      "✂️ Trimmed feature: 250 → 128\n",
      "✂️ Trimmed feature: 250 → 195\n",
      "✂️ Trimmed feature: 250 → 181\n",
      "✂️ Trimmed feature: 250 → 238\n",
      "✂️ Trimmed feature: 250 → 118\n",
      "✂️ Trimmed feature: 250 → 101\n",
      "✂️ Trimmed feature: 250 → 216\n",
      "✂️ Trimmed feature: 250 → 114\n",
      "✂️ Trimmed feature: 253 → 5\n",
      "✂️ Trimmed feature: 250 → 61\n",
      "✂️ Trimmed feature: 250 → 235\n",
      "✂️ Trimmed feature: 250 → 155\n",
      "✂️ Trimmed feature: 250 → 100\n",
      "✂️ Trimmed feature: 250 → 96\n",
      "✂️ Trimmed feature: 250 → 237\n",
      "✂️ Trimmed feature: 250 → 232\n",
      "✂️ Trimmed feature: 250 → 90\n",
      "✂️ Trimmed feature: 250 → 90\n",
      "✂️ Trimmed feature: 250 → 42\n",
      "✂️ Trimmed feature: 250 → 210\n",
      "✂️ Trimmed feature: 250 → 136\n",
      "✂️ Trimmed feature: 250 → 96\n",
      "✂️ Trimmed feature: 250 → 147\n",
      "✂️ Trimmed feature: 250 → 114\n",
      "✂️ Trimmed feature: 250 → 18\n",
      "✂️ Trimmed feature: 250 → 150\n",
      "✂️ Trimmed feature: 250 → 71\n",
      "✂️ Trimmed feature: 250 → 125\n",
      "✂️ Trimmed feature: 251 → 21\n",
      "✂️ Trimmed feature: 250 → 83\n",
      "✂️ Trimmed feature: 250 → 133\n",
      "✂️ Trimmed feature: 250 → 165\n",
      "✂️ Trimmed feature: 250 → 68\n",
      "✂️ Trimmed feature: 250 → 71\n",
      "✂️ Trimmed feature: 250 → 24\n",
      "✂️ Trimmed feature: 250 → 177\n",
      "✂️ Trimmed feature: 250 → 54\n"
     ]
    }
   ],
   "source": [
    "# mul_test\n",
    "features_concatenate_mul_aligned, labels_mul_aligned = align_features_labels(\n",
    "    features_concatenate_mul_test, mul_test_labels\n",
    ")\n",
    "np.save(\"Features/concatenate/concatenate_waveform_mul_test.npy\", np.array(features_concatenate_mul_aligned, dtype=object))\n",
    "np.save(\"Features/labels_mul_test.npy\", np.array(labels_mul_aligned, dtype=object))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 163)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_val_aligned), len(labels_mul_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401, (250, 1024))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트 데이터 로드\n",
    "features_concatenate_train = np.load('Features/concatenate/concatenate_waveform_av_train.npy',allow_pickle=True)\n",
    "len(features_concatenate_train), features_concatenate_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, (97, 1024))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트 데이터 로드\n",
    "features_concatenate_val= np.load( 'Features/concatenate/concatenate_waveform_av_test.npy',allow_pickle=True)\n",
    "len(features_concatenate_val), features_concatenate_val[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, (96, 1024))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트 데이터 로드\n",
    "features_concatenate_test= np.load( 'Features/concatenate/concatenate_waveform_mul_test.npy',allow_pickle=True)\n",
    "len(features_concatenate_test), features_concatenate_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(401, (250,))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav2vec2_labels_train=np.load('Features/labels_av_train.npy',allow_pickle=True)\n",
    "len(wav2vec2_labels_train), wav2vec2_labels_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, (97,))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav2vec2_labels_val=np.load('Features/labels_av_test.npy',allow_pickle=True)\n",
    "len(wav2vec2_labels_val), wav2vec2_labels_val[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, (96,))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav2vec2_labels_test=np.load('Features/labels_mul_test.npy',allow_pickle=True)\n",
    "len(wav2vec2_labels_test), wav2vec2_labels_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wav2vec2_labels_train =   [torch.tensor(arr) for arr in wav2vec2_labels_train]\n",
    "new_wav2vec2_labels_val =   [torch.tensor(arr) for arr in wav2vec2_labels_val]\n",
    "new_wav2vec2_labels_test =   [torch.tensor(arr) for arr in wav2vec2_labels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_train = MyDataset(features_concatenate_train,new_wav2vec2_labels_train)\n",
    "md_val = MyDataset(features_concatenate_val,new_wav2vec2_labels_val)\n",
    "md_test = MyDataset(features_concatenate_test,new_wav2vec2_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(md_train)\n",
    "val_dataloader = DataLoader(md_val)\n",
    "test_dataloader = DataLoader(md_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 246)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wav2vec2_labels_train[2]), len(wav2vec2_labels_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        \n",
    "        # Decode with fully connected layer\n",
    "        output = self.fc(out)\n",
    "        \n",
    "        # Apply sigmoid activation function to output\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 1024\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "output_size = 1\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "model.load_state_dict(torch.load('/home/jovyan/EmotionDetection/model/lstm_audio_waveform_emotion_160_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dcea1a049746699eb8870577c39c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([246, 1, 1024]), labels.shape: torch.Size([246])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([106, 1, 1024]), labels.shape: torch.Size([106])\n",
      "inputs.shape: torch.Size([133, 1, 1024]), labels.shape: torch.Size([133])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([102, 1, 1024]), labels.shape: torch.Size([102])\n",
      "inputs.shape: torch.Size([124, 1, 1024]), labels.shape: torch.Size([124])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([28, 1, 1024]), labels.shape: torch.Size([28])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([95, 1, 1024]), labels.shape: torch.Size([95])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([69, 1, 1024]), labels.shape: torch.Size([69])\n",
      "inputs.shape: torch.Size([18, 1, 1024]), labels.shape: torch.Size([18])\n",
      "inputs.shape: torch.Size([86, 1, 1024]), labels.shape: torch.Size([86])\n",
      "inputs.shape: torch.Size([219, 1, 1024]), labels.shape: torch.Size([219])\n",
      "inputs.shape: torch.Size([79, 1, 1024]), labels.shape: torch.Size([79])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([136, 1, 1024]), labels.shape: torch.Size([136])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([118, 1, 1024]), labels.shape: torch.Size([118])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([83, 1, 1024]), labels.shape: torch.Size([83])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([159, 1, 1024]), labels.shape: torch.Size([159])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([106, 1, 1024]), labels.shape: torch.Size([106])\n",
      "inputs.shape: torch.Size([196, 1, 1024]), labels.shape: torch.Size([196])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([47, 1, 1024]), labels.shape: torch.Size([47])\n",
      "inputs.shape: torch.Size([201, 1, 1024]), labels.shape: torch.Size([201])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([67, 1, 1024]), labels.shape: torch.Size([67])\n",
      "inputs.shape: torch.Size([102, 1, 1024]), labels.shape: torch.Size([102])\n",
      "inputs.shape: torch.Size([246, 1, 1024]), labels.shape: torch.Size([246])\n",
      "inputs.shape: torch.Size([134, 1, 1024]), labels.shape: torch.Size([134])\n",
      "inputs.shape: torch.Size([59, 1, 1024]), labels.shape: torch.Size([59])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([163, 1, 1024]), labels.shape: torch.Size([163])\n",
      "inputs.shape: torch.Size([241, 1, 1024]), labels.shape: torch.Size([241])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([71, 1, 1024]), labels.shape: torch.Size([71])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([30, 1, 1024]), labels.shape: torch.Size([30])\n",
      "inputs.shape: torch.Size([231, 1, 1024]), labels.shape: torch.Size([231])\n",
      "inputs.shape: torch.Size([111, 1, 1024]), labels.shape: torch.Size([111])\n",
      "inputs.shape: torch.Size([93, 1, 1024]), labels.shape: torch.Size([93])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([94, 1, 1024]), labels.shape: torch.Size([94])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([105, 1, 1024]), labels.shape: torch.Size([105])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([158, 1, 1024]), labels.shape: torch.Size([158])\n",
      "inputs.shape: torch.Size([109, 1, 1024]), labels.shape: torch.Size([109])\n",
      "inputs.shape: torch.Size([43, 1, 1024]), labels.shape: torch.Size([43])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([130, 1, 1024]), labels.shape: torch.Size([130])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([35, 1, 1024]), labels.shape: torch.Size([35])\n",
      "inputs.shape: torch.Size([52, 1, 1024]), labels.shape: torch.Size([52])\n",
      "inputs.shape: torch.Size([142, 1, 1024]), labels.shape: torch.Size([142])\n",
      "inputs.shape: torch.Size([183, 1, 1024]), labels.shape: torch.Size([183])\n",
      "inputs.shape: torch.Size([196, 1, 1024]), labels.shape: torch.Size([196])\n",
      "inputs.shape: torch.Size([51, 1, 1024]), labels.shape: torch.Size([51])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([50, 1, 1024]), labels.shape: torch.Size([50])\n",
      "inputs.shape: torch.Size([43, 1, 1024]), labels.shape: torch.Size([43])\n",
      "inputs.shape: torch.Size([64, 1, 1024]), labels.shape: torch.Size([64])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([101, 1, 1024]), labels.shape: torch.Size([101])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([186, 1, 1024]), labels.shape: torch.Size([186])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([3, 1, 1024]), labels.shape: torch.Size([3])\n",
      "inputs.shape: torch.Size([54, 1, 1024]), labels.shape: torch.Size([54])\n",
      "inputs.shape: torch.Size([167, 1, 1024]), labels.shape: torch.Size([167])\n",
      "inputs.shape: torch.Size([124, 1, 1024]), labels.shape: torch.Size([124])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([248, 1, 1024]), labels.shape: torch.Size([248])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([155, 1, 1024]), labels.shape: torch.Size([155])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([105, 1, 1024]), labels.shape: torch.Size([105])\n",
      "inputs.shape: torch.Size([67, 1, 1024]), labels.shape: torch.Size([67])\n",
      "inputs.shape: torch.Size([114, 1, 1024]), labels.shape: torch.Size([114])\n",
      "inputs.shape: torch.Size([129, 1, 1024]), labels.shape: torch.Size([129])\n",
      "inputs.shape: torch.Size([20, 1, 1024]), labels.shape: torch.Size([20])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([71, 1, 1024]), labels.shape: torch.Size([71])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([191, 1, 1024]), labels.shape: torch.Size([191])\n",
      "inputs.shape: torch.Size([236, 1, 1024]), labels.shape: torch.Size([236])\n",
      "inputs.shape: torch.Size([117, 1, 1024]), labels.shape: torch.Size([117])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([121, 1, 1024]), labels.shape: torch.Size([121])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([67, 1, 1024]), labels.shape: torch.Size([67])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([101, 1, 1024]), labels.shape: torch.Size([101])\n",
      "inputs.shape: torch.Size([128, 1, 1024]), labels.shape: torch.Size([128])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([136, 1, 1024]), labels.shape: torch.Size([136])\n",
      "inputs.shape: torch.Size([179, 1, 1024]), labels.shape: torch.Size([179])\n",
      "inputs.shape: torch.Size([51, 1, 1024]), labels.shape: torch.Size([51])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([243, 1, 1024]), labels.shape: torch.Size([243])\n",
      "inputs.shape: torch.Size([18, 1, 1024]), labels.shape: torch.Size([18])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([19, 1, 1024]), labels.shape: torch.Size([19])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([208, 1, 1024]), labels.shape: torch.Size([208])\n",
      "inputs.shape: torch.Size([129, 1, 1024]), labels.shape: torch.Size([129])\n",
      "inputs.shape: torch.Size([208, 1, 1024]), labels.shape: torch.Size([208])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([49, 1, 1024]), labels.shape: torch.Size([49])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([96, 1, 1024]), labels.shape: torch.Size([96])\n",
      "inputs.shape: torch.Size([106, 1, 1024]), labels.shape: torch.Size([106])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([156, 1, 1024]), labels.shape: torch.Size([156])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([57, 1, 1024]), labels.shape: torch.Size([57])\n",
      "inputs.shape: torch.Size([177, 1, 1024]), labels.shape: torch.Size([177])\n",
      "inputs.shape: torch.Size([65, 1, 1024]), labels.shape: torch.Size([65])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([55, 1, 1024]), labels.shape: torch.Size([55])\n",
      "inputs.shape: torch.Size([224, 1, 1024]), labels.shape: torch.Size([224])\n",
      "inputs.shape: torch.Size([107, 1, 1024]), labels.shape: torch.Size([107])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([97, 1, 1024]), labels.shape: torch.Size([97])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([102, 1, 1024]), labels.shape: torch.Size([102])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([86, 1, 1024]), labels.shape: torch.Size([86])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([26, 1, 1024]), labels.shape: torch.Size([26])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([189, 1, 1024]), labels.shape: torch.Size([189])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([97, 1, 1024]), labels.shape: torch.Size([97])\n",
      "inputs.shape: torch.Size([201, 1, 1024]), labels.shape: torch.Size([201])\n",
      "inputs.shape: torch.Size([98, 1, 1024]), labels.shape: torch.Size([98])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([248, 1, 1024]), labels.shape: torch.Size([248])\n",
      "inputs.shape: torch.Size([212, 1, 1024]), labels.shape: torch.Size([212])\n",
      "inputs.shape: torch.Size([121, 1, 1024]), labels.shape: torch.Size([121])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([76, 1, 1024]), labels.shape: torch.Size([76])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([97, 1, 1024]), labels.shape: torch.Size([97])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([148, 1, 1024]), labels.shape: torch.Size([148])\n",
      "inputs.shape: torch.Size([95, 1, 1024]), labels.shape: torch.Size([95])\n",
      "inputs.shape: torch.Size([241, 1, 1024]), labels.shape: torch.Size([241])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([96, 1, 1024]), labels.shape: torch.Size([96])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([156, 1, 1024]), labels.shape: torch.Size([156])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([97, 1, 1024]), labels.shape: torch.Size([97])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([174, 1, 1024]), labels.shape: torch.Size([174])\n",
      "inputs.shape: torch.Size([99, 1, 1024]), labels.shape: torch.Size([99])\n",
      "inputs.shape: torch.Size([147, 1, 1024]), labels.shape: torch.Size([147])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([135, 1, 1024]), labels.shape: torch.Size([135])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([78, 1, 1024]), labels.shape: torch.Size([78])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([103, 1, 1024]), labels.shape: torch.Size([103])\n",
      "inputs.shape: torch.Size([245, 1, 1024]), labels.shape: torch.Size([245])\n",
      "inputs.shape: torch.Size([145, 1, 1024]), labels.shape: torch.Size([145])\n",
      "inputs.shape: torch.Size([153, 1, 1024]), labels.shape: torch.Size([153])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([206, 1, 1024]), labels.shape: torch.Size([206])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([54, 1, 1024]), labels.shape: torch.Size([54])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([40, 1, 1024]), labels.shape: torch.Size([40])\n",
      "inputs.shape: torch.Size([41, 1, 1024]), labels.shape: torch.Size([41])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([142, 1, 1024]), labels.shape: torch.Size([142])\n",
      "inputs.shape: torch.Size([112, 1, 1024]), labels.shape: torch.Size([112])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([111, 1, 1024]), labels.shape: torch.Size([111])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([150, 1, 1024]), labels.shape: torch.Size([150])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([43, 1, 1024]), labels.shape: torch.Size([43])\n",
      "inputs.shape: torch.Size([20, 1, 1024]), labels.shape: torch.Size([20])\n",
      "inputs.shape: torch.Size([90, 1, 1024]), labels.shape: torch.Size([90])\n",
      "inputs.shape: torch.Size([216, 1, 1024]), labels.shape: torch.Size([216])\n",
      "inputs.shape: torch.Size([148, 1, 1024]), labels.shape: torch.Size([148])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([100, 1, 1024]), labels.shape: torch.Size([100])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([94, 1, 1024]), labels.shape: torch.Size([94])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([132, 1, 1024]), labels.shape: torch.Size([132])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([96, 1, 1024]), labels.shape: torch.Size([96])\n",
      "inputs.shape: torch.Size([78, 1, 1024]), labels.shape: torch.Size([78])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([3, 1, 1024]), labels.shape: torch.Size([3])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([119, 1, 1024]), labels.shape: torch.Size([119])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([218, 1, 1024]), labels.shape: torch.Size([218])\n",
      "inputs.shape: torch.Size([117, 1, 1024]), labels.shape: torch.Size([117])\n",
      "inputs.shape: torch.Size([72, 1, 1024]), labels.shape: torch.Size([72])\n",
      "inputs.shape: torch.Size([67, 1, 1024]), labels.shape: torch.Size([67])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([91, 1, 1024]), labels.shape: torch.Size([91])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([98, 1, 1024]), labels.shape: torch.Size([98])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([68, 1, 1024]), labels.shape: torch.Size([68])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([102, 1, 1024]), labels.shape: torch.Size([102])\n",
      "inputs.shape: torch.Size([208, 1, 1024]), labels.shape: torch.Size([208])\n",
      "inputs.shape: torch.Size([99, 1, 1024]), labels.shape: torch.Size([99])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([242, 1, 1024]), labels.shape: torch.Size([242])\n",
      "inputs.shape: torch.Size([169, 1, 1024]), labels.shape: torch.Size([169])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([15, 1, 1024]), labels.shape: torch.Size([15])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([96, 1, 1024]), labels.shape: torch.Size([96])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([82, 1, 1024]), labels.shape: torch.Size([82])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([175, 1, 1024]), labels.shape: torch.Size([175])\n",
      "inputs.shape: torch.Size([118, 1, 1024]), labels.shape: torch.Size([118])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([72, 1, 1024]), labels.shape: torch.Size([72])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([46, 1, 1024]), labels.shape: torch.Size([46])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([232, 1, 1024]), labels.shape: torch.Size([232])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([94, 1, 1024]), labels.shape: torch.Size([94])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([112, 1, 1024]), labels.shape: torch.Size([112])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([207, 1, 1024]), labels.shape: torch.Size([207])\n",
      "inputs.shape: torch.Size([195, 1, 1024]), labels.shape: torch.Size([195])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([167, 1, 1024]), labels.shape: torch.Size([167])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "y_pred = []\n",
    "lstm_emotion_waveform_multimodal_train = []\n",
    "\n",
    "for inputs, labels in tqdm_notebook(train_dataloader,total=len(train_dataloader), desc='Processing dataset'):\n",
    "    with torch.no_grad():\n",
    "        inputs_dim = len(inputs.shape)\n",
    "        labels_dim = len(labels.shape)\n",
    "        if inputs_dim < 2 or labels_dim < 2:\n",
    "            model.eval()\n",
    "            print(f\"Mismatched dimensions: inputs({inputs.shape}), labels({labels.shape})\")\n",
    "            inputs = inputs.transpose(0,1).to(device)\n",
    "            labels = labels.squeeze(0).to(device)\n",
    "\n",
    "            y_p, y_f = model(inputs.cpu())\n",
    "            y_pred.append(y_p)\n",
    "            lstm_emotion_waveform_multimodal_train.append(y_f)\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if (inputs.shape[1])!= (labels.shape[1]):\n",
    "            print( inputs.shape[1], labels.shape[1])\n",
    "            continue\n",
    "        \n",
    "        model.eval()\n",
    "        inputs = inputs.transpose(0,1).to(device)\n",
    "        labels = labels.squeeze(0).to(device)\n",
    "        print(f'inputs.shape: {inputs.shape}, labels.shape: {labels.shape}')\n",
    "        y_p, y_f = model(inputs.cpu())\n",
    "        y_pred.append(y_p)\n",
    "        lstm_emotion_waveform_multimodal_train.append(y_f)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25013/2819844503.py:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  lstm_emotion_waveform_multimodal_train  = np.array([p.cpu() for p in lstm_emotion_waveform_multimodal_train])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (401,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lstm_emotion_waveform_multimodal_train  \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlstm_emotion_waveform_multimodal_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (401,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 리스트 내부 tensor들을 numpy object 배열로 저장\n",
    "lstm_emotion_waveform_multimodal_train_np = np.array(\n",
    "    [p.cpu().numpy() for p in lstm_emotion_waveform_multimodal_train],\n",
    "    dtype=object  # 다양한 shape 허용\n",
    ")\n",
    "\n",
    "# 저장\n",
    "np.save(\"Features/lstm_emotion_waveform_av_train.npy\", lstm_emotion_waveform_multimodal_train_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_emotion_waveform_multimodal_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../multimodal/lstm_emotion_waveform_av_train.npy',lstm_emotion_waveform_multimodal_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddc505ddf2148d7b674be1193bf83fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([55, 1, 1024]), labels.shape: torch.Size([55])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([72, 1, 1024]), labels.shape: torch.Size([72])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([60, 1, 1024]), labels.shape: torch.Size([60])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([182, 1, 1024]), labels.shape: torch.Size([182])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([223, 1, 1024]), labels.shape: torch.Size([223])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([159, 1, 1024]), labels.shape: torch.Size([159])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([55, 1, 1024]), labels.shape: torch.Size([55])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([240, 1, 1024]), labels.shape: torch.Size([240])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([145, 1, 1024]), labels.shape: torch.Size([145])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([52, 1, 1024]), labels.shape: torch.Size([52])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([239, 1, 1024]), labels.shape: torch.Size([239])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([176, 1, 1024]), labels.shape: torch.Size([176])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([179, 1, 1024]), labels.shape: torch.Size([179])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([239, 1, 1024]), labels.shape: torch.Size([239])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([237, 1, 1024]), labels.shape: torch.Size([237])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([250, 1, 1024]), labels.shape: torch.Size([250])\n",
      "inputs.shape: torch.Size([196, 1, 1024]), labels.shape: torch.Size([196])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "y_pred = []\n",
    "lstm_emotion_waveform_multimodal_val = []\n",
    "\n",
    "for inputs, labels in tqdm_notebook(val_dataloader,total=len(val_dataloader), desc='Processing dataset'):\n",
    "    with torch.no_grad():\n",
    "        inputs_dim = len(inputs.shape)\n",
    "        labels_dim = len(labels.shape)\n",
    "        if inputs_dim < 2 or labels_dim < 2:\n",
    "            model.eval()\n",
    "            print(f\"Mismatched dimensions: inputs({inputs.shape}), labels({labels.shape})\")\n",
    "            inputs = inputs.transpose(0,1).to(device)\n",
    "            labels = labels.squeeze(0).to(device)\n",
    "\n",
    "            y_p, y_f = model(inputs.cpu())\n",
    "            y_pred.append(y_p)\n",
    "            lstm_emotion_waveform_multimodal_val.append(y_f)\n",
    "            continue\n",
    "            \n",
    "        if inputs.shape[1]!= labels.shape[1] :\n",
    "            print(\"mismatch\")\n",
    "            continue\n",
    "        model.eval()\n",
    "        inputs = inputs.transpose(0,1).to(device)\n",
    "        labels = labels.squeeze(0).to(device)\n",
    "        print(f'inputs.shape: {inputs.shape}, labels.shape: {labels.shape}')\n",
    "        y_p, y_f = model(inputs.cpu())\n",
    "        y_pred.append(y_p)\n",
    "        lstm_emotion_waveform_multimodal_val.append(y_f)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm_emotion_waveform_multimodal_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-118-099d563c4bd4>:1: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  lstm_emotion_waveform_multimodal_val  = np.array([p.cpu() for p in lstm_emotion_waveform_multimodal_val])\n",
      "<ipython-input-118-099d563c4bd4>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  lstm_emotion_waveform_multimodal_val  = np.array([p.cpu() for p in lstm_emotion_waveform_multimodal_val])\n"
     ]
    }
   ],
   "source": [
    "lstm_emotion_waveform_multimodal_val  = np.array([p.cpu() for p in lstm_emotion_waveform_multimodal_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 128])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_emotion_waveform_multimodal_val[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../multimodal/lstm_emotion_waveform_av_val.npy',lstm_emotion_waveform_multimodal_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
