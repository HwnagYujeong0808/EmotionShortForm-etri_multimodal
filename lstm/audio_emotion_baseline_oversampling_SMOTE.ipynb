{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "import argparse\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/EmotionShortForm\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/EmotionShortForm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(41) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav 파일의 mel Feature와 상태정보를 합친 학습데이터를 불러옵니다.\n",
    "train_df = pd.read_csv('preprocessing_csv/train_mfcc_data.csv')\n",
    "test_df = pd.read_csv('preprocessing_csv/test_mfcc_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "+ target 변수가 너무 많고, neutral이 포함된 복합 감정 value가 많음\n",
    "+ 복합 감정에 한해서만 neutral 변수 이름을 제거한 뒤 새로운 column 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'angry', 'happy', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_train = train_df[\"Emotion\"]\n",
    "data_test = test_df[\"Emotion\"]\n",
    "\n",
    "newEmotion_tr = [0]*train_df.shape[0]\n",
    "newEmotion_te = [0]*test_df.shape[0]\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    if ';neutral' in data_train[i]: \n",
    "        new_emotion_lst = data_train[i].split(\";neutral\")\n",
    "        new_emotion = \"\".join(new_emotion_lst)\n",
    "        newEmotion_tr[i] = new_emotion\n",
    "    elif 'neutral;' in data_train[i]: \n",
    "        new_emotion_lst = data_train[i].split(\"neutral;\")\n",
    "        new_emotion = \"\".join(new_emotion_lst)\n",
    "        newEmotion_tr[i] = new_emotion\n",
    "    else:\n",
    "        newEmotion_tr[i]= data_train[i]\n",
    "\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    if ';neutral' in data_test[i]: \n",
    "        new_emotion_lst = data_test[i].split(\";neutral\")\n",
    "        new_emotion = \"\".join(new_emotion_lst)\n",
    "        newEmotion_te[i] = new_emotion\n",
    "    elif 'neutral;' in data_test[i]: \n",
    "        new_emotion_lst = data_test[i].split(\"neutral;\")\n",
    "        new_emotion = \"\".join(new_emotion_lst)\n",
    "        newEmotion_te[i] = new_emotion\n",
    "    else:\n",
    "        newEmotion_te[i]= data_test[i]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "print(newEmotion_tr[:30])\n",
    "\n",
    "counts = {}\n",
    "for d in newEmotion_tr:\n",
    "    if d in counts:\n",
    "        counts[d] += 1\n",
    "    else:\n",
    "        counts[d] = 1\n",
    "\n",
    "# Emotion별 원소 개수 확인\n",
    "plt.barh(list(counts.keys()), list(counts.values()), color='lightseagreen')\n",
    "plt.title('Bar Plot with Neutral')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Keys')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# neutral 제거 후 barplot\n",
    "counts.pop(\"neutral\")\n",
    "\n",
    "plt.barh(list(counts.keys()), list(counts.values()), color='darkcyan')\n",
    "\n",
    "# 그래프 제목과 레이블 추가\n",
    "plt.title('Bar Plot without Neutral')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Keys')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "newEmotion_tr_df = pd.DataFrame(newEmotion_tr, columns=['newEmotion'])\n",
    "newEmotion_te_df = pd.DataFrame(newEmotion_te, columns=['newEmotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test df 생성\n",
    "+ wav 파일의 mel feature과 상태 정보를 합친 학습데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, newEmotion_tr_df], axis=1)\n",
    "test_df = pd.concat([test_df, newEmotion_te_df], axis=1)\n",
    "\n",
    "train_df = train_df.drop(['Emotion'], axis=1)\n",
    "test_df = test_df.drop(['Emotion'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10769, 38)\n",
      "(2693, 38)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SegmentId</th>\n",
       "      <th>time</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_24</th>\n",
       "      <th>mfcc_25</th>\n",
       "      <th>mfcc_26</th>\n",
       "      <th>mfcc_27</th>\n",
       "      <th>mfcc_28</th>\n",
       "      <th>mfcc_29</th>\n",
       "      <th>mfcc_30</th>\n",
       "      <th>mfcc_31</th>\n",
       "      <th>mfcc_32</th>\n",
       "      <th>newEmotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>Sess13_script01_User026F_029</td>\n",
       "      <td>7.254990</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-336.985138</td>\n",
       "      <td>106.803337</td>\n",
       "      <td>4.534938</td>\n",
       "      <td>10.542789</td>\n",
       "      <td>-19.378389</td>\n",
       "      <td>...</td>\n",
       "      <td>8.967966</td>\n",
       "      <td>8.074034</td>\n",
       "      <td>11.653221</td>\n",
       "      <td>11.598641</td>\n",
       "      <td>8.817598</td>\n",
       "      <td>9.400865</td>\n",
       "      <td>8.987887</td>\n",
       "      <td>5.992146</td>\n",
       "      <td>4.622094</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5163</td>\n",
       "      <td>Sess03_script03_User005M_021</td>\n",
       "      <td>2.212000</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-337.904297</td>\n",
       "      <td>105.446899</td>\n",
       "      <td>11.310282</td>\n",
       "      <td>24.076952</td>\n",
       "      <td>-6.819501</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.050204</td>\n",
       "      <td>-3.444622</td>\n",
       "      <td>-0.045880</td>\n",
       "      <td>-2.624972</td>\n",
       "      <td>-0.375320</td>\n",
       "      <td>-1.709600</td>\n",
       "      <td>-1.171931</td>\n",
       "      <td>-4.099090</td>\n",
       "      <td>-1.587989</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7844</td>\n",
       "      <td>Sess23_script02_User046M_022</td>\n",
       "      <td>9.281020</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-419.555511</td>\n",
       "      <td>125.555763</td>\n",
       "      <td>18.391697</td>\n",
       "      <td>30.752876</td>\n",
       "      <td>-5.573301</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.478583</td>\n",
       "      <td>-2.749439</td>\n",
       "      <td>1.098240</td>\n",
       "      <td>1.010692</td>\n",
       "      <td>0.460480</td>\n",
       "      <td>-1.992547</td>\n",
       "      <td>1.084825</td>\n",
       "      <td>-0.576132</td>\n",
       "      <td>0.662649</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304</td>\n",
       "      <td>Sess13_script05_User026F_017</td>\n",
       "      <td>6.614991</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>-261.221222</td>\n",
       "      <td>108.892136</td>\n",
       "      <td>-26.196730</td>\n",
       "      <td>9.421447</td>\n",
       "      <td>-25.387993</td>\n",
       "      <td>...</td>\n",
       "      <td>13.833956</td>\n",
       "      <td>7.379666</td>\n",
       "      <td>8.362466</td>\n",
       "      <td>1.706965</td>\n",
       "      <td>5.596114</td>\n",
       "      <td>5.339090</td>\n",
       "      <td>5.811155</td>\n",
       "      <td>1.621077</td>\n",
       "      <td>3.745970</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5216</td>\n",
       "      <td>Sess03_script04_User005M_008</td>\n",
       "      <td>3.404998</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-346.095032</td>\n",
       "      <td>101.206184</td>\n",
       "      <td>21.618317</td>\n",
       "      <td>20.990105</td>\n",
       "      <td>-11.223962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.532429</td>\n",
       "      <td>-1.102016</td>\n",
       "      <td>1.010110</td>\n",
       "      <td>-1.844896</td>\n",
       "      <td>-1.788239</td>\n",
       "      <td>0.140551</td>\n",
       "      <td>-2.089751</td>\n",
       "      <td>-1.204731</td>\n",
       "      <td>-1.450962</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>8873</td>\n",
       "      <td>Sess39_script05_User078F_007</td>\n",
       "      <td>4.404998</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-452.831024</td>\n",
       "      <td>80.856758</td>\n",
       "      <td>19.427954</td>\n",
       "      <td>32.913322</td>\n",
       "      <td>-3.974150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.763288</td>\n",
       "      <td>2.534321</td>\n",
       "      <td>1.811994</td>\n",
       "      <td>4.191569</td>\n",
       "      <td>6.094894</td>\n",
       "      <td>5.072481</td>\n",
       "      <td>7.664765</td>\n",
       "      <td>4.959315</td>\n",
       "      <td>4.205091</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>11974</td>\n",
       "      <td>Sess37_script05_User073F_012</td>\n",
       "      <td>6.781000</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-450.678375</td>\n",
       "      <td>98.769257</td>\n",
       "      <td>-4.727075</td>\n",
       "      <td>7.584974</td>\n",
       "      <td>-11.716160</td>\n",
       "      <td>...</td>\n",
       "      <td>3.983363</td>\n",
       "      <td>4.685560</td>\n",
       "      <td>2.902997</td>\n",
       "      <td>0.235584</td>\n",
       "      <td>2.744654</td>\n",
       "      <td>1.443039</td>\n",
       "      <td>5.997917</td>\n",
       "      <td>3.257085</td>\n",
       "      <td>5.762399</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>10559</td>\n",
       "      <td>Sess33_script04_User065M_028</td>\n",
       "      <td>3.066020</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>-371.517609</td>\n",
       "      <td>117.556679</td>\n",
       "      <td>5.498871</td>\n",
       "      <td>27.535383</td>\n",
       "      <td>7.929699</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.782557</td>\n",
       "      <td>-2.510203</td>\n",
       "      <td>-2.095202</td>\n",
       "      <td>-3.341051</td>\n",
       "      <td>-1.580801</td>\n",
       "      <td>-0.980542</td>\n",
       "      <td>0.799156</td>\n",
       "      <td>-2.367372</td>\n",
       "      <td>0.982645</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>3076</td>\n",
       "      <td>Sess02_script02_User003F_001</td>\n",
       "      <td>12.123001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>-375.577759</td>\n",
       "      <td>106.716896</td>\n",
       "      <td>2.852894</td>\n",
       "      <td>26.215828</td>\n",
       "      <td>-20.312307</td>\n",
       "      <td>...</td>\n",
       "      <td>4.127676</td>\n",
       "      <td>8.754350</td>\n",
       "      <td>5.358984</td>\n",
       "      <td>4.600071</td>\n",
       "      <td>9.529908</td>\n",
       "      <td>5.941333</td>\n",
       "      <td>14.158734</td>\n",
       "      <td>8.534500</td>\n",
       "      <td>10.391392</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>11617</td>\n",
       "      <td>Sess25_script05_User050F_018</td>\n",
       "      <td>2.240010</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-469.148651</td>\n",
       "      <td>96.403061</td>\n",
       "      <td>26.530113</td>\n",
       "      <td>8.035880</td>\n",
       "      <td>-2.224418</td>\n",
       "      <td>...</td>\n",
       "      <td>5.588368</td>\n",
       "      <td>4.237176</td>\n",
       "      <td>7.683784</td>\n",
       "      <td>0.811043</td>\n",
       "      <td>1.432141</td>\n",
       "      <td>1.804263</td>\n",
       "      <td>4.709819</td>\n",
       "      <td>1.102094</td>\n",
       "      <td>-0.579098</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2693 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                     SegmentId       time  Valence  Arousal  \\\n",
       "0             63  Sess13_script01_User026F_029   7.254990      3.7      3.5   \n",
       "1           5163  Sess03_script03_User005M_021   2.212000      2.9      3.4   \n",
       "2           7844  Sess23_script02_User046M_022   9.281020      2.3      3.0   \n",
       "3            304  Sess13_script05_User026F_017   6.614991      4.4      4.2   \n",
       "4           5216  Sess03_script04_User005M_008   3.404998      1.6      3.2   \n",
       "...          ...                           ...        ...      ...      ...   \n",
       "2688        8873  Sess39_script05_User078F_007   4.404998      3.0      3.0   \n",
       "2689       11974  Sess37_script05_User073F_012   6.781000      2.6      3.2   \n",
       "2690       10559  Sess33_script04_User065M_028   3.066020      3.4      3.4   \n",
       "2691        3076  Sess02_script02_User003F_001  12.123001      2.4      2.6   \n",
       "2692       11617  Sess25_script05_User050F_018   2.240010      3.2      3.3   \n",
       "\n",
       "          mfcc_1      mfcc_2     mfcc_3     mfcc_4     mfcc_5  ...    mfcc_24  \\\n",
       "0    -336.985138  106.803337   4.534938  10.542789 -19.378389  ...   8.967966   \n",
       "1    -337.904297  105.446899  11.310282  24.076952  -6.819501  ...  -4.050204   \n",
       "2    -419.555511  125.555763  18.391697  30.752876  -5.573301  ...  -3.478583   \n",
       "3    -261.221222  108.892136 -26.196730   9.421447 -25.387993  ...  13.833956   \n",
       "4    -346.095032  101.206184  21.618317  20.990105 -11.223962  ...  -0.532429   \n",
       "...          ...         ...        ...        ...        ...  ...        ...   \n",
       "2688 -452.831024   80.856758  19.427954  32.913322  -3.974150  ...   1.763288   \n",
       "2689 -450.678375   98.769257  -4.727075   7.584974 -11.716160  ...   3.983363   \n",
       "2690 -371.517609  117.556679   5.498871  27.535383   7.929699  ...  -1.782557   \n",
       "2691 -375.577759  106.716896   2.852894  26.215828 -20.312307  ...   4.127676   \n",
       "2692 -469.148651   96.403061  26.530113   8.035880  -2.224418  ...   5.588368   \n",
       "\n",
       "       mfcc_25    mfcc_26    mfcc_27   mfcc_28   mfcc_29    mfcc_30   mfcc_31  \\\n",
       "0     8.074034  11.653221  11.598641  8.817598  9.400865   8.987887  5.992146   \n",
       "1    -3.444622  -0.045880  -2.624972 -0.375320 -1.709600  -1.171931 -4.099090   \n",
       "2    -2.749439   1.098240   1.010692  0.460480 -1.992547   1.084825 -0.576132   \n",
       "3     7.379666   8.362466   1.706965  5.596114  5.339090   5.811155  1.621077   \n",
       "4    -1.102016   1.010110  -1.844896 -1.788239  0.140551  -2.089751 -1.204731   \n",
       "...        ...        ...        ...       ...       ...        ...       ...   \n",
       "2688  2.534321   1.811994   4.191569  6.094894  5.072481   7.664765  4.959315   \n",
       "2689  4.685560   2.902997   0.235584  2.744654  1.443039   5.997917  3.257085   \n",
       "2690 -2.510203  -2.095202  -3.341051 -1.580801 -0.980542   0.799156 -2.367372   \n",
       "2691  8.754350   5.358984   4.600071  9.529908  5.941333  14.158734  8.534500   \n",
       "2692  4.237176   7.683784   0.811043  1.432141  1.804263   4.709819  1.102094   \n",
       "\n",
       "        mfcc_32  newEmotion  \n",
       "0      4.622094       happy  \n",
       "1     -1.587989    surprise  \n",
       "2      0.662649     neutral  \n",
       "3      3.745970       happy  \n",
       "4     -1.450962       angry  \n",
       "...         ...         ...  \n",
       "2688   4.205091     neutral  \n",
       "2689   5.762399     neutral  \n",
       "2690   0.982645     neutral  \n",
       "2691  10.391392     neutral  \n",
       "2692  -0.579098     neutral  \n",
       "\n",
       "[2693 rows x 38 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dropna()\n",
    "test_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labeling 다시 (감정 레이블 7개만 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (\n",
    "    (train_df.newEmotion == 'neutral') |\n",
    "    (train_df.newEmotion == 'happy')|\n",
    "    (train_df.newEmotion == 'angry') |\n",
    "    (train_df.newEmotion == 'surprise') |\n",
    "    (train_df.newEmotion == 'sad') |\n",
    "    (train_df.newEmotion == 'disqust')|\n",
    "    (train_df.newEmotion == 'fear')\n",
    ")\n",
    "\n",
    "train_df = train_df.loc[condition]\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (\n",
    "    (test_df.newEmotion == 'neutral') |\n",
    "    (test_df.newEmotion == 'happy')|\n",
    "    (test_df.newEmotion == 'angry') |\n",
    "    (test_df.newEmotion == 'surprise') |\n",
    "    (test_df.newEmotion == 'sad') |\n",
    "    (test_df.newEmotion == 'disqust')|\n",
    "    (test_df.newEmotion == 'fear')\n",
    ")\n",
    "\n",
    "test_df = test_df.loc[condition]\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10754, 38)\n",
      "(2686, 38)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['newEmotion'] = train_df['newEmotion'].replace(['neutral',\n",
    "                                                         'happy',\n",
    "                                                         'angry',\n",
    "                                                         'surprise',\n",
    "                                                         'sad',\n",
    "                                                         'disqust',\n",
    "                                                         'fear'],\n",
    "                                                        [0,1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['newEmotion'] = test_df['newEmotion'].replace(['neutral',\n",
    "                                                         'happy',\n",
    "                                                         'angry',\n",
    "                                                         'surprise',\n",
    "                                                         'sad',\n",
    "                                                         'disqust',\n",
    "                                                         'fear'\n",
    "                                                      ],[0,1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## neutral 데이터 개수 줄이기 knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10754, 38)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5754, 38)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# train 데이터셋에서 중립 데이터 개수 줄이기\n",
    "neutral_indices = np.where(train_df['newEmotion'] == 0)[0]\n",
    "\n",
    "# # 중립 데이터 중 일부를 삭제\n",
    "num_to_delete = 5000\n",
    "delete_indices = np.random.choice(neutral_indices, size=num_to_delete, replace=False)\n",
    "#print(delete_indices.shape)\n",
    "# # 추출한 인덱스를 사용하여 중립 데이터를 삭제합니다.\n",
    "#print(train_df.shape)\n",
    "train_df = train_df.drop(delete_indices, axis=0)\n",
    "print(train_df.shape)\n",
    "# label_array = np.delete(label_array, delete_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1686, 38)\n"
     ]
    }
   ],
   "source": [
    "## test 데이터 중립 데이터 개수 줄이기\n",
    "neutral_indices = np.where(test_df['newEmotion'] == 0)[0] # 인덱스 추출\n",
    "\n",
    "# # 중립 데이터 중 일부를 삭제\n",
    "num_to_delete = 1000 \n",
    "delete_indices = np.random.choice(neutral_indices, size=num_to_delete, replace=False)\n",
    "#print(delete_indices.shape)\n",
    "# # 추출한 인덱스를 사용하여 중립 데이터를 삭제합니다.\n",
    "#print(train_df.shape)\n",
    "test_df = test_df.drop(delete_indices, axis=0)\n",
    "print(test_df.shape)\n",
    "# label_array = np.delete(label_array, delete_indices, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check sample ratio + oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3880\n",
       "1    1237\n",
       "2     180\n",
       "3     169\n",
       "4     154\n",
       "5      81\n",
       "6      53\n",
       "Name: newEmotion, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['newEmotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1240\n",
       "1     291\n",
       "2      47\n",
       "3      43\n",
       "4      38\n",
       "5      20\n",
       "6       7\n",
       "Name: newEmotion, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['newEmotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터를 모델의 input으로 들어갈 x와 label로 사용할 y로 분할\n",
    "train_x = train_df.drop(columns=['Unnamed: 0', 'SegmentId','time','Valence','Arousal','newEmotion'])\n",
    "train_y = train_df['newEmotion']\n",
    "test_x = test_df.drop(columns=['Unnamed: 0', 'SegmentId','time','Valence','Arousal','newEmotion'])\n",
    "test_y = test_df['newEmotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Samples')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels =['neutral', 'happy','angry','surprise','sad','disqust','fear']\n",
    "counts = train_y.value_counts().tolist()\n",
    "# 그래프 그리기\n",
    "plt.bar(labels, counts)\n",
    "\n",
    "# 그래프 타이틀, 라벨 등 설정\n",
    "plt.title('Class Imbalance')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Number of Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1. SMOTE (Oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm=SMOTE(k_neighbors=2, random_state=0) #same size of the training data set as random over sampling\n",
    "X_train_resampled_sm, Y_train_resampled_sm = sm.fit_resample(train_x,train_y)\n",
    "X_test_resampled_sm, Y_test_resampled_sm = sm.fit_resample(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Samples')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels =['neutral', 'happy','angry','surprise','sad','disqust','fear']\n",
    "counts = Y_train_resampled_sm.value_counts().tolist()\n",
    "# 그래프 그리기\n",
    "plt.bar(labels, counts)\n",
    "\n",
    "# 그래프 타이틀, 라벨 등 설정\n",
    "plt.title('Class Imbalance')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Number of Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    3880\n",
       "5    3880\n",
       "4    3880\n",
       "3    3880\n",
       "2    3880\n",
       "1    3880\n",
       "0    3880\n",
       "Name: newEmotion, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_resampled_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test_resampled_sm.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27160, 32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_sm = torch.from_numpy(X_train_resampled_sm.to_numpy()).float()\n",
    "Y_train_resampled_sm = torch.tensor(Y_train_resampled_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27160, 32])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_resampled_sm = torch.from_numpy(X_test_resampled_sm.to_numpy()).float()\n",
    "Y_test_resampled_sm = torch.tensor(Y_test_resampled_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/EmotionShortForm/wandb/run-20230421_021434-hyskp7h1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hwangyujeong/EmotionShortForm/runs/hyskp7h1' target=\"_blank\">graceful-voice-141</a></strong> to <a href='https://wandb.ai/hwangyujeong/EmotionShortForm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hwangyujeong/EmotionShortForm' target=\"_blank\">https://wandb.ai/hwangyujeong/EmotionShortForm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hwangyujeong/EmotionShortForm/runs/hyskp7h1' target=\"_blank\">https://wandb.ai/hwangyujeong/EmotionShortForm/runs/hyskp7h1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hwangyujeong/EmotionShortForm/runs/hyskp7h1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8411375430>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"EmotionShortForm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Baseline\")\n",
    "parser.add_argument('--optimizer', default=\"adam\", type=str) # sgd or adam\n",
    "parser.add_argument('--loss', default=\"fl\", type=str) # bc or fl or fl_af\n",
    "parser.add_argument('--learning_rate', default=0.001, type=float)\n",
    "parser.add_argument('--epochs', default=1500, type=int)\n",
    "parser.add_argument('--input_size', default=32, type=int) # 130\n",
    "parser.add_argument('--hidden_size', default=128, type=int)\n",
    "parser.add_argument('--output_size', default=7, type=int)\n",
    "parser.add_argument('--num_layers', default=3, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = args.optimizer\n",
    "loss = args.loss\n",
    "learning_rate = args.learning_rate\n",
    "EPOCHS = args.epochs\n",
    "input_size= args.input_size # number of input data size\n",
    "hidden_size=args.hidden_size\n",
    "output_size = args.output_size\n",
    "num_layers = args.num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size,  dropout_rate = 0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, (hidden_state, cell_state) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        output = self.fc(out[:,-1,:])\n",
    "        hidden = self.fc(hidden_state[-1])\n",
    "        output = F.softmax(output,dim=1) \n",
    "        return output, hidden,  out[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model, Criterion, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model=LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss, BCEWithLogitsLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hyskp7h1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b306b627c846d38d587e2e2b43173d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-voice-141</strong> at: <a href='https://wandb.ai/hwangyujeong/EmotionShortForm/runs/hyskp7h1' target=\"_blank\">https://wandb.ai/hwangyujeong/EmotionShortForm/runs/hyskp7h1</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230421_021434-hyskp7h1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hyskp7h1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fc6d559d204693a1cb3afbf28558fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669755871407687, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/EmotionShortForm/wandb/run-20230421_021440-gcceu9bq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hwangyujeong/EmotionShortForm/runs/gcceu9bq' target=\"_blank\">20230420_1</a></strong> to <a href='https://wandb.ai/hwangyujeong/EmotionShortForm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hwangyujeong/EmotionShortForm' target=\"_blank\">https://wandb.ai/hwangyujeong/EmotionShortForm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hwangyujeong/EmotionShortForm/runs/gcceu9bq' target=\"_blank\">https://wandb.ai/hwangyujeong/EmotionShortForm/runs/gcceu9bq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454fb6cd026e4a51825a309b9b7abef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1500 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1500], Loss: 1.9459\n",
      "min_f1: 0.03571428571428571\n",
      "Epoch [2/1500], Loss: 1.9459\n",
      "Epoch [3/1500], Loss: 1.9459\n",
      "Epoch [4/1500], Loss: 1.9458\n",
      "Epoch [5/1500], Loss: 1.9458\n",
      "Epoch [6/1500], Loss: 1.9458\n",
      "min_f1: 0.035717886307663496\n",
      "Epoch [7/1500], Loss: 1.9457\n",
      "min_f1: 0.03833774245734478\n",
      "Epoch [8/1500], Loss: 1.9457\n",
      "min_f1: 0.057694539527592924\n",
      "Epoch [9/1500], Loss: 1.9457\n",
      "min_f1: 0.08739525779359157\n",
      "Epoch [10/1500], Loss: 1.9456\n",
      "min_f1: 0.11262118848221322\n",
      "Epoch [11/1500], Loss: 1.9455\n",
      "min_f1: 0.1287965681220893\n",
      "Epoch [12/1500], Loss: 1.9455\n",
      "min_f1: 0.1448438554286398\n",
      "Epoch [13/1500], Loss: 1.9454\n",
      "min_f1: 0.15040204717997566\n",
      "Epoch [14/1500], Loss: 1.9453\n",
      "Epoch [15/1500], Loss: 1.9452\n",
      "Epoch [16/1500], Loss: 1.9450\n",
      "Epoch [17/1500], Loss: 1.9449\n",
      "min_f1: 0.16443062657838886\n",
      "Epoch [18/1500], Loss: 1.9447\n",
      "min_f1: 0.17528056147210805\n",
      "Epoch [19/1500], Loss: 1.9445\n",
      "Epoch [20/1500], Loss: 1.9442\n",
      "Epoch [21/1500], Loss: 1.9438\n",
      "Epoch [22/1500], Loss: 1.9434\n",
      "Epoch [23/1500], Loss: 1.9430\n",
      "Epoch [24/1500], Loss: 1.9425\n",
      "Epoch [25/1500], Loss: 1.9419\n",
      "Epoch [26/1500], Loss: 1.9410\n",
      "Epoch [27/1500], Loss: 1.9403\n",
      "Epoch [28/1500], Loss: 1.9394\n",
      "Epoch [29/1500], Loss: 1.9383\n",
      "Epoch [30/1500], Loss: 1.9370\n",
      "Epoch [31/1500], Loss: 1.9358\n",
      "Epoch [32/1500], Loss: 1.9344\n",
      "Epoch [33/1500], Loss: 1.9333\n",
      "Epoch [34/1500], Loss: 1.9318\n",
      "Epoch [35/1500], Loss: 1.9298\n",
      "Epoch [36/1500], Loss: 1.9277\n",
      "Epoch [37/1500], Loss: 1.9261\n",
      "Epoch [38/1500], Loss: 1.9246\n",
      "Epoch [39/1500], Loss: 1.9227\n",
      "Epoch [40/1500], Loss: 1.9209\n",
      "Epoch [41/1500], Loss: 1.9203\n",
      "Epoch [42/1500], Loss: 1.9182\n",
      "Epoch [43/1500], Loss: 1.9170\n",
      "Epoch [44/1500], Loss: 1.9152\n",
      "min_f1: 0.17562142021437913\n",
      "Epoch [45/1500], Loss: 1.9142\n",
      "min_f1: 0.180573223374489\n",
      "Epoch [46/1500], Loss: 1.9130\n",
      "min_f1: 0.19078908714823392\n",
      "Epoch [47/1500], Loss: 1.9120\n",
      "min_f1: 0.20200501064229423\n",
      "Epoch [48/1500], Loss: 1.9126\n",
      "min_f1: 0.21397233832566573\n",
      "Epoch [49/1500], Loss: 1.9110\n",
      "Epoch [50/1500], Loss: 1.9091\n",
      "Epoch [51/1500], Loss: 1.9073\n",
      "Epoch [52/1500], Loss: 1.9064\n",
      "min_f1: 0.2209720344385511\n",
      "Epoch [53/1500], Loss: 1.9044\n",
      "min_f1: 0.23161615593318663\n",
      "Epoch [54/1500], Loss: 1.9031\n",
      "min_f1: 0.2374889912804505\n",
      "Epoch [55/1500], Loss: 1.9018\n",
      "Epoch [56/1500], Loss: 1.9003\n",
      "Epoch [57/1500], Loss: 1.9004\n",
      "Epoch [58/1500], Loss: 1.8983\n",
      "Epoch [59/1500], Loss: 1.8963\n",
      "Epoch [60/1500], Loss: 1.8944\n",
      "Epoch [61/1500], Loss: 1.8930\n",
      "min_f1: 0.24416139320188765\n",
      "Epoch [62/1500], Loss: 1.8908\n",
      "min_f1: 0.2461193200934781\n",
      "Epoch [63/1500], Loss: 1.8899\n",
      "min_f1: 0.24762816296932502\n",
      "Epoch [64/1500], Loss: 1.8889\n",
      "Epoch [65/1500], Loss: 1.8886\n",
      "min_f1: 0.2490034541926688\n",
      "Epoch [66/1500], Loss: 1.8856\n",
      "Epoch [67/1500], Loss: 1.8847\n",
      "Epoch [68/1500], Loss: 1.8839\n",
      "Epoch [69/1500], Loss: 1.8859\n",
      "min_f1: 0.2577238079610687\n",
      "Epoch [70/1500], Loss: 1.8810\n",
      "min_f1: 0.2638408832230087\n",
      "Epoch [71/1500], Loss: 1.8831\n",
      "min_f1: 0.2646593769873976\n",
      "Epoch [72/1500], Loss: 1.8810\n",
      "min_f1: 0.26563329603398167\n",
      "Epoch [73/1500], Loss: 1.8792\n",
      "Epoch [74/1500], Loss: 1.8783\n",
      "Epoch [75/1500], Loss: 1.8776\n",
      "Epoch [76/1500], Loss: 1.8748\n",
      "Epoch [77/1500], Loss: 1.8753\n",
      "min_f1: 0.26945424307903265\n",
      "Epoch [78/1500], Loss: 1.8723\n",
      "min_f1: 0.27363650145940127\n",
      "Epoch [79/1500], Loss: 1.8720\n",
      "Epoch [80/1500], Loss: 1.8744\n",
      "Epoch [81/1500], Loss: 1.8689\n",
      "Epoch [82/1500], Loss: 1.8717\n",
      "Epoch [83/1500], Loss: 1.8691\n",
      "Epoch [84/1500], Loss: 1.8697\n",
      "Epoch [85/1500], Loss: 1.8675\n",
      "Epoch [86/1500], Loss: 1.8690\n",
      "Epoch [87/1500], Loss: 1.8693\n",
      "Epoch [88/1500], Loss: 1.8632\n",
      "Epoch [89/1500], Loss: 1.8629\n",
      "min_f1: 0.2739706663937653\n",
      "Epoch [90/1500], Loss: 1.8632\n",
      "min_f1: 0.27702101708103916\n",
      "Epoch [91/1500], Loss: 1.8636\n",
      "min_f1: 0.2799166758167338\n",
      "Epoch [92/1500], Loss: 1.8611\n",
      "min_f1: 0.28318246219925763\n",
      "Epoch [93/1500], Loss: 1.8615\n",
      "Epoch [94/1500], Loss: 1.8632\n",
      "Epoch [95/1500], Loss: 1.8605\n",
      "Epoch [96/1500], Loss: 1.8580\n",
      "Epoch [97/1500], Loss: 1.8576\n",
      "Epoch [98/1500], Loss: 1.8577\n",
      "Epoch [99/1500], Loss: 1.8560\n",
      "Epoch [100/1500], Loss: 1.8528\n",
      "Epoch [101/1500], Loss: 1.8549\n",
      "Epoch [102/1500], Loss: 1.8517\n",
      "Epoch [103/1500], Loss: 1.8508\n",
      "Epoch [104/1500], Loss: 1.8491\n",
      "Epoch [105/1500], Loss: 1.8509\n",
      "Epoch [106/1500], Loss: 1.8480\n",
      "Epoch [107/1500], Loss: 1.8506\n",
      "Epoch [108/1500], Loss: 1.8474\n",
      "Epoch [109/1500], Loss: 1.8474\n",
      "Epoch [110/1500], Loss: 1.8492\n",
      "Epoch [111/1500], Loss: 1.8455\n",
      "Epoch [112/1500], Loss: 1.8489\n",
      "Epoch [113/1500], Loss: 1.8478\n",
      "Epoch [114/1500], Loss: 1.8452\n",
      "Epoch [115/1500], Loss: 1.8430\n",
      "Epoch [116/1500], Loss: 1.8418\n",
      "Epoch [117/1500], Loss: 1.8402\n",
      "Epoch [118/1500], Loss: 1.8390\n",
      "Epoch [119/1500], Loss: 1.8381\n",
      "Epoch [120/1500], Loss: 1.8369\n",
      "Epoch [121/1500], Loss: 1.8390\n",
      "Epoch [122/1500], Loss: 1.8399\n",
      "Epoch [123/1500], Loss: 1.8363\n",
      "Epoch [124/1500], Loss: 1.8355\n",
      "Epoch [125/1500], Loss: 1.8361\n",
      "Epoch [126/1500], Loss: 1.8378\n",
      "Epoch [127/1500], Loss: 1.8366\n",
      "Epoch [128/1500], Loss: 1.8344\n",
      "Epoch [129/1500], Loss: 1.8323\n",
      "Epoch [130/1500], Loss: 1.8305\n",
      "Epoch [131/1500], Loss: 1.8326\n",
      "Epoch [132/1500], Loss: 1.8303\n",
      "Epoch [133/1500], Loss: 1.8286\n",
      "Epoch [134/1500], Loss: 1.8292\n",
      "Epoch [135/1500], Loss: 1.8283\n",
      "Epoch [136/1500], Loss: 1.8317\n",
      "Epoch [137/1500], Loss: 1.8270\n",
      "Epoch [138/1500], Loss: 1.8278\n",
      "Epoch [139/1500], Loss: 1.8258\n",
      "min_f1: 0.28701589530455\n",
      "Epoch [140/1500], Loss: 1.8260\n",
      "min_f1: 0.28910562868093387\n",
      "Epoch [141/1500], Loss: 1.8273\n",
      "Epoch [142/1500], Loss: 1.8263\n",
      "Epoch [143/1500], Loss: 1.8250\n",
      "Epoch [144/1500], Loss: 1.8233\n",
      "Epoch [145/1500], Loss: 1.8279\n",
      "Epoch [146/1500], Loss: 1.8246\n",
      "Epoch [147/1500], Loss: 1.8236\n",
      "min_f1: 0.2946415522401928\n",
      "Epoch [148/1500], Loss: 1.8193\n",
      "min_f1: 0.3016065311311965\n",
      "Epoch [149/1500], Loss: 1.8213\n",
      "min_f1: 0.3018719632454901\n",
      "Epoch [150/1500], Loss: 1.8190\n",
      "Epoch [151/1500], Loss: 1.8208\n",
      "Epoch [152/1500], Loss: 1.8201\n",
      "Epoch [153/1500], Loss: 1.8186\n",
      "Epoch [154/1500], Loss: 1.8171\n",
      "Epoch [155/1500], Loss: 1.8185\n",
      "Epoch [156/1500], Loss: 1.8163\n",
      "Epoch [157/1500], Loss: 1.8160\n",
      "Epoch [158/1500], Loss: 1.8197\n",
      "Epoch [159/1500], Loss: 1.8178\n",
      "min_f1: 0.3052840946890741\n",
      "Epoch [160/1500], Loss: 1.8160\n",
      "min_f1: 0.3116295587084532\n",
      "Epoch [161/1500], Loss: 1.8162\n",
      "Epoch [162/1500], Loss: 1.8153\n",
      "Epoch [163/1500], Loss: 1.8124\n",
      "Epoch [164/1500], Loss: 1.8124\n",
      "min_f1: 0.31251734500701944\n",
      "Epoch [165/1500], Loss: 1.8128\n",
      "Epoch [166/1500], Loss: 1.8125\n",
      "min_f1: 0.31421713634177456\n",
      "Epoch [167/1500], Loss: 1.8104\n",
      "min_f1: 0.31739422135544515\n",
      "Epoch [168/1500], Loss: 1.8110\n",
      "min_f1: 0.3177535182320592\n",
      "Epoch [169/1500], Loss: 1.8099\n",
      "min_f1: 0.3185850459826439\n",
      "Epoch [170/1500], Loss: 1.8082\n",
      "min_f1: 0.32497522063940937\n",
      "Epoch [171/1500], Loss: 1.8087\n",
      "min_f1: 0.32673509675125695\n",
      "Epoch [172/1500], Loss: 1.8101\n",
      "Epoch [173/1500], Loss: 1.8074\n",
      "Epoch [174/1500], Loss: 1.8062\n",
      "Epoch [175/1500], Loss: 1.8092\n",
      "Epoch [176/1500], Loss: 1.8082\n",
      "Epoch [177/1500], Loss: 1.8070\n",
      "Epoch [178/1500], Loss: 1.8074\n",
      "Epoch [179/1500], Loss: 1.8059\n",
      "Epoch [180/1500], Loss: 1.8049\n",
      "Epoch [181/1500], Loss: 1.8032\n",
      "Epoch [182/1500], Loss: 1.8055\n",
      "Epoch [183/1500], Loss: 1.8058\n",
      "Epoch [184/1500], Loss: 1.8048\n",
      "min_f1: 0.32779811184254676\n",
      "Epoch [185/1500], Loss: 1.8040\n",
      "Epoch [186/1500], Loss: 1.8045\n",
      "Epoch [187/1500], Loss: 1.8025\n",
      "Epoch [188/1500], Loss: 1.8017\n",
      "Epoch [189/1500], Loss: 1.8056\n",
      "Epoch [190/1500], Loss: 1.8033\n",
      "Epoch [191/1500], Loss: 1.7991\n",
      "Epoch [192/1500], Loss: 1.8008\n",
      "Epoch [193/1500], Loss: 1.7992\n",
      "Epoch [194/1500], Loss: 1.8043\n",
      "Epoch [195/1500], Loss: 1.8008\n",
      "Epoch [196/1500], Loss: 1.8020\n",
      "Epoch [197/1500], Loss: 1.8012\n",
      "Epoch [198/1500], Loss: 1.7994\n",
      "min_f1: 0.33040758145765203\n",
      "Epoch [199/1500], Loss: 1.8010\n",
      "Epoch [200/1500], Loss: 1.8017\n",
      "Epoch [201/1500], Loss: 1.8010\n",
      "Epoch [202/1500], Loss: 1.8016\n",
      "Epoch [203/1500], Loss: 1.7984\n",
      "Epoch [204/1500], Loss: 1.7988\n",
      "Epoch [205/1500], Loss: 1.7944\n",
      "Epoch [206/1500], Loss: 1.7951\n",
      "min_f1: 0.3393731810875283\n",
      "Epoch [207/1500], Loss: 1.7958\n",
      "Epoch [208/1500], Loss: 1.7982\n",
      "min_f1: 0.34080088366245226\n",
      "Epoch [209/1500], Loss: 1.7960\n",
      "Epoch [210/1500], Loss: 1.7974\n",
      "Epoch [211/1500], Loss: 1.7953\n",
      "Epoch [212/1500], Loss: 1.7943\n",
      "Epoch [213/1500], Loss: 1.7945\n",
      "min_f1: 0.3446454792420968\n",
      "Epoch [214/1500], Loss: 1.7940\n",
      "Epoch [215/1500], Loss: 1.7933\n",
      "Epoch [216/1500], Loss: 1.7930\n",
      "Epoch [217/1500], Loss: 1.7908\n",
      "Epoch [218/1500], Loss: 1.7927\n",
      "Epoch [219/1500], Loss: 1.7910\n",
      "Epoch [220/1500], Loss: 1.7932\n",
      "Epoch [221/1500], Loss: 1.7910\n",
      "Epoch [222/1500], Loss: 1.7913\n",
      "min_f1: 0.34481289562011763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [223/1500], Loss: 1.7933\n",
      "Epoch [224/1500], Loss: 1.7913\n",
      "Epoch [225/1500], Loss: 1.7924\n",
      "Epoch [226/1500], Loss: 1.7897\n",
      "Epoch [227/1500], Loss: 1.7933\n",
      "min_f1: 0.3467201534035258\n",
      "Epoch [228/1500], Loss: 1.7917\n",
      "min_f1: 0.3485237235219706\n",
      "Epoch [229/1500], Loss: 1.7910\n",
      "min_f1: 0.3517307213293365\n",
      "Epoch [230/1500], Loss: 1.7923\n",
      "Epoch [231/1500], Loss: 1.7903\n",
      "Epoch [232/1500], Loss: 1.7896\n",
      "Epoch [233/1500], Loss: 1.7905\n",
      "Epoch [234/1500], Loss: 1.7870\n",
      "Epoch [235/1500], Loss: 1.7910\n",
      "Epoch [236/1500], Loss: 1.7877\n",
      "Epoch [237/1500], Loss: 1.7880\n",
      "Epoch [238/1500], Loss: 1.7892\n",
      "Epoch [239/1500], Loss: 1.7899\n",
      "Epoch [240/1500], Loss: 1.7872\n",
      "Epoch [241/1500], Loss: 1.7817\n",
      "Epoch [242/1500], Loss: 1.7851\n",
      "Epoch [243/1500], Loss: 1.7876\n",
      "Epoch [244/1500], Loss: 1.7843\n",
      "Epoch [245/1500], Loss: 1.7842\n",
      "Epoch [246/1500], Loss: 1.7839\n",
      "Epoch [247/1500], Loss: 1.7840\n",
      "Epoch [248/1500], Loss: 1.7871\n",
      "Epoch [249/1500], Loss: 1.7870\n",
      "Epoch [250/1500], Loss: 1.7827\n",
      "Epoch [251/1500], Loss: 1.7858\n",
      "Epoch [252/1500], Loss: 1.7861\n",
      "min_f1: 0.3584228840070453\n",
      "Epoch [253/1500], Loss: 1.7835\n",
      "Epoch [254/1500], Loss: 1.7837\n",
      "Epoch [255/1500], Loss: 1.7837\n",
      "Epoch [256/1500], Loss: 1.7821\n",
      "Epoch [257/1500], Loss: 1.7810\n",
      "Epoch [258/1500], Loss: 1.7811\n",
      "Epoch [259/1500], Loss: 1.7841\n",
      "Epoch [260/1500], Loss: 1.7801\n",
      "Epoch [261/1500], Loss: 1.7855\n",
      "Epoch [262/1500], Loss: 1.7883\n",
      "Epoch [263/1500], Loss: 1.7818\n",
      "Epoch [264/1500], Loss: 1.7823\n",
      "Epoch [265/1500], Loss: 1.7823\n",
      "Epoch [266/1500], Loss: 1.7821\n",
      "Epoch [267/1500], Loss: 1.7814\n",
      "Epoch [268/1500], Loss: 1.7826\n",
      "Epoch [269/1500], Loss: 1.7820\n",
      "Epoch [270/1500], Loss: 1.7794\n",
      "Epoch [271/1500], Loss: 1.7820\n",
      "Epoch [272/1500], Loss: 1.7803\n",
      "Epoch [273/1500], Loss: 1.7752\n",
      "Epoch [274/1500], Loss: 1.7790\n",
      "Epoch [275/1500], Loss: 1.7775\n",
      "Epoch [276/1500], Loss: 1.7798\n",
      "Epoch [277/1500], Loss: 1.7791\n",
      "Epoch [278/1500], Loss: 1.7831\n",
      "Epoch [279/1500], Loss: 1.7784\n",
      "Epoch [280/1500], Loss: 1.7785\n",
      "Epoch [281/1500], Loss: 1.7763\n",
      "Epoch [282/1500], Loss: 1.7788\n",
      "Epoch [283/1500], Loss: 1.7765\n",
      "Epoch [284/1500], Loss: 1.7758\n",
      "Epoch [285/1500], Loss: 1.7773\n",
      "Epoch [286/1500], Loss: 1.7790\n",
      "Epoch [287/1500], Loss: 1.7785\n",
      "Epoch [288/1500], Loss: 1.7770\n",
      "min_f1: 0.36315063114922846\n",
      "Epoch [289/1500], Loss: 1.7733\n",
      "min_f1: 0.36866377512509896\n",
      "Epoch [290/1500], Loss: 1.7746\n",
      "Epoch [291/1500], Loss: 1.7782\n",
      "Epoch [292/1500], Loss: 1.7751\n",
      "Epoch [293/1500], Loss: 1.7753\n",
      "Epoch [294/1500], Loss: 1.7765\n",
      "Epoch [295/1500], Loss: 1.7749\n",
      "Epoch [296/1500], Loss: 1.7772\n",
      "Epoch [297/1500], Loss: 1.7751\n",
      "Epoch [298/1500], Loss: 1.7746\n",
      "Epoch [299/1500], Loss: 1.7751\n",
      "Epoch [300/1500], Loss: 1.7754\n",
      "Epoch [301/1500], Loss: 1.7750\n",
      "Epoch [302/1500], Loss: 1.7734\n",
      "Epoch [303/1500], Loss: 1.7756\n",
      "Epoch [304/1500], Loss: 1.7781\n",
      "Epoch [305/1500], Loss: 1.7760\n",
      "Epoch [306/1500], Loss: 1.7782\n",
      "Epoch [307/1500], Loss: 1.7731\n",
      "Epoch [308/1500], Loss: 1.7763\n",
      "Epoch [309/1500], Loss: 1.7746\n",
      "Epoch [310/1500], Loss: 1.7764\n",
      "Epoch [311/1500], Loss: 1.7726\n",
      "Epoch [312/1500], Loss: 1.7718\n",
      "Epoch [313/1500], Loss: 1.7724\n",
      "Epoch [314/1500], Loss: 1.7757\n",
      "Epoch [315/1500], Loss: 1.7732\n",
      "Epoch [316/1500], Loss: 1.7717\n",
      "Epoch [317/1500], Loss: 1.7717\n",
      "Epoch [318/1500], Loss: 1.7724\n",
      "Epoch [319/1500], Loss: 1.7694\n",
      "Epoch [320/1500], Loss: 1.7682\n",
      "Epoch [321/1500], Loss: 1.7744\n",
      "Epoch [322/1500], Loss: 1.7682\n",
      "Epoch [323/1500], Loss: 1.7704\n",
      "Epoch [324/1500], Loss: 1.7726\n",
      "Epoch [325/1500], Loss: 1.7716\n",
      "Epoch [326/1500], Loss: 1.7678\n",
      "Epoch [327/1500], Loss: 1.7713\n",
      "Epoch [328/1500], Loss: 1.7722\n",
      "Epoch [329/1500], Loss: 1.7681\n",
      "Epoch [330/1500], Loss: 1.7722\n",
      "Epoch [331/1500], Loss: 1.7697\n",
      "Epoch [332/1500], Loss: 1.7695\n",
      "Epoch [333/1500], Loss: 1.7735\n",
      "Epoch [334/1500], Loss: 1.7719\n",
      "Epoch [335/1500], Loss: 1.7714\n",
      "Epoch [336/1500], Loss: 1.7715\n",
      "Epoch [337/1500], Loss: 1.7689\n",
      "Epoch [338/1500], Loss: 1.7655\n",
      "Epoch [339/1500], Loss: 1.7695\n",
      "Epoch [340/1500], Loss: 1.7702\n",
      "Epoch [341/1500], Loss: 1.7677\n",
      "Epoch [342/1500], Loss: 1.7677\n",
      "Epoch [343/1500], Loss: 1.7660\n",
      "Epoch [344/1500], Loss: 1.7698\n",
      "Epoch [345/1500], Loss: 1.7718\n",
      "Epoch [346/1500], Loss: 1.7661\n",
      "Epoch [347/1500], Loss: 1.7707\n",
      "Epoch [348/1500], Loss: 1.7672\n",
      "Epoch [349/1500], Loss: 1.7679\n",
      "Epoch [350/1500], Loss: 1.7661\n",
      "Epoch [351/1500], Loss: 1.7685\n",
      "Epoch [352/1500], Loss: 1.7655\n",
      "Epoch [353/1500], Loss: 1.7661\n",
      "Epoch [354/1500], Loss: 1.7690\n",
      "Epoch [355/1500], Loss: 1.7706\n",
      "Epoch [356/1500], Loss: 1.7659\n",
      "Epoch [357/1500], Loss: 1.7654\n",
      "Epoch [358/1500], Loss: 1.7635\n",
      "Epoch [359/1500], Loss: 1.7670\n",
      "Epoch [360/1500], Loss: 1.7638\n",
      "Epoch [361/1500], Loss: 1.7683\n",
      "Epoch [362/1500], Loss: 1.7665\n",
      "Epoch [363/1500], Loss: 1.7654\n",
      "Epoch [364/1500], Loss: 1.7676\n",
      "Epoch [365/1500], Loss: 1.7651\n",
      "Epoch [366/1500], Loss: 1.7668\n",
      "Epoch [367/1500], Loss: 1.7663\n",
      "Epoch [368/1500], Loss: 1.7607\n",
      "Epoch [369/1500], Loss: 1.7680\n",
      "Epoch [370/1500], Loss: 1.7675\n",
      "Epoch [371/1500], Loss: 1.7641\n",
      "Epoch [372/1500], Loss: 1.7678\n",
      "Epoch [373/1500], Loss: 1.7657\n",
      "Epoch [374/1500], Loss: 1.7681\n",
      "Epoch [375/1500], Loss: 1.7672\n",
      "Epoch [376/1500], Loss: 1.7653\n",
      "Epoch [377/1500], Loss: 1.7649\n",
      "Epoch [378/1500], Loss: 1.7625\n",
      "Epoch [379/1500], Loss: 1.7649\n",
      "Epoch [380/1500], Loss: 1.7661\n",
      "Epoch [381/1500], Loss: 1.7637\n",
      "Epoch [382/1500], Loss: 1.7648\n",
      "Epoch [383/1500], Loss: 1.7684\n",
      "Epoch [384/1500], Loss: 1.7651\n",
      "Epoch [385/1500], Loss: 1.7646\n",
      "Epoch [386/1500], Loss: 1.7642\n",
      "Epoch [387/1500], Loss: 1.7633\n",
      "Epoch [388/1500], Loss: 1.7633\n",
      "Epoch [389/1500], Loss: 1.7623\n",
      "Epoch [390/1500], Loss: 1.7656\n",
      "Epoch [391/1500], Loss: 1.7646\n",
      "Epoch [392/1500], Loss: 1.7619\n",
      "Epoch [393/1500], Loss: 1.7586\n",
      "Epoch [394/1500], Loss: 1.7648\n",
      "Epoch [395/1500], Loss: 1.7633\n",
      "Epoch [396/1500], Loss: 1.7612\n",
      "Epoch [397/1500], Loss: 1.7605\n",
      "Epoch [398/1500], Loss: 1.7594\n",
      "Epoch [399/1500], Loss: 1.7619\n",
      "Epoch [400/1500], Loss: 1.7611\n",
      "Epoch [401/1500], Loss: 1.7624\n",
      "Epoch [402/1500], Loss: 1.7610\n",
      "Epoch [403/1500], Loss: 1.7639\n",
      "Epoch [404/1500], Loss: 1.7653\n",
      "Epoch [405/1500], Loss: 1.7640\n",
      "Epoch [406/1500], Loss: 1.7618\n",
      "Epoch [407/1500], Loss: 1.7623\n",
      "Epoch [408/1500], Loss: 1.7648\n",
      "Epoch [409/1500], Loss: 1.7599\n",
      "Epoch [410/1500], Loss: 1.7613\n",
      "Epoch [411/1500], Loss: 1.7605\n",
      "Epoch [412/1500], Loss: 1.7629\n",
      "Epoch [413/1500], Loss: 1.7602\n",
      "Epoch [414/1500], Loss: 1.7603\n",
      "Epoch [415/1500], Loss: 1.7563\n",
      "Epoch [416/1500], Loss: 1.7616\n",
      "Epoch [417/1500], Loss: 1.7617\n",
      "Epoch [418/1500], Loss: 1.7611\n",
      "Epoch [419/1500], Loss: 1.7609\n",
      "Epoch [420/1500], Loss: 1.7586\n",
      "Epoch [421/1500], Loss: 1.7602\n",
      "Epoch [422/1500], Loss: 1.7607\n",
      "Epoch [423/1500], Loss: 1.7609\n",
      "Epoch [424/1500], Loss: 1.7573\n",
      "Epoch [425/1500], Loss: 1.7594\n",
      "Epoch [426/1500], Loss: 1.7582\n",
      "Epoch [427/1500], Loss: 1.7608\n",
      "Epoch [428/1500], Loss: 1.7629\n",
      "Epoch [429/1500], Loss: 1.7597\n",
      "min_f1: 0.3694044043483352\n",
      "Epoch [430/1500], Loss: 1.7592\n",
      "Epoch [431/1500], Loss: 1.7615\n",
      "Epoch [432/1500], Loss: 1.7598\n",
      "Epoch [433/1500], Loss: 1.7618\n",
      "Epoch [434/1500], Loss: 1.7581\n",
      "Epoch [435/1500], Loss: 1.7600\n",
      "Epoch [436/1500], Loss: 1.7557\n",
      "Epoch [437/1500], Loss: 1.7601\n",
      "Epoch [438/1500], Loss: 1.7581\n",
      "Epoch [439/1500], Loss: 1.7578\n",
      "Epoch [440/1500], Loss: 1.7602\n",
      "Epoch [441/1500], Loss: 1.7604\n",
      "Epoch [442/1500], Loss: 1.7576\n",
      "Epoch [443/1500], Loss: 1.7581\n",
      "Epoch [444/1500], Loss: 1.7587\n",
      "Epoch [445/1500], Loss: 1.7599\n",
      "Epoch [446/1500], Loss: 1.7596\n",
      "Epoch [447/1500], Loss: 1.7575\n",
      "Epoch [448/1500], Loss: 1.7570\n",
      "Epoch [449/1500], Loss: 1.7589\n",
      "Epoch [450/1500], Loss: 1.7580\n",
      "Epoch [451/1500], Loss: 1.7577\n",
      "Epoch [452/1500], Loss: 1.7571\n",
      "Epoch [453/1500], Loss: 1.7561\n",
      "Epoch [454/1500], Loss: 1.7593\n",
      "Epoch [455/1500], Loss: 1.7592\n",
      "Epoch [456/1500], Loss: 1.7557\n",
      "Epoch [457/1500], Loss: 1.7586\n",
      "Epoch [458/1500], Loss: 1.7576\n",
      "Epoch [459/1500], Loss: 1.7595\n",
      "Epoch [460/1500], Loss: 1.7571\n",
      "Epoch [461/1500], Loss: 1.7584\n",
      "Epoch [462/1500], Loss: 1.7557\n",
      "Epoch [463/1500], Loss: 1.7558\n",
      "Epoch [464/1500], Loss: 1.7540\n",
      "Epoch [465/1500], Loss: 1.7549\n",
      "Epoch [466/1500], Loss: 1.7585\n",
      "Epoch [467/1500], Loss: 1.7547\n",
      "Epoch [468/1500], Loss: 1.7558\n",
      "Epoch [469/1500], Loss: 1.7548\n",
      "Epoch [470/1500], Loss: 1.7551\n",
      "Epoch [471/1500], Loss: 1.7547\n",
      "Epoch [472/1500], Loss: 1.7550\n",
      "Epoch [473/1500], Loss: 1.7536\n",
      "Epoch [474/1500], Loss: 1.7558\n",
      "Epoch [475/1500], Loss: 1.7567\n",
      "Epoch [476/1500], Loss: 1.7534\n",
      "Epoch [477/1500], Loss: 1.7570\n",
      "Epoch [478/1500], Loss: 1.7548\n",
      "Epoch [479/1500], Loss: 1.7575\n",
      "Epoch [480/1500], Loss: 1.7534\n",
      "Epoch [481/1500], Loss: 1.7592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [482/1500], Loss: 1.7552\n",
      "Epoch [483/1500], Loss: 1.7538\n",
      "Epoch [484/1500], Loss: 1.7494\n",
      "Epoch [485/1500], Loss: 1.7515\n",
      "Epoch [486/1500], Loss: 1.7550\n",
      "Epoch [487/1500], Loss: 1.7535\n",
      "Epoch [488/1500], Loss: 1.7531\n",
      "Epoch [489/1500], Loss: 1.7536\n",
      "Epoch [490/1500], Loss: 1.7539\n",
      "Epoch [491/1500], Loss: 1.7567\n",
      "Epoch [492/1500], Loss: 1.7560\n",
      "Epoch [493/1500], Loss: 1.7566\n",
      "Epoch [494/1500], Loss: 1.7532\n",
      "Epoch [495/1500], Loss: 1.7550\n",
      "Epoch [496/1500], Loss: 1.7575\n",
      "Epoch [497/1500], Loss: 1.7507\n",
      "Epoch [498/1500], Loss: 1.7534\n",
      "Epoch [499/1500], Loss: 1.7503\n",
      "Epoch [500/1500], Loss: 1.7532\n",
      "Epoch [501/1500], Loss: 1.7526\n",
      "Epoch [502/1500], Loss: 1.7549\n",
      "Epoch [503/1500], Loss: 1.7557\n",
      "Epoch [504/1500], Loss: 1.7532\n",
      "Epoch [505/1500], Loss: 1.7529\n",
      "Epoch [506/1500], Loss: 1.7538\n",
      "Epoch [507/1500], Loss: 1.7542\n",
      "Epoch [508/1500], Loss: 1.7518\n",
      "Epoch [509/1500], Loss: 1.7534\n",
      "Epoch [510/1500], Loss: 1.7557\n",
      "Epoch [511/1500], Loss: 1.7538\n",
      "Epoch [512/1500], Loss: 1.7489\n",
      "Epoch [513/1500], Loss: 1.7525\n",
      "Epoch [514/1500], Loss: 1.7534\n",
      "Epoch [515/1500], Loss: 1.7545\n",
      "Epoch [516/1500], Loss: 1.7531\n",
      "Epoch [517/1500], Loss: 1.7563\n",
      "Epoch [518/1500], Loss: 1.7527\n",
      "Epoch [519/1500], Loss: 1.7534\n",
      "Epoch [520/1500], Loss: 1.7542\n",
      "Epoch [521/1500], Loss: 1.7536\n",
      "Epoch [522/1500], Loss: 1.7462\n",
      "Epoch [523/1500], Loss: 1.7509\n",
      "Epoch [524/1500], Loss: 1.7515\n",
      "Epoch [525/1500], Loss: 1.7531\n",
      "Epoch [526/1500], Loss: 1.7533\n",
      "Epoch [527/1500], Loss: 1.7497\n",
      "Epoch [528/1500], Loss: 1.7515\n",
      "Epoch [529/1500], Loss: 1.7520\n",
      "Epoch [530/1500], Loss: 1.7478\n",
      "Epoch [531/1500], Loss: 1.7515\n",
      "Epoch [532/1500], Loss: 1.7508\n",
      "Epoch [533/1500], Loss: 1.7525\n",
      "Epoch [534/1500], Loss: 1.7507\n",
      "Epoch [535/1500], Loss: 1.7506\n",
      "Epoch [536/1500], Loss: 1.7481\n",
      "Epoch [537/1500], Loss: 1.7490\n",
      "Epoch [538/1500], Loss: 1.7481\n",
      "Epoch [539/1500], Loss: 1.7495\n",
      "Epoch [540/1500], Loss: 1.7533\n",
      "Epoch [541/1500], Loss: 1.7543\n",
      "Epoch [542/1500], Loss: 1.7514\n",
      "Epoch [543/1500], Loss: 1.7514\n",
      "Epoch [544/1500], Loss: 1.7482\n",
      "Epoch [545/1500], Loss: 1.7544\n",
      "Epoch [546/1500], Loss: 1.7485\n",
      "Epoch [547/1500], Loss: 1.7531\n",
      "Epoch [548/1500], Loss: 1.7501\n",
      "Epoch [549/1500], Loss: 1.7493\n",
      "Epoch [550/1500], Loss: 1.7457\n",
      "Epoch [551/1500], Loss: 1.7516\n",
      "Epoch [552/1500], Loss: 1.7517\n",
      "Epoch [553/1500], Loss: 1.7477\n",
      "Epoch [554/1500], Loss: 1.7509\n",
      "Epoch [555/1500], Loss: 1.7481\n",
      "Epoch [556/1500], Loss: 1.7515\n",
      "Epoch [557/1500], Loss: 1.7501\n",
      "Epoch [558/1500], Loss: 1.7458\n",
      "Epoch [559/1500], Loss: 1.7451\n",
      "Epoch [560/1500], Loss: 1.7488\n",
      "Epoch [561/1500], Loss: 1.7470\n",
      "Epoch [562/1500], Loss: 1.7474\n",
      "Epoch [563/1500], Loss: 1.7468\n",
      "Epoch [564/1500], Loss: 1.7485\n",
      "Epoch [565/1500], Loss: 1.7491\n",
      "Epoch [566/1500], Loss: 1.7473\n",
      "Epoch [567/1500], Loss: 1.7492\n",
      "Epoch [568/1500], Loss: 1.7489\n",
      "Epoch [569/1500], Loss: 1.7490\n",
      "Epoch [570/1500], Loss: 1.7533\n",
      "Epoch [571/1500], Loss: 1.7497\n",
      "Epoch [572/1500], Loss: 1.7503\n",
      "Epoch [573/1500], Loss: 1.7510\n",
      "Epoch [574/1500], Loss: 1.7503\n",
      "Epoch [575/1500], Loss: 1.7507\n",
      "Epoch [576/1500], Loss: 1.7492\n",
      "Epoch [577/1500], Loss: 1.7471\n",
      "Epoch [578/1500], Loss: 1.7487\n",
      "Epoch [579/1500], Loss: 1.7497\n",
      "Epoch [580/1500], Loss: 1.7489\n",
      "Epoch [581/1500], Loss: 1.7475\n",
      "Epoch [582/1500], Loss: 1.7495\n",
      "Epoch [583/1500], Loss: 1.7475\n",
      "Epoch [584/1500], Loss: 1.7495\n",
      "Epoch [585/1500], Loss: 1.7515\n",
      "Epoch [586/1500], Loss: 1.7467\n",
      "Epoch [587/1500], Loss: 1.7498\n",
      "Epoch [588/1500], Loss: 1.7460\n",
      "Epoch [589/1500], Loss: 1.7498\n",
      "Epoch [590/1500], Loss: 1.7464\n",
      "Epoch [591/1500], Loss: 1.7472\n",
      "Epoch [592/1500], Loss: 1.7469\n",
      "Epoch [593/1500], Loss: 1.7506\n",
      "Epoch [594/1500], Loss: 1.7451\n",
      "Epoch [595/1500], Loss: 1.7471\n",
      "Epoch [596/1500], Loss: 1.7474\n",
      "Epoch [597/1500], Loss: 1.7425\n",
      "Epoch [598/1500], Loss: 1.7459\n",
      "Epoch [599/1500], Loss: 1.7453\n",
      "Epoch [600/1500], Loss: 1.7501\n",
      "Epoch [601/1500], Loss: 1.7452\n",
      "Epoch [602/1500], Loss: 1.7482\n",
      "Epoch [603/1500], Loss: 1.7488\n",
      "Epoch [604/1500], Loss: 1.7480\n",
      "Epoch [605/1500], Loss: 1.7485\n",
      "Epoch [606/1500], Loss: 1.7469\n",
      "Epoch [607/1500], Loss: 1.7430\n",
      "Epoch [608/1500], Loss: 1.7462\n",
      "Epoch [609/1500], Loss: 1.7468\n",
      "Epoch [610/1500], Loss: 1.7452\n",
      "Epoch [611/1500], Loss: 1.7465\n",
      "Epoch [612/1500], Loss: 1.7476\n",
      "Epoch [613/1500], Loss: 1.7439\n",
      "Epoch [614/1500], Loss: 1.7468\n",
      "Epoch [615/1500], Loss: 1.7467\n",
      "Epoch [616/1500], Loss: 1.7438\n",
      "Epoch [617/1500], Loss: 1.7475\n",
      "Epoch [618/1500], Loss: 1.7455\n",
      "Epoch [619/1500], Loss: 1.7464\n",
      "Epoch [620/1500], Loss: 1.7501\n",
      "Epoch [621/1500], Loss: 1.7447\n",
      "Epoch [622/1500], Loss: 1.7478\n",
      "Epoch [623/1500], Loss: 1.7443\n",
      "Epoch [624/1500], Loss: 1.7464\n",
      "Epoch [625/1500], Loss: 1.7427\n",
      "Epoch [626/1500], Loss: 1.7477\n",
      "Epoch [627/1500], Loss: 1.7451\n",
      "Epoch [628/1500], Loss: 1.7424\n",
      "Epoch [629/1500], Loss: 1.7435\n",
      "Epoch [630/1500], Loss: 1.7473\n",
      "Epoch [631/1500], Loss: 1.7465\n",
      "Epoch [632/1500], Loss: 1.7458\n",
      "Epoch [633/1500], Loss: 1.7457\n",
      "Epoch [634/1500], Loss: 1.7457\n",
      "Epoch [635/1500], Loss: 1.7432\n",
      "Epoch [636/1500], Loss: 1.7472\n",
      "Epoch [637/1500], Loss: 1.7486\n",
      "Epoch [638/1500], Loss: 1.7450\n",
      "Epoch [639/1500], Loss: 1.7456\n",
      "Epoch [640/1500], Loss: 1.7448\n",
      "Epoch [641/1500], Loss: 1.7444\n",
      "Epoch [642/1500], Loss: 1.7469\n",
      "Epoch [643/1500], Loss: 1.7513\n",
      "Epoch [644/1500], Loss: 1.7467\n",
      "min_f1: 0.3721866223380078\n",
      "Epoch [645/1500], Loss: 1.7443\n",
      "Epoch [646/1500], Loss: 1.7472\n",
      "Epoch [647/1500], Loss: 1.7476\n",
      "Epoch [648/1500], Loss: 1.7433\n",
      "Epoch [649/1500], Loss: 1.7455\n",
      "Epoch [650/1500], Loss: 1.7428\n",
      "Epoch [651/1500], Loss: 1.7441\n",
      "Epoch [652/1500], Loss: 1.7471\n",
      "Epoch [653/1500], Loss: 1.7508\n",
      "Epoch [654/1500], Loss: 1.7465\n",
      "Epoch [655/1500], Loss: 1.7501\n",
      "Epoch [656/1500], Loss: 1.7419\n",
      "Epoch [657/1500], Loss: 1.7454\n",
      "Epoch [658/1500], Loss: 1.7448\n",
      "Epoch [659/1500], Loss: 1.7439\n",
      "Epoch [660/1500], Loss: 1.7416\n",
      "Epoch [661/1500], Loss: 1.7440\n",
      "Epoch [662/1500], Loss: 1.7446\n",
      "Epoch [663/1500], Loss: 1.7420\n",
      "Epoch [664/1500], Loss: 1.7441\n",
      "Epoch [665/1500], Loss: 1.7462\n",
      "Epoch [666/1500], Loss: 1.7440\n",
      "Epoch [667/1500], Loss: 1.7476\n",
      "Epoch [668/1500], Loss: 1.7467\n",
      "Epoch [669/1500], Loss: 1.7432\n",
      "Epoch [670/1500], Loss: 1.7445\n",
      "Epoch [671/1500], Loss: 1.7423\n",
      "Epoch [672/1500], Loss: 1.7438\n",
      "Epoch [673/1500], Loss: 1.7394\n",
      "Epoch [674/1500], Loss: 1.7444\n",
      "Epoch [675/1500], Loss: 1.7406\n",
      "Epoch [676/1500], Loss: 1.7462\n",
      "Epoch [677/1500], Loss: 1.7463\n",
      "Epoch [678/1500], Loss: 1.7445\n",
      "Epoch [679/1500], Loss: 1.7411\n",
      "Epoch [680/1500], Loss: 1.7441\n",
      "Epoch [681/1500], Loss: 1.7414\n",
      "Epoch [682/1500], Loss: 1.7458\n",
      "Epoch [683/1500], Loss: 1.7430\n",
      "Epoch [684/1500], Loss: 1.7452\n",
      "Epoch [685/1500], Loss: 1.7447\n",
      "Epoch [686/1500], Loss: 1.7429\n",
      "Epoch [687/1500], Loss: 1.7405\n",
      "Epoch [688/1500], Loss: 1.7431\n",
      "Epoch [689/1500], Loss: 1.7387\n",
      "Epoch [690/1500], Loss: 1.7447\n",
      "Epoch [691/1500], Loss: 1.7437\n",
      "Epoch [692/1500], Loss: 1.7429\n",
      "Epoch [693/1500], Loss: 1.7438\n",
      "Epoch [694/1500], Loss: 1.7448\n",
      "Epoch [695/1500], Loss: 1.7434\n",
      "Epoch [696/1500], Loss: 1.7446\n",
      "Epoch [697/1500], Loss: 1.7450\n",
      "Epoch [698/1500], Loss: 1.7420\n",
      "Epoch [699/1500], Loss: 1.7438\n",
      "Epoch [700/1500], Loss: 1.7405\n",
      "Epoch [701/1500], Loss: 1.7440\n",
      "Epoch [702/1500], Loss: 1.7414\n",
      "Epoch [703/1500], Loss: 1.7438\n",
      "Epoch [704/1500], Loss: 1.7408\n",
      "Epoch [705/1500], Loss: 1.7419\n",
      "Epoch [706/1500], Loss: 1.7421\n",
      "Epoch [707/1500], Loss: 1.7386\n",
      "Epoch [708/1500], Loss: 1.7427\n",
      "Epoch [709/1500], Loss: 1.7417\n",
      "Epoch [710/1500], Loss: 1.7414\n",
      "Epoch [711/1500], Loss: 1.7440\n",
      "Epoch [712/1500], Loss: 1.7412\n",
      "Epoch [713/1500], Loss: 1.7421\n",
      "Epoch [714/1500], Loss: 1.7444\n",
      "Epoch [715/1500], Loss: 1.7419\n",
      "Epoch [716/1500], Loss: 1.7410\n",
      "Epoch [717/1500], Loss: 1.7444\n",
      "Epoch [718/1500], Loss: 1.7393\n",
      "Epoch [719/1500], Loss: 1.7452\n",
      "Epoch [720/1500], Loss: 1.7382\n",
      "Epoch [721/1500], Loss: 1.7433\n",
      "Epoch [722/1500], Loss: 1.7412\n",
      "Epoch [723/1500], Loss: 1.7406\n",
      "Epoch [724/1500], Loss: 1.7383\n",
      "Epoch [725/1500], Loss: 1.7391\n",
      "min_f1: 0.37458614423689773\n",
      "Epoch [726/1500], Loss: 1.7404\n",
      "min_f1: 0.3746951517443301\n",
      "Epoch [727/1500], Loss: 1.7390\n",
      "Epoch [728/1500], Loss: 1.7420\n",
      "Epoch [729/1500], Loss: 1.7398\n",
      "Epoch [730/1500], Loss: 1.7424\n",
      "Epoch [731/1500], Loss: 1.7397\n",
      "Epoch [732/1500], Loss: 1.7414\n",
      "Epoch [733/1500], Loss: 1.7402\n",
      "Epoch [734/1500], Loss: 1.7416\n",
      "Epoch [735/1500], Loss: 1.7397\n",
      "Epoch [736/1500], Loss: 1.7395\n",
      "Epoch [737/1500], Loss: 1.7392\n",
      "Epoch [738/1500], Loss: 1.7407\n",
      "Epoch [739/1500], Loss: 1.7422\n",
      "Epoch [740/1500], Loss: 1.7429\n",
      "Epoch [741/1500], Loss: 1.7438\n",
      "Epoch [742/1500], Loss: 1.7418\n",
      "Epoch [743/1500], Loss: 1.7394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [744/1500], Loss: 1.7396\n",
      "Epoch [745/1500], Loss: 1.7371\n",
      "Epoch [746/1500], Loss: 1.7425\n",
      "Epoch [747/1500], Loss: 1.7451\n",
      "Epoch [748/1500], Loss: 1.7426\n",
      "Epoch [749/1500], Loss: 1.7415\n",
      "Epoch [750/1500], Loss: 1.7421\n",
      "Epoch [751/1500], Loss: 1.7392\n",
      "Epoch [752/1500], Loss: 1.7413\n",
      "Epoch [753/1500], Loss: 1.7408\n",
      "Epoch [754/1500], Loss: 1.7399\n",
      "Epoch [755/1500], Loss: 1.7419\n",
      "Epoch [756/1500], Loss: 1.7385\n",
      "Epoch [757/1500], Loss: 1.7402\n",
      "Epoch [758/1500], Loss: 1.7405\n",
      "Epoch [759/1500], Loss: 1.7458\n",
      "Epoch [760/1500], Loss: 1.7402\n",
      "Epoch [761/1500], Loss: 1.7368\n",
      "Epoch [762/1500], Loss: 1.7416\n",
      "Epoch [763/1500], Loss: 1.7378\n",
      "Epoch [764/1500], Loss: 1.7390\n",
      "Epoch [765/1500], Loss: 1.7442\n",
      "Epoch [766/1500], Loss: 1.7398\n",
      "Epoch [767/1500], Loss: 1.7351\n",
      "Epoch [768/1500], Loss: 1.7398\n",
      "Epoch [769/1500], Loss: 1.7392\n",
      "Epoch [770/1500], Loss: 1.7388\n",
      "Epoch [771/1500], Loss: 1.7393\n",
      "Epoch [772/1500], Loss: 1.7380\n",
      "Epoch [773/1500], Loss: 1.7425\n",
      "Epoch [774/1500], Loss: 1.7374\n",
      "Epoch [775/1500], Loss: 1.7406\n",
      "Epoch [776/1500], Loss: 1.7371\n",
      "Epoch [777/1500], Loss: 1.7399\n",
      "Epoch [778/1500], Loss: 1.7405\n",
      "Epoch [779/1500], Loss: 1.7393\n",
      "Epoch [780/1500], Loss: 1.7395\n",
      "Epoch [781/1500], Loss: 1.7371\n",
      "Epoch [782/1500], Loss: 1.7361\n",
      "Epoch [783/1500], Loss: 1.7403\n",
      "min_f1: 0.3779648684086046\n",
      "Epoch [784/1500], Loss: 1.7378\n",
      "min_f1: 0.3788718372532623\n",
      "Epoch [785/1500], Loss: 1.7380\n",
      "min_f1: 0.38317357295565363\n",
      "Epoch [786/1500], Loss: 1.7416\n",
      "Epoch [787/1500], Loss: 1.7413\n",
      "Epoch [788/1500], Loss: 1.7381\n",
      "Epoch [789/1500], Loss: 1.7374\n",
      "Epoch [790/1500], Loss: 1.7409\n",
      "Epoch [791/1500], Loss: 1.7418\n",
      "Epoch [792/1500], Loss: 1.7394\n",
      "Epoch [793/1500], Loss: 1.7403\n",
      "Epoch [794/1500], Loss: 1.7385\n",
      "Epoch [795/1500], Loss: 1.7375\n",
      "Epoch [796/1500], Loss: 1.7396\n",
      "Epoch [797/1500], Loss: 1.7368\n",
      "Epoch [798/1500], Loss: 1.7394\n",
      "Epoch [799/1500], Loss: 1.7389\n",
      "Epoch [800/1500], Loss: 1.7347\n",
      "Epoch [801/1500], Loss: 1.7361\n",
      "Epoch [802/1500], Loss: 1.7410\n",
      "Epoch [803/1500], Loss: 1.7370\n",
      "Epoch [804/1500], Loss: 1.7382\n",
      "Epoch [805/1500], Loss: 1.7375\n",
      "Epoch [806/1500], Loss: 1.7393\n",
      "Epoch [807/1500], Loss: 1.7381\n",
      "Epoch [808/1500], Loss: 1.7330\n",
      "Epoch [809/1500], Loss: 1.7375\n",
      "Epoch [810/1500], Loss: 1.7423\n",
      "Epoch [811/1500], Loss: 1.7371\n",
      "Epoch [812/1500], Loss: 1.7383\n",
      "Epoch [813/1500], Loss: 1.7378\n",
      "Epoch [814/1500], Loss: 1.7393\n",
      "Epoch [815/1500], Loss: 1.7373\n",
      "Epoch [816/1500], Loss: 1.7336\n",
      "Epoch [817/1500], Loss: 1.7339\n",
      "Epoch [818/1500], Loss: 1.7379\n",
      "Epoch [819/1500], Loss: 1.7354\n",
      "Epoch [820/1500], Loss: 1.7422\n",
      "Epoch [821/1500], Loss: 1.7351\n",
      "Epoch [822/1500], Loss: 1.7361\n",
      "Epoch [823/1500], Loss: 1.7398\n",
      "Epoch [824/1500], Loss: 1.7353\n",
      "Epoch [825/1500], Loss: 1.7351\n",
      "Epoch [826/1500], Loss: 1.7377\n",
      "Epoch [827/1500], Loss: 1.7385\n",
      "Epoch [828/1500], Loss: 1.7382\n",
      "Epoch [829/1500], Loss: 1.7366\n",
      "Epoch [830/1500], Loss: 1.7391\n",
      "Epoch [831/1500], Loss: 1.7354\n",
      "Epoch [832/1500], Loss: 1.7378\n",
      "Epoch [833/1500], Loss: 1.7365\n",
      "Epoch [834/1500], Loss: 1.7380\n",
      "Epoch [835/1500], Loss: 1.7387\n",
      "Epoch [836/1500], Loss: 1.7368\n",
      "Epoch [837/1500], Loss: 1.7405\n",
      "Epoch [838/1500], Loss: 1.7372\n",
      "Epoch [839/1500], Loss: 1.7400\n",
      "Epoch [840/1500], Loss: 1.7384\n",
      "Epoch [841/1500], Loss: 1.7388\n",
      "Epoch [842/1500], Loss: 1.7371\n",
      "Epoch [843/1500], Loss: 1.7364\n",
      "Epoch [844/1500], Loss: 1.7381\n",
      "Epoch [845/1500], Loss: 1.7378\n",
      "Epoch [846/1500], Loss: 1.7329\n",
      "Epoch [847/1500], Loss: 1.7326\n",
      "Epoch [848/1500], Loss: 1.7354\n",
      "Epoch [849/1500], Loss: 1.7362\n",
      "Epoch [850/1500], Loss: 1.7339\n",
      "Epoch [851/1500], Loss: 1.7398\n",
      "Epoch [852/1500], Loss: 1.7384\n",
      "Epoch [853/1500], Loss: 1.7384\n",
      "Epoch [854/1500], Loss: 1.7324\n",
      "Epoch [855/1500], Loss: 1.7366\n",
      "Epoch [856/1500], Loss: 1.7352\n",
      "Epoch [857/1500], Loss: 1.7363\n",
      "Epoch [858/1500], Loss: 1.7348\n",
      "Epoch [859/1500], Loss: 1.7345\n",
      "Epoch [860/1500], Loss: 1.7357\n",
      "Epoch [861/1500], Loss: 1.7355\n",
      "Epoch [862/1500], Loss: 1.7386\n",
      "Epoch [863/1500], Loss: 1.7365\n",
      "Epoch [864/1500], Loss: 1.7373\n",
      "Epoch [865/1500], Loss: 1.7331\n",
      "min_f1: 0.38496856661348033\n",
      "Epoch [866/1500], Loss: 1.7322\n",
      "min_f1: 0.3850000084309234\n",
      "Epoch [867/1500], Loss: 1.7366\n",
      "Epoch [868/1500], Loss: 1.7395\n",
      "Epoch [869/1500], Loss: 1.7371\n",
      "Epoch [870/1500], Loss: 1.7355\n",
      "Epoch [871/1500], Loss: 1.7342\n",
      "Epoch [872/1500], Loss: 1.7344\n",
      "Epoch [873/1500], Loss: 1.7345\n",
      "Epoch [874/1500], Loss: 1.7347\n",
      "Epoch [875/1500], Loss: 1.7342\n",
      "Epoch [876/1500], Loss: 1.7373\n",
      "Epoch [877/1500], Loss: 1.7366\n",
      "Epoch [878/1500], Loss: 1.7356\n",
      "Epoch [879/1500], Loss: 1.7388\n",
      "Epoch [880/1500], Loss: 1.7334\n",
      "Epoch [881/1500], Loss: 1.7323\n",
      "Epoch [882/1500], Loss: 1.7350\n",
      "Epoch [883/1500], Loss: 1.7359\n",
      "Epoch [884/1500], Loss: 1.7339\n",
      "Epoch [885/1500], Loss: 1.7331\n",
      "Epoch [886/1500], Loss: 1.7337\n",
      "Epoch [887/1500], Loss: 1.7403\n",
      "Epoch [888/1500], Loss: 1.7329\n",
      "Epoch [889/1500], Loss: 1.7349\n",
      "Epoch [890/1500], Loss: 1.7355\n",
      "Epoch [891/1500], Loss: 1.7382\n",
      "Epoch [892/1500], Loss: 1.7333\n",
      "Epoch [893/1500], Loss: 1.7355\n",
      "Epoch [894/1500], Loss: 1.7348\n",
      "Epoch [895/1500], Loss: 1.7333\n",
      "Epoch [896/1500], Loss: 1.7327\n",
      "Epoch [897/1500], Loss: 1.7345\n",
      "Epoch [898/1500], Loss: 1.7366\n",
      "Epoch [899/1500], Loss: 1.7343\n",
      "Epoch [900/1500], Loss: 1.7323\n",
      "Epoch [901/1500], Loss: 1.7334\n",
      "Epoch [902/1500], Loss: 1.7335\n",
      "Epoch [903/1500], Loss: 1.7343\n",
      "Epoch [904/1500], Loss: 1.7381\n",
      "Epoch [905/1500], Loss: 1.7346\n",
      "Epoch [906/1500], Loss: 1.7368\n",
      "Epoch [907/1500], Loss: 1.7344\n",
      "Epoch [908/1500], Loss: 1.7328\n",
      "Epoch [909/1500], Loss: 1.7349\n",
      "Epoch [910/1500], Loss: 1.7352\n",
      "Epoch [911/1500], Loss: 1.7332\n",
      "Epoch [912/1500], Loss: 1.7288\n",
      "Epoch [913/1500], Loss: 1.7367\n",
      "Epoch [914/1500], Loss: 1.7341\n",
      "Epoch [915/1500], Loss: 1.7345\n",
      "Epoch [916/1500], Loss: 1.7365\n",
      "Epoch [917/1500], Loss: 1.7352\n",
      "Epoch [918/1500], Loss: 1.7353\n",
      "Epoch [919/1500], Loss: 1.7324\n",
      "Epoch [920/1500], Loss: 1.7339\n",
      "Epoch [921/1500], Loss: 1.7336\n",
      "Epoch [922/1500], Loss: 1.7351\n",
      "Epoch [923/1500], Loss: 1.7358\n",
      "Epoch [924/1500], Loss: 1.7333\n",
      "Epoch [925/1500], Loss: 1.7328\n",
      "Epoch [926/1500], Loss: 1.7361\n",
      "Epoch [927/1500], Loss: 1.7325\n",
      "Epoch [928/1500], Loss: 1.7307\n",
      "Epoch [929/1500], Loss: 1.7329\n",
      "Epoch [930/1500], Loss: 1.7315\n",
      "Epoch [931/1500], Loss: 1.7346\n",
      "Epoch [932/1500], Loss: 1.7359\n",
      "Epoch [933/1500], Loss: 1.7293\n",
      "Epoch [934/1500], Loss: 1.7296\n",
      "Epoch [935/1500], Loss: 1.7323\n",
      "Epoch [936/1500], Loss: 1.7362\n",
      "Epoch [937/1500], Loss: 1.7334\n",
      "Epoch [938/1500], Loss: 1.7340\n",
      "Epoch [939/1500], Loss: 1.7333\n",
      "Epoch [940/1500], Loss: 1.7303\n",
      "Epoch [941/1500], Loss: 1.7299\n",
      "Epoch [942/1500], Loss: 1.7354\n",
      "Epoch [943/1500], Loss: 1.7334\n",
      "Epoch [944/1500], Loss: 1.7346\n",
      "Epoch [945/1500], Loss: 1.7313\n",
      "Epoch [946/1500], Loss: 1.7361\n",
      "Epoch [947/1500], Loss: 1.7356\n",
      "Epoch [948/1500], Loss: 1.7290\n",
      "Epoch [949/1500], Loss: 1.7354\n",
      "Epoch [950/1500], Loss: 1.7325\n",
      "Epoch [951/1500], Loss: 1.7302\n",
      "Epoch [952/1500], Loss: 1.7368\n",
      "Epoch [953/1500], Loss: 1.7340\n",
      "Epoch [954/1500], Loss: 1.7305\n",
      "Epoch [955/1500], Loss: 1.7355\n",
      "Epoch [956/1500], Loss: 1.7299\n",
      "Epoch [957/1500], Loss: 1.7328\n",
      "Epoch [958/1500], Loss: 1.7331\n",
      "Epoch [959/1500], Loss: 1.7334\n",
      "Epoch [960/1500], Loss: 1.7313\n",
      "Epoch [961/1500], Loss: 1.7348\n",
      "Epoch [962/1500], Loss: 1.7316\n",
      "Epoch [963/1500], Loss: 1.7351\n",
      "Epoch [964/1500], Loss: 1.7310\n",
      "Epoch [965/1500], Loss: 1.7312\n",
      "Epoch [966/1500], Loss: 1.7325\n",
      "Epoch [967/1500], Loss: 1.7365\n",
      "Epoch [968/1500], Loss: 1.7318\n",
      "Epoch [969/1500], Loss: 1.7320\n",
      "Epoch [970/1500], Loss: 1.7284\n",
      "Epoch [971/1500], Loss: 1.7322\n",
      "Epoch [972/1500], Loss: 1.7345\n",
      "Epoch [973/1500], Loss: 1.7301\n",
      "Epoch [974/1500], Loss: 1.7323\n",
      "Epoch [975/1500], Loss: 1.7308\n",
      "Epoch [976/1500], Loss: 1.7334\n",
      "Epoch [977/1500], Loss: 1.7301\n",
      "Epoch [978/1500], Loss: 1.7328\n",
      "Epoch [979/1500], Loss: 1.7345\n",
      "Epoch [980/1500], Loss: 1.7316\n",
      "Epoch [981/1500], Loss: 1.7303\n",
      "Epoch [982/1500], Loss: 1.7308\n",
      "Epoch [983/1500], Loss: 1.7324\n",
      "Epoch [984/1500], Loss: 1.7342\n",
      "Epoch [985/1500], Loss: 1.7381\n",
      "Epoch [986/1500], Loss: 1.7306\n",
      "Epoch [987/1500], Loss: 1.7318\n",
      "Epoch [988/1500], Loss: 1.7359\n",
      "Epoch [989/1500], Loss: 1.7273\n",
      "Epoch [990/1500], Loss: 1.7319\n",
      "Epoch [991/1500], Loss: 1.7317\n",
      "Epoch [992/1500], Loss: 1.7344\n",
      "Epoch [993/1500], Loss: 1.7295\n",
      "Epoch [994/1500], Loss: 1.7295\n",
      "Epoch [995/1500], Loss: 1.7325\n",
      "Epoch [996/1500], Loss: 1.7286\n",
      "Epoch [997/1500], Loss: 1.7330\n",
      "Epoch [998/1500], Loss: 1.7306\n",
      "Epoch [999/1500], Loss: 1.7309\n",
      "Epoch [1000/1500], Loss: 1.7310\n",
      "Epoch [1001/1500], Loss: 1.7307\n",
      "Epoch [1002/1500], Loss: 1.7359\n",
      "Epoch [1003/1500], Loss: 1.7308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1004/1500], Loss: 1.7324\n",
      "Epoch [1005/1500], Loss: 1.7333\n",
      "Epoch [1006/1500], Loss: 1.7316\n",
      "Epoch [1007/1500], Loss: 1.7270\n",
      "Epoch [1008/1500], Loss: 1.7301\n",
      "Epoch [1009/1500], Loss: 1.7300\n",
      "Epoch [1010/1500], Loss: 1.7323\n",
      "Epoch [1011/1500], Loss: 1.7299\n",
      "Epoch [1012/1500], Loss: 1.7297\n",
      "Epoch [1013/1500], Loss: 1.7342\n",
      "Epoch [1014/1500], Loss: 1.7318\n",
      "Epoch [1015/1500], Loss: 1.7294\n",
      "Epoch [1016/1500], Loss: 1.7305\n",
      "Epoch [1017/1500], Loss: 1.7278\n",
      "Epoch [1018/1500], Loss: 1.7269\n",
      "Epoch [1019/1500], Loss: 1.7297\n",
      "Epoch [1020/1500], Loss: 1.7304\n",
      "Epoch [1021/1500], Loss: 1.7289\n",
      "Epoch [1022/1500], Loss: 1.7322\n",
      "Epoch [1023/1500], Loss: 1.7304\n",
      "Epoch [1024/1500], Loss: 1.7321\n",
      "Epoch [1025/1500], Loss: 1.7287\n",
      "Epoch [1026/1500], Loss: 1.7309\n",
      "Epoch [1027/1500], Loss: 1.7304\n",
      "Epoch [1028/1500], Loss: 1.7319\n",
      "Epoch [1029/1500], Loss: 1.7269\n",
      "Epoch [1030/1500], Loss: 1.7293\n",
      "Epoch [1031/1500], Loss: 1.7295\n",
      "Epoch [1032/1500], Loss: 1.7330\n",
      "Epoch [1033/1500], Loss: 1.7330\n",
      "Epoch [1034/1500], Loss: 1.7297\n",
      "Epoch [1035/1500], Loss: 1.7308\n",
      "Epoch [1036/1500], Loss: 1.7304\n",
      "Epoch [1037/1500], Loss: 1.7284\n",
      "Epoch [1038/1500], Loss: 1.7307\n",
      "Epoch [1039/1500], Loss: 1.7300\n",
      "Epoch [1040/1500], Loss: 1.7297\n",
      "Epoch [1041/1500], Loss: 1.7307\n",
      "Epoch [1042/1500], Loss: 1.7299\n",
      "Epoch [1043/1500], Loss: 1.7294\n",
      "Epoch [1044/1500], Loss: 1.7308\n",
      "Epoch [1045/1500], Loss: 1.7299\n",
      "Epoch [1046/1500], Loss: 1.7356\n",
      "Epoch [1047/1500], Loss: 1.7299\n",
      "Epoch [1048/1500], Loss: 1.7317\n",
      "Epoch [1049/1500], Loss: 1.7283\n",
      "Epoch [1050/1500], Loss: 1.7282\n",
      "Epoch [1051/1500], Loss: 1.7324\n",
      "Epoch [1052/1500], Loss: 1.7361\n",
      "Epoch [1053/1500], Loss: 1.7290\n",
      "Epoch [1054/1500], Loss: 1.7279\n",
      "Epoch [1055/1500], Loss: 1.7320\n",
      "Epoch [1056/1500], Loss: 1.7314\n",
      "Epoch [1057/1500], Loss: 1.7324\n",
      "Epoch [1058/1500], Loss: 1.7304\n",
      "Epoch [1059/1500], Loss: 1.7302\n",
      "Epoch [1060/1500], Loss: 1.7305\n",
      "Epoch [1061/1500], Loss: 1.7307\n",
      "Epoch [1062/1500], Loss: 1.7283\n",
      "Epoch [1063/1500], Loss: 1.7277\n",
      "Epoch [1064/1500], Loss: 1.7289\n",
      "Epoch [1065/1500], Loss: 1.7295\n",
      "Epoch [1066/1500], Loss: 1.7290\n",
      "Epoch [1067/1500], Loss: 1.7285\n",
      "Epoch [1068/1500], Loss: 1.7306\n",
      "Epoch [1069/1500], Loss: 1.7307\n",
      "Epoch [1070/1500], Loss: 1.7293\n",
      "Epoch [1071/1500], Loss: 1.7296\n",
      "Epoch [1072/1500], Loss: 1.7303\n",
      "Epoch [1073/1500], Loss: 1.7321\n",
      "Epoch [1074/1500], Loss: 1.7324\n",
      "Epoch [1075/1500], Loss: 1.7266\n",
      "Epoch [1076/1500], Loss: 1.7292\n",
      "Epoch [1077/1500], Loss: 1.7317\n",
      "Epoch [1078/1500], Loss: 1.7308\n",
      "Epoch [1079/1500], Loss: 1.7301\n",
      "Epoch [1080/1500], Loss: 1.7284\n",
      "Epoch [1081/1500], Loss: 1.7292\n",
      "Epoch [1082/1500], Loss: 1.7289\n",
      "min_f1: 0.3864090473132045\n",
      "Epoch [1083/1500], Loss: 1.7290\n",
      "Epoch [1084/1500], Loss: 1.7284\n",
      "Epoch [1085/1500], Loss: 1.7260\n",
      "Epoch [1086/1500], Loss: 1.7301\n",
      "Epoch [1087/1500], Loss: 1.7284\n",
      "Epoch [1088/1500], Loss: 1.7282\n",
      "Epoch [1089/1500], Loss: 1.7299\n",
      "Epoch [1090/1500], Loss: 1.7329\n",
      "Epoch [1091/1500], Loss: 1.7313\n",
      "Epoch [1092/1500], Loss: 1.7307\n",
      "Epoch [1093/1500], Loss: 1.7264\n",
      "Epoch [1094/1500], Loss: 1.7294\n",
      "Epoch [1095/1500], Loss: 1.7305\n",
      "Epoch [1096/1500], Loss: 1.7287\n",
      "Epoch [1097/1500], Loss: 1.7276\n",
      "Epoch [1098/1500], Loss: 1.7304\n",
      "Epoch [1099/1500], Loss: 1.7307\n",
      "Epoch [1100/1500], Loss: 1.7319\n",
      "Epoch [1101/1500], Loss: 1.7287\n",
      "Epoch [1102/1500], Loss: 1.7259\n",
      "Epoch [1103/1500], Loss: 1.7318\n",
      "Epoch [1104/1500], Loss: 1.7308\n",
      "Epoch [1105/1500], Loss: 1.7291\n",
      "Epoch [1106/1500], Loss: 1.7263\n",
      "Epoch [1107/1500], Loss: 1.7257\n",
      "Epoch [1108/1500], Loss: 1.7325\n",
      "Epoch [1109/1500], Loss: 1.7317\n",
      "Epoch [1110/1500], Loss: 1.7272\n",
      "Epoch [1111/1500], Loss: 1.7274\n",
      "Epoch [1112/1500], Loss: 1.7294\n",
      "Epoch [1113/1500], Loss: 1.7281\n",
      "Epoch [1114/1500], Loss: 1.7288\n",
      "Epoch [1115/1500], Loss: 1.7287\n",
      "Epoch [1116/1500], Loss: 1.7298\n",
      "Epoch [1117/1500], Loss: 1.7275\n",
      "Epoch [1118/1500], Loss: 1.7264\n",
      "Epoch [1119/1500], Loss: 1.7276\n",
      "Epoch [1120/1500], Loss: 1.7279\n",
      "Epoch [1121/1500], Loss: 1.7262\n",
      "Epoch [1122/1500], Loss: 1.7263\n",
      "Epoch [1123/1500], Loss: 1.7267\n",
      "Epoch [1124/1500], Loss: 1.7275\n",
      "Epoch [1125/1500], Loss: 1.7258\n",
      "Epoch [1126/1500], Loss: 1.7271\n",
      "Epoch [1127/1500], Loss: 1.7299\n",
      "Epoch [1128/1500], Loss: 1.7283\n",
      "Epoch [1129/1500], Loss: 1.7281\n",
      "Epoch [1130/1500], Loss: 1.7281\n",
      "Epoch [1131/1500], Loss: 1.7303\n",
      "Epoch [1132/1500], Loss: 1.7319\n",
      "Epoch [1133/1500], Loss: 1.7271\n",
      "Epoch [1134/1500], Loss: 1.7308\n",
      "Epoch [1135/1500], Loss: 1.7316\n",
      "Epoch [1136/1500], Loss: 1.7300\n",
      "Epoch [1137/1500], Loss: 1.7329\n",
      "Epoch [1138/1500], Loss: 1.7275\n",
      "Epoch [1139/1500], Loss: 1.7298\n",
      "Epoch [1140/1500], Loss: 1.7290\n",
      "Epoch [1141/1500], Loss: 1.7301\n",
      "Epoch [1142/1500], Loss: 1.7293\n",
      "Epoch [1143/1500], Loss: 1.7307\n",
      "Epoch [1144/1500], Loss: 1.7279\n",
      "Epoch [1145/1500], Loss: 1.7329\n",
      "Epoch [1146/1500], Loss: 1.7294\n",
      "Epoch [1147/1500], Loss: 1.7321\n",
      "Epoch [1148/1500], Loss: 1.7285\n",
      "Epoch [1149/1500], Loss: 1.7304\n",
      "Epoch [1150/1500], Loss: 1.7263\n",
      "Epoch [1151/1500], Loss: 1.7241\n",
      "Epoch [1152/1500], Loss: 1.7272\n",
      "Epoch [1153/1500], Loss: 1.7298\n",
      "Epoch [1154/1500], Loss: 1.7287\n",
      "Epoch [1155/1500], Loss: 1.7315\n",
      "Epoch [1156/1500], Loss: 1.7267\n",
      "Epoch [1157/1500], Loss: 1.7246\n",
      "Epoch [1158/1500], Loss: 1.7273\n",
      "Epoch [1159/1500], Loss: 1.7296\n",
      "Epoch [1160/1500], Loss: 1.7288\n",
      "Epoch [1161/1500], Loss: 1.7276\n",
      "Epoch [1162/1500], Loss: 1.7278\n",
      "Epoch [1163/1500], Loss: 1.7306\n",
      "Epoch [1164/1500], Loss: 1.7288\n",
      "Epoch [1165/1500], Loss: 1.7293\n",
      "Epoch [1166/1500], Loss: 1.7291\n",
      "Epoch [1167/1500], Loss: 1.7258\n",
      "Epoch [1168/1500], Loss: 1.7259\n",
      "Epoch [1169/1500], Loss: 1.7285\n",
      "Epoch [1170/1500], Loss: 1.7310\n",
      "Epoch [1171/1500], Loss: 1.7265\n",
      "Epoch [1172/1500], Loss: 1.7319\n",
      "Epoch [1173/1500], Loss: 1.7303\n",
      "Epoch [1174/1500], Loss: 1.7284\n",
      "Epoch [1175/1500], Loss: 1.7257\n",
      "Epoch [1176/1500], Loss: 1.7253\n",
      "Epoch [1177/1500], Loss: 1.7247\n",
      "Epoch [1178/1500], Loss: 1.7245\n",
      "Epoch [1179/1500], Loss: 1.7246\n",
      "Epoch [1180/1500], Loss: 1.7248\n",
      "Epoch [1181/1500], Loss: 1.7284\n",
      "Epoch [1182/1500], Loss: 1.7279\n",
      "Epoch [1183/1500], Loss: 1.7232\n",
      "Epoch [1184/1500], Loss: 1.7258\n",
      "Epoch [1185/1500], Loss: 1.7245\n",
      "Epoch [1186/1500], Loss: 1.7248\n",
      "Epoch [1187/1500], Loss: 1.7222\n",
      "Epoch [1188/1500], Loss: 1.7296\n",
      "Epoch [1189/1500], Loss: 1.7267\n",
      "Epoch [1190/1500], Loss: 1.7279\n",
      "Epoch [1191/1500], Loss: 1.7266\n",
      "Epoch [1192/1500], Loss: 1.7263\n",
      "Epoch [1193/1500], Loss: 1.7255\n",
      "Epoch [1194/1500], Loss: 1.7274\n",
      "min_f1: 0.38880378394527915\n",
      "Epoch [1195/1500], Loss: 1.7272\n",
      "min_f1: 0.3922465711705769\n",
      "Epoch [1196/1500], Loss: 1.7255\n",
      "min_f1: 0.3958098134343545\n",
      "Epoch [1197/1500], Loss: 1.7283\n",
      "Epoch [1198/1500], Loss: 1.7282\n",
      "Epoch [1199/1500], Loss: 1.7220\n",
      "Epoch [1200/1500], Loss: 1.7241\n",
      "Epoch [1201/1500], Loss: 1.7280\n",
      "Epoch [1202/1500], Loss: 1.7268\n",
      "Epoch [1203/1500], Loss: 1.7216\n",
      "Epoch [1204/1500], Loss: 1.7264\n",
      "Epoch [1205/1500], Loss: 1.7289\n",
      "Epoch [1206/1500], Loss: 1.7273\n",
      "Epoch [1207/1500], Loss: 1.7281\n",
      "Epoch [1208/1500], Loss: 1.7283\n",
      "Epoch [1209/1500], Loss: 1.7273\n",
      "Epoch [1210/1500], Loss: 1.7252\n",
      "Epoch [1211/1500], Loss: 1.7254\n",
      "Epoch [1212/1500], Loss: 1.7244\n",
      "Epoch [1213/1500], Loss: 1.7245\n",
      "Epoch [1214/1500], Loss: 1.7304\n",
      "Epoch [1215/1500], Loss: 1.7293\n",
      "Epoch [1216/1500], Loss: 1.7297\n",
      "Epoch [1217/1500], Loss: 1.7262\n",
      "Epoch [1218/1500], Loss: 1.7244\n",
      "Epoch [1219/1500], Loss: 1.7271\n",
      "Epoch [1220/1500], Loss: 1.7282\n",
      "Epoch [1221/1500], Loss: 1.7214\n",
      "Epoch [1222/1500], Loss: 1.7281\n",
      "Epoch [1223/1500], Loss: 1.7253\n",
      "Epoch [1224/1500], Loss: 1.7226\n",
      "Epoch [1225/1500], Loss: 1.7228\n",
      "Epoch [1226/1500], Loss: 1.7236\n",
      "Epoch [1227/1500], Loss: 1.7284\n",
      "Epoch [1228/1500], Loss: 1.7300\n",
      "Epoch [1229/1500], Loss: 1.7255\n",
      "Epoch [1230/1500], Loss: 1.7280\n",
      "Epoch [1231/1500], Loss: 1.7236\n",
      "Epoch [1232/1500], Loss: 1.7275\n",
      "Epoch [1233/1500], Loss: 1.7247\n",
      "Epoch [1234/1500], Loss: 1.7297\n",
      "Epoch [1235/1500], Loss: 1.7267\n",
      "Epoch [1236/1500], Loss: 1.7293\n",
      "Epoch [1237/1500], Loss: 1.7298\n",
      "Epoch [1238/1500], Loss: 1.7269\n",
      "Epoch [1239/1500], Loss: 1.7266\n",
      "Epoch [1240/1500], Loss: 1.7290\n",
      "Epoch [1241/1500], Loss: 1.7295\n",
      "Epoch [1242/1500], Loss: 1.7275\n",
      "Epoch [1243/1500], Loss: 1.7254\n",
      "Epoch [1244/1500], Loss: 1.7253\n",
      "Epoch [1245/1500], Loss: 1.7224\n",
      "Epoch [1246/1500], Loss: 1.7254\n",
      "Epoch [1247/1500], Loss: 1.7255\n",
      "Epoch [1248/1500], Loss: 1.7273\n",
      "Epoch [1249/1500], Loss: 1.7228\n",
      "Epoch [1250/1500], Loss: 1.7276\n",
      "Epoch [1251/1500], Loss: 1.7280\n",
      "Epoch [1252/1500], Loss: 1.7275\n",
      "Epoch [1253/1500], Loss: 1.7261\n",
      "min_f1: 0.39676196178417117\n",
      "Epoch [1254/1500], Loss: 1.7256\n",
      "Epoch [1255/1500], Loss: 1.7286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1256/1500], Loss: 1.7226\n",
      "Epoch [1257/1500], Loss: 1.7264\n",
      "Epoch [1258/1500], Loss: 1.7245\n",
      "Epoch [1259/1500], Loss: 1.7250\n",
      "Epoch [1260/1500], Loss: 1.7286\n",
      "Epoch [1261/1500], Loss: 1.7240\n",
      "Epoch [1262/1500], Loss: 1.7277\n",
      "Epoch [1263/1500], Loss: 1.7244\n",
      "Epoch [1264/1500], Loss: 1.7230\n",
      "Epoch [1265/1500], Loss: 1.7227\n",
      "Epoch [1266/1500], Loss: 1.7233\n",
      "Epoch [1267/1500], Loss: 1.7271\n",
      "Epoch [1268/1500], Loss: 1.7272\n",
      "Epoch [1269/1500], Loss: 1.7279\n",
      "Epoch [1270/1500], Loss: 1.7228\n",
      "Epoch [1271/1500], Loss: 1.7225\n",
      "Epoch [1272/1500], Loss: 1.7257\n",
      "Epoch [1273/1500], Loss: 1.7263\n",
      "Epoch [1274/1500], Loss: 1.7252\n",
      "Epoch [1275/1500], Loss: 1.7281\n",
      "Epoch [1276/1500], Loss: 1.7251\n",
      "Epoch [1277/1500], Loss: 1.7260\n",
      "Epoch [1278/1500], Loss: 1.7250\n",
      "Epoch [1279/1500], Loss: 1.7250\n",
      "Epoch [1280/1500], Loss: 1.7255\n",
      "Epoch [1281/1500], Loss: 1.7272\n",
      "Epoch [1282/1500], Loss: 1.7223\n",
      "Epoch [1283/1500], Loss: 1.7259\n",
      "Epoch [1284/1500], Loss: 1.7231\n",
      "Epoch [1285/1500], Loss: 1.7231\n",
      "Epoch [1286/1500], Loss: 1.7244\n",
      "Epoch [1287/1500], Loss: 1.7253\n",
      "Epoch [1288/1500], Loss: 1.7246\n",
      "Epoch [1289/1500], Loss: 1.7228\n",
      "Epoch [1290/1500], Loss: 1.7249\n",
      "Epoch [1291/1500], Loss: 1.7260\n",
      "Epoch [1292/1500], Loss: 1.7224\n",
      "Epoch [1293/1500], Loss: 1.7217\n",
      "Epoch [1294/1500], Loss: 1.7205\n",
      "Epoch [1295/1500], Loss: 1.7230\n",
      "Epoch [1296/1500], Loss: 1.7255\n",
      "Epoch [1297/1500], Loss: 1.7230\n",
      "Epoch [1298/1500], Loss: 1.7254\n",
      "Epoch [1299/1500], Loss: 1.7236\n",
      "Epoch [1300/1500], Loss: 1.7282\n",
      "Epoch [1301/1500], Loss: 1.7244\n",
      "Epoch [1302/1500], Loss: 1.7245\n",
      "Epoch [1303/1500], Loss: 1.7241\n",
      "Epoch [1304/1500], Loss: 1.7227\n",
      "Epoch [1305/1500], Loss: 1.7245\n",
      "Epoch [1306/1500], Loss: 1.7244\n",
      "Epoch [1307/1500], Loss: 1.7258\n",
      "Epoch [1308/1500], Loss: 1.7233\n",
      "Epoch [1309/1500], Loss: 1.7214\n",
      "Epoch [1310/1500], Loss: 1.7286\n",
      "Epoch [1311/1500], Loss: 1.7231\n",
      "Epoch [1312/1500], Loss: 1.7243\n",
      "Epoch [1313/1500], Loss: 1.7262\n",
      "Epoch [1314/1500], Loss: 1.7192\n",
      "Epoch [1315/1500], Loss: 1.7226\n",
      "Epoch [1316/1500], Loss: 1.7223\n",
      "Epoch [1317/1500], Loss: 1.7218\n",
      "Epoch [1318/1500], Loss: 1.7248\n",
      "Epoch [1319/1500], Loss: 1.7245\n",
      "Epoch [1320/1500], Loss: 1.7257\n",
      "Epoch [1321/1500], Loss: 1.7227\n",
      "Epoch [1322/1500], Loss: 1.7233\n",
      "Epoch [1323/1500], Loss: 1.7250\n",
      "Epoch [1324/1500], Loss: 1.7237\n",
      "Epoch [1325/1500], Loss: 1.7245\n",
      "Epoch [1326/1500], Loss: 1.7241\n",
      "Epoch [1327/1500], Loss: 1.7243\n",
      "Epoch [1328/1500], Loss: 1.7236\n",
      "Epoch [1329/1500], Loss: 1.7221\n",
      "Epoch [1330/1500], Loss: 1.7193\n",
      "Epoch [1331/1500], Loss: 1.7284\n",
      "Epoch [1332/1500], Loss: 1.7269\n",
      "Epoch [1333/1500], Loss: 1.7189\n",
      "Epoch [1334/1500], Loss: 1.7235\n",
      "Epoch [1335/1500], Loss: 1.7262\n",
      "Epoch [1336/1500], Loss: 1.7214\n",
      "Epoch [1337/1500], Loss: 1.7245\n",
      "Epoch [1338/1500], Loss: 1.7224\n",
      "Epoch [1339/1500], Loss: 1.7217\n",
      "Epoch [1340/1500], Loss: 1.7240\n",
      "Epoch [1341/1500], Loss: 1.7219\n",
      "Epoch [1342/1500], Loss: 1.7211\n",
      "Epoch [1343/1500], Loss: 1.7248\n",
      "Epoch [1344/1500], Loss: 1.7248\n",
      "Epoch [1345/1500], Loss: 1.7268\n",
      "Epoch [1346/1500], Loss: 1.7232\n",
      "Epoch [1347/1500], Loss: 1.7245\n",
      "Epoch [1348/1500], Loss: 1.7226\n",
      "Epoch [1349/1500], Loss: 1.7211\n",
      "Epoch [1350/1500], Loss: 1.7233\n",
      "Epoch [1351/1500], Loss: 1.7254\n",
      "Epoch [1352/1500], Loss: 1.7202\n",
      "Epoch [1353/1500], Loss: 1.7240\n",
      "Epoch [1354/1500], Loss: 1.7216\n",
      "Epoch [1355/1500], Loss: 1.7209\n",
      "Epoch [1356/1500], Loss: 1.7209\n",
      "Epoch [1357/1500], Loss: 1.7259\n",
      "Epoch [1358/1500], Loss: 1.7207\n",
      "Epoch [1359/1500], Loss: 1.7254\n",
      "Epoch [1360/1500], Loss: 1.7187\n",
      "Epoch [1361/1500], Loss: 1.7215\n",
      "Epoch [1362/1500], Loss: 1.7208\n",
      "Epoch [1363/1500], Loss: 1.7220\n",
      "Epoch [1364/1500], Loss: 1.7212\n",
      "Epoch [1365/1500], Loss: 1.7242\n",
      "Epoch [1366/1500], Loss: 1.7229\n",
      "Epoch [1367/1500], Loss: 1.7282\n",
      "Epoch [1368/1500], Loss: 1.7273\n",
      "Epoch [1369/1500], Loss: 1.7251\n",
      "Epoch [1370/1500], Loss: 1.7240\n",
      "Epoch [1371/1500], Loss: 1.7227\n",
      "Epoch [1372/1500], Loss: 1.7219\n",
      "Epoch [1373/1500], Loss: 1.7226\n",
      "Epoch [1374/1500], Loss: 1.7254\n",
      "Epoch [1375/1500], Loss: 1.7240\n",
      "Epoch [1376/1500], Loss: 1.7258\n",
      "Epoch [1377/1500], Loss: 1.7193\n",
      "Epoch [1378/1500], Loss: 1.7227\n",
      "Epoch [1379/1500], Loss: 1.7216\n",
      "Epoch [1380/1500], Loss: 1.7239\n",
      "Epoch [1381/1500], Loss: 1.7208\n",
      "Epoch [1382/1500], Loss: 1.7244\n",
      "Epoch [1383/1500], Loss: 1.7235\n",
      "Epoch [1384/1500], Loss: 1.7228\n",
      "Epoch [1385/1500], Loss: 1.7217\n",
      "Epoch [1386/1500], Loss: 1.7223\n",
      "Epoch [1387/1500], Loss: 1.7216\n",
      "Epoch [1388/1500], Loss: 1.7232\n",
      "Epoch [1389/1500], Loss: 1.7277\n",
      "Epoch [1390/1500], Loss: 1.7216\n",
      "Epoch [1391/1500], Loss: 1.7234\n",
      "Epoch [1392/1500], Loss: 1.7237\n",
      "Epoch [1393/1500], Loss: 1.7201\n",
      "Epoch [1394/1500], Loss: 1.7184\n",
      "Epoch [1395/1500], Loss: 1.7232\n",
      "Epoch [1396/1500], Loss: 1.7215\n",
      "Epoch [1397/1500], Loss: 1.7226\n",
      "Epoch [1398/1500], Loss: 1.7224\n",
      "Epoch [1399/1500], Loss: 1.7227\n",
      "Epoch [1400/1500], Loss: 1.7191\n",
      "Epoch [1401/1500], Loss: 1.7280\n",
      "Epoch [1402/1500], Loss: 1.7256\n",
      "Epoch [1403/1500], Loss: 1.7225\n",
      "Epoch [1404/1500], Loss: 1.7238\n",
      "Epoch [1405/1500], Loss: 1.7229\n",
      "Epoch [1406/1500], Loss: 1.7199\n",
      "Epoch [1407/1500], Loss: 1.7242\n",
      "Epoch [1408/1500], Loss: 1.7252\n",
      "Epoch [1409/1500], Loss: 1.7182\n",
      "Epoch [1410/1500], Loss: 1.7213\n",
      "Epoch [1411/1500], Loss: 1.7220\n",
      "Epoch [1412/1500], Loss: 1.7232\n",
      "Epoch [1413/1500], Loss: 1.7213\n",
      "Epoch [1414/1500], Loss: 1.7193\n",
      "Epoch [1415/1500], Loss: 1.7228\n",
      "Epoch [1416/1500], Loss: 1.7173\n",
      "Epoch [1417/1500], Loss: 1.7223\n",
      "Epoch [1418/1500], Loss: 1.7179\n",
      "Epoch [1419/1500], Loss: 1.7191\n",
      "Epoch [1420/1500], Loss: 1.7181\n",
      "Epoch [1421/1500], Loss: 1.7213\n",
      "Epoch [1422/1500], Loss: 1.7274\n",
      "Epoch [1423/1500], Loss: 1.7221\n",
      "Epoch [1424/1500], Loss: 1.7233\n",
      "Epoch [1425/1500], Loss: 1.7223\n",
      "Epoch [1426/1500], Loss: 1.7214\n",
      "Epoch [1427/1500], Loss: 1.7224\n",
      "Epoch [1428/1500], Loss: 1.7208\n",
      "Epoch [1429/1500], Loss: 1.7210\n",
      "Epoch [1430/1500], Loss: 1.7194\n",
      "Epoch [1431/1500], Loss: 1.7208\n",
      "Epoch [1432/1500], Loss: 1.7222\n",
      "Epoch [1433/1500], Loss: 1.7208\n",
      "Epoch [1434/1500], Loss: 1.7214\n",
      "Epoch [1435/1500], Loss: 1.7204\n",
      "Epoch [1436/1500], Loss: 1.7207\n",
      "Epoch [1437/1500], Loss: 1.7204\n",
      "Epoch [1438/1500], Loss: 1.7253\n",
      "Epoch [1439/1500], Loss: 1.7204\n",
      "Epoch [1440/1500], Loss: 1.7187\n",
      "Epoch [1441/1500], Loss: 1.7216\n",
      "Epoch [1442/1500], Loss: 1.7256\n",
      "Epoch [1443/1500], Loss: 1.7262\n",
      "Epoch [1444/1500], Loss: 1.7182\n",
      "Epoch [1445/1500], Loss: 1.7222\n",
      "Epoch [1446/1500], Loss: 1.7201\n",
      "Epoch [1447/1500], Loss: 1.7200\n",
      "Epoch [1448/1500], Loss: 1.7185\n",
      "Epoch [1449/1500], Loss: 1.7191\n",
      "Epoch [1450/1500], Loss: 1.7252\n",
      "Epoch [1451/1500], Loss: 1.7241\n",
      "Epoch [1452/1500], Loss: 1.7208\n",
      "Epoch [1453/1500], Loss: 1.7219\n",
      "Epoch [1454/1500], Loss: 1.7207\n",
      "Epoch [1455/1500], Loss: 1.7220\n",
      "Epoch [1456/1500], Loss: 1.7201\n",
      "Epoch [1457/1500], Loss: 1.7219\n",
      "Epoch [1458/1500], Loss: 1.7247\n",
      "Epoch [1459/1500], Loss: 1.7231\n",
      "Epoch [1460/1500], Loss: 1.7235\n",
      "Epoch [1461/1500], Loss: 1.7220\n",
      "Epoch [1462/1500], Loss: 1.7214\n",
      "Epoch [1463/1500], Loss: 1.7187\n",
      "Epoch [1464/1500], Loss: 1.7199\n",
      "Epoch [1465/1500], Loss: 1.7193\n",
      "Epoch [1466/1500], Loss: 1.7226\n",
      "Epoch [1467/1500], Loss: 1.7228\n",
      "Epoch [1468/1500], Loss: 1.7264\n",
      "Epoch [1469/1500], Loss: 1.7243\n",
      "Epoch [1470/1500], Loss: 1.7203\n",
      "Epoch [1471/1500], Loss: 1.7222\n",
      "Epoch [1472/1500], Loss: 1.7241\n",
      "Epoch [1473/1500], Loss: 1.7207\n",
      "Epoch [1474/1500], Loss: 1.7229\n",
      "Epoch [1475/1500], Loss: 1.7211\n",
      "Epoch [1476/1500], Loss: 1.7163\n",
      "Epoch [1477/1500], Loss: 1.7204\n",
      "Epoch [1478/1500], Loss: 1.7220\n",
      "Epoch [1479/1500], Loss: 1.7190\n",
      "Epoch [1480/1500], Loss: 1.7214\n",
      "Epoch [1481/1500], Loss: 1.7221\n",
      "Epoch [1482/1500], Loss: 1.7195\n",
      "Epoch [1483/1500], Loss: 1.7238\n",
      "Epoch [1484/1500], Loss: 1.7275\n",
      "Epoch [1485/1500], Loss: 1.7233\n",
      "Epoch [1486/1500], Loss: 1.7219\n",
      "Epoch [1487/1500], Loss: 1.7210\n",
      "Epoch [1488/1500], Loss: 1.7188\n",
      "Epoch [1489/1500], Loss: 1.7217\n",
      "Epoch [1490/1500], Loss: 1.7230\n",
      "Epoch [1491/1500], Loss: 1.7256\n",
      "Epoch [1492/1500], Loss: 1.7241\n",
      "Epoch [1493/1500], Loss: 1.7265\n",
      "Epoch [1494/1500], Loss: 1.7201\n",
      "Epoch [1495/1500], Loss: 1.7159\n",
      "Epoch [1496/1500], Loss: 1.7210\n",
      "Epoch [1497/1500], Loss: 1.7182\n",
      "Epoch [1498/1500], Loss: 1.7232\n",
      "Epoch [1499/1500], Loss: 1.7176\n",
      "Epoch [1500/1500], Loss: 1.7215\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ffda2e5d8b41e1a72f02f164eb8155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.387031…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Oversampling_Train_Loss</td><td>█▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Oversampling_Train_Loss</td><td>1.72147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">20230420_1</strong> at: <a href='https://wandb.ai/hwangyujeong/EmotionShortForm/runs/gcceu9bq' target=\"_blank\">https://wandb.ai/hwangyujeong/EmotionShortForm/runs/gcceu9bq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230421_021440-gcceu9bq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dropout 적용\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "wandb.init(project=\"EmotionShortForm\", \n",
    "           name=\"20230420_1\",\n",
    "          )\n",
    "\n",
    "Y_train_resampled_sm = Y_train_resampled_sm.long()\n",
    "min_f1=0\n",
    "for epoch in tqdm(range(EPOCHS), desc='Training', unit='epoch'):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass with dropout\n",
    "    X_train_resampled_sm_drop = nn.Dropout(p=0.5)(X_train_resampled_sm.unsqueeze(1))\n",
    "    output, hidden_state, _ = model(X_train_resampled_sm_drop)\n",
    "    #print(f\"output: {output.squeeze().shape}\")\n",
    "    #print(f\"Y_train_resampled_sm: {Y_train_resampled_sm.shape}\")\n",
    "    # Compute loss\n",
    "    loss = criterion(output.squeeze(), Y_train_resampled_sm)\n",
    "    wandb.log({\n",
    "    'Oversampling_Train_Loss': loss.item()\n",
    "    })\n",
    "\n",
    "    # Backward and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    tqdm.write('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, EPOCHS, loss.item()))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions, hidden, out = model(X_test_resampled_sm.unsqueeze(1))\n",
    "        y_pred = predictions.argmax(dim=1)\n",
    "\n",
    "        f1 = f1_score(Y_test_resampled_sm, y_pred, average='macro')\n",
    "        if f1 > min_f1:\n",
    "            min_f1 = f1\n",
    "            print(f'min_f1: {min_f1}') \n",
    "            torch.save(model.state_dict(), f'./model/lstm_emotion_classification_model_best.pt')\n",
    "   \n",
    "    \n",
    "   \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "model.load_state_dict(torch.load('./model/lstm_emotion_classification_model_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5798\n",
      "Test Loss: 1.7537\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_predict,target_hidden,_ = model(X_train_resampled_sm.unsqueeze(1))\n",
    "    test_predict,test_hidden,_ = model(X_test_resampled_sm.unsqueeze(1))\n",
    "    train_loss = criterion(train_predict, Y_train_resampled_sm)\n",
    "    test_loss = criterion(test_predict, Y_test_resampled_sm)\n",
    "    print('Train Loss: {:.4f}'.format(train_loss.item()))\n",
    "    print('Test Loss: {:.4f}'.format(test_loss.item()))\n",
    "    ## oversampling 문제 발생\n",
    "    \n",
    "    '''\n",
    "    1)\n",
    "    dropout했을 때 + smote\n",
    "    epoch: 500\n",
    "    Train Loss: 1.2529\n",
    "    Test Loss: 2.0355\n",
    "    2) \n",
    "    dropout했을 때 + smote\n",
    "    epoch: 700\n",
    "    Train Loss: 1.0857\n",
    "Test Loss: 2.1099\n",
    "\n",
    "    3) \n",
    "    dropout + neutral데이터 4000개 줄임\n",
    "    epoch: 700\n",
    "    Train Loss: 0.6769\n",
    "    Test Loss: 2.0612\n",
    "    \n",
    "    4) \n",
    "     dropout + neutral데이터 Train 5000, test 1000개 줄임\n",
    "    Train Loss: 0.7101\n",
    "    Test Loss: 1.9471\n",
    "    \n",
    "    Train Loss: 1.1158\n",
    "    Test Loss: 1.7357\n",
    "    \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "validation_set = TensorDataset(X_test_resampled_sm, Y_test_resampled_sm)\n",
    "validation_loader = DataLoader(validation_set, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 40.84%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n800 epoch\\nAccuracy on validation set: 38.56%'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()  # set model to evaluation mode\n",
    "\n",
    "with torch.no_grad(): # temporarily disable gradient calculation for efficiency\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data in validation_loader:\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = inputs.unsqueeze(1) # add an extra dimension for the sequence length\n",
    "        #print(inputs.shape)\n",
    "        outputs,outputs_hidden,_ = model(inputs)\n",
    "        #print(outputs.data)\n",
    "        _, predicted = torch.max(outputs.data, 1) # get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        #print(f'predicted: {predicted}, label_size: {labels.size(0)}, labels: {labels}')\n",
    "        \n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy on validation set: {:.2f}%'.format(accuracy))\n",
    "    \n",
    "'''\n",
    "800 epoch\n",
    "Accuracy on validation set: 38.56%'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_test_resampled_sm: \n",
      " tensor([1, 3, 1,  ..., 6, 6, 6])\n",
      "y_pred: \n",
      " tensor([2, 0, 1,  ..., 4, 6, 5])\n",
      "=============================\n",
      "Precision: 0.4242735529760329\n",
      "Recall: 0.4084101382488479\n",
      "F1 score: 0.39676196178417117\n",
      "Predictions:  tensor([2, 0, 1,  ..., 4, 6, 5])\n",
      "Extracting 128 sized features: torch.Size([8680, 128])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions, hidden, out = model(X_test_resampled_sm.unsqueeze(1))\n",
    "    \n",
    "    y_pred = predictions.argmax(dim=1)\n",
    "    \n",
    "#     lstm_feature = model.fc(model.hidden.squeeze())\n",
    "    \n",
    "    precision = precision_score(Y_test_resampled_sm, y_pred, average='macro')\n",
    "    recall = recall_score(Y_test_resampled_sm, y_pred, average='macro')\n",
    "    f1 = f1_score(Y_test_resampled_sm, y_pred, average='macro')\n",
    "    print('Y_test_resampled_sm: \\n' , Y_test_resampled_sm)\n",
    "    print('y_pred: \\n', y_pred)\n",
    "    print('=============================')\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 score:', f1)\n",
    "    print('Predictions: ', y_pred)\n",
    "    print(f'Extracting 128 sized features: {out.shape}') # 128 sized feature \n",
    "#     print(f'shape: {out.shape}')\n",
    "\n",
    "\n",
    "    '''\n",
    "    1)\n",
    "    dropout + smote \n",
    "    epoch:500\n",
    "    Precision: 0.3007015306122449\n",
    "    Recall: 0.3007015306122449\n",
    "    F1 score: 0.3007015306122449\n",
    "    2)\n",
    "    dropout했을 때 + smote\n",
    "    epoch: 700\n",
    "    Precision: 0.30031887755102044\n",
    "    Recall: 0.30031887755102044\n",
    "    F1 score: 0.30031887755102044\n",
    "    3) \n",
    " 중립 데이터 4000개 줄였을 때\n",
    "     Precision: 0.31868622448979594\n",
    "    Recall: 0.31868622448979594\n",
    "    F1 score: 0.31868622448979594\n",
    "    \n",
    "    4)   dropout + neutral데이터 Train 5000, test 1000개 줄임\n",
    "    Precision: 0.3968933771621839\n",
    "    Recall: 0.3768433179723502\n",
    "    F1 score: 0.3662198839219138\n",
    "    \n",
    "    5) 800 epoch\n",
    "    Precision: 0.42570893934757414\n",
    "Recall: 0.3855990783410138\n",
    "F1 score: 0.3747435795726259\n",
    "\n",
    "\n",
    "6) 1000 epoch \n",
    "Accuracy on validation set: 39.61%\n",
    "Precision: 0.4147809657621006\n",
    "Recall: 0.39608294930875576\n",
    "F1 score: 0.3872494008185158\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = './model/lstm_emotion_classification_model.pkl'\n",
    "# pickle.dump(model, open(filename,'wb'))\n",
    "\n",
    "# Save model\n",
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), './model/lstm_emotion_classification_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
