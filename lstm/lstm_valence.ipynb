{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/EmotionShortForm\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/EmotionShortForm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav 파일의 MFCC Feature와 상태정보를 합친 학습데이터를 불러옵니다.\n",
    "train_df = pd.read_csv('preprocessing_csv/train_mfcc_data.csv')\n",
    "test_df = pd.read_csv('preprocessing_csv/train_mfcc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SegmentId</th>\n",
       "      <th>time</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_23</th>\n",
       "      <th>mfcc_24</th>\n",
       "      <th>mfcc_25</th>\n",
       "      <th>mfcc_26</th>\n",
       "      <th>mfcc_27</th>\n",
       "      <th>mfcc_28</th>\n",
       "      <th>mfcc_29</th>\n",
       "      <th>mfcc_30</th>\n",
       "      <th>mfcc_31</th>\n",
       "      <th>mfcc_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11164</td>\n",
       "      <td>Sess26_script04_User052F_010</td>\n",
       "      <td>7.785990</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-379.964142</td>\n",
       "      <td>109.689522</td>\n",
       "      <td>8.409287</td>\n",
       "      <td>14.947894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390324</td>\n",
       "      <td>2.626545</td>\n",
       "      <td>0.437545</td>\n",
       "      <td>2.089825</td>\n",
       "      <td>0.171152</td>\n",
       "      <td>5.063381</td>\n",
       "      <td>3.834727</td>\n",
       "      <td>1.973997</td>\n",
       "      <td>-3.659752</td>\n",
       "      <td>-1.247434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13031</td>\n",
       "      <td>Sess24_script03_User047F_009</td>\n",
       "      <td>12.188990</td>\n",
       "      <td>happy</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-316.165100</td>\n",
       "      <td>96.198570</td>\n",
       "      <td>-13.874292</td>\n",
       "      <td>23.989883</td>\n",
       "      <td>...</td>\n",
       "      <td>2.732685</td>\n",
       "      <td>5.995924</td>\n",
       "      <td>5.040682</td>\n",
       "      <td>8.586355</td>\n",
       "      <td>6.141405</td>\n",
       "      <td>8.720468</td>\n",
       "      <td>8.573661</td>\n",
       "      <td>10.320451</td>\n",
       "      <td>7.674820</td>\n",
       "      <td>4.114799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1147</td>\n",
       "      <td>Sess09_script01_User018M_030</td>\n",
       "      <td>4.147000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-354.932220</td>\n",
       "      <td>124.622856</td>\n",
       "      <td>13.342740</td>\n",
       "      <td>26.263084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242917</td>\n",
       "      <td>0.397124</td>\n",
       "      <td>4.878927</td>\n",
       "      <td>5.264405</td>\n",
       "      <td>2.994150</td>\n",
       "      <td>2.870259</td>\n",
       "      <td>0.314687</td>\n",
       "      <td>2.809367</td>\n",
       "      <td>-0.628893</td>\n",
       "      <td>3.489642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6435</td>\n",
       "      <td>Sess14_script02_User028M_002</td>\n",
       "      <td>1.556999</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-385.279846</td>\n",
       "      <td>118.912277</td>\n",
       "      <td>13.351930</td>\n",
       "      <td>38.183296</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936629</td>\n",
       "      <td>-0.579203</td>\n",
       "      <td>-1.418086</td>\n",
       "      <td>2.701938</td>\n",
       "      <td>-0.128480</td>\n",
       "      <td>-0.650981</td>\n",
       "      <td>0.222955</td>\n",
       "      <td>1.449542</td>\n",
       "      <td>-1.601669</td>\n",
       "      <td>-0.025312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7053</td>\n",
       "      <td>Sess32_script01_User064F_049</td>\n",
       "      <td>5.443000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-351.163513</td>\n",
       "      <td>109.502029</td>\n",
       "      <td>4.125424</td>\n",
       "      <td>35.244442</td>\n",
       "      <td>...</td>\n",
       "      <td>3.420357</td>\n",
       "      <td>6.290679</td>\n",
       "      <td>3.681709</td>\n",
       "      <td>5.164492</td>\n",
       "      <td>3.685917</td>\n",
       "      <td>7.506263</td>\n",
       "      <td>7.469996</td>\n",
       "      <td>7.370858</td>\n",
       "      <td>6.551222</td>\n",
       "      <td>5.917849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10764</th>\n",
       "      <td>5191</td>\n",
       "      <td>Sess03_script03_User005M_032</td>\n",
       "      <td>3.847000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-307.079376</td>\n",
       "      <td>110.748199</td>\n",
       "      <td>3.071705</td>\n",
       "      <td>26.098019</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.210361</td>\n",
       "      <td>-1.421704</td>\n",
       "      <td>-2.658003</td>\n",
       "      <td>1.054526</td>\n",
       "      <td>-3.188072</td>\n",
       "      <td>0.515963</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.752482</td>\n",
       "      <td>-1.321549</td>\n",
       "      <td>-3.410846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10765</th>\n",
       "      <td>13418</td>\n",
       "      <td>Sess22_script06_User043F_006</td>\n",
       "      <td>5.639000</td>\n",
       "      <td>angry</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>-239.924500</td>\n",
       "      <td>72.984070</td>\n",
       "      <td>-18.580250</td>\n",
       "      <td>-6.845965</td>\n",
       "      <td>...</td>\n",
       "      <td>6.675857</td>\n",
       "      <td>10.649000</td>\n",
       "      <td>7.589522</td>\n",
       "      <td>8.607011</td>\n",
       "      <td>2.216211</td>\n",
       "      <td>7.668085</td>\n",
       "      <td>-0.767406</td>\n",
       "      <td>2.716651</td>\n",
       "      <td>-3.422506</td>\n",
       "      <td>4.849663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>5390</td>\n",
       "      <td>Sess12_script01_User081F_018</td>\n",
       "      <td>3.157000</td>\n",
       "      <td>happy;neutral</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-333.780945</td>\n",
       "      <td>95.989861</td>\n",
       "      <td>-14.832805</td>\n",
       "      <td>15.677377</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.285286</td>\n",
       "      <td>1.564119</td>\n",
       "      <td>6.515084</td>\n",
       "      <td>10.374402</td>\n",
       "      <td>8.091890</td>\n",
       "      <td>14.184700</td>\n",
       "      <td>12.203178</td>\n",
       "      <td>19.104069</td>\n",
       "      <td>6.390728</td>\n",
       "      <td>4.591437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10767</th>\n",
       "      <td>860</td>\n",
       "      <td>Sess19_script02_User037M_002</td>\n",
       "      <td>6.590000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-339.677338</td>\n",
       "      <td>118.469772</td>\n",
       "      <td>10.663605</td>\n",
       "      <td>37.051121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.823455</td>\n",
       "      <td>1.826434</td>\n",
       "      <td>-0.563071</td>\n",
       "      <td>3.543955</td>\n",
       "      <td>-0.973314</td>\n",
       "      <td>3.040696</td>\n",
       "      <td>3.090068</td>\n",
       "      <td>4.243995</td>\n",
       "      <td>0.834076</td>\n",
       "      <td>1.139718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10768</th>\n",
       "      <td>7270</td>\n",
       "      <td>Sess32_script04_User064F_015</td>\n",
       "      <td>4.884996</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-347.986877</td>\n",
       "      <td>119.509628</td>\n",
       "      <td>-8.292249</td>\n",
       "      <td>15.593624</td>\n",
       "      <td>...</td>\n",
       "      <td>5.915211</td>\n",
       "      <td>5.233806</td>\n",
       "      <td>6.230196</td>\n",
       "      <td>8.562304</td>\n",
       "      <td>2.252800</td>\n",
       "      <td>5.576434</td>\n",
       "      <td>2.432891</td>\n",
       "      <td>6.054037</td>\n",
       "      <td>9.382090</td>\n",
       "      <td>3.946812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10769 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                     SegmentId       time        Emotion  \\\n",
       "0           11164  Sess26_script04_User052F_010   7.785990        neutral   \n",
       "1           13031  Sess24_script03_User047F_009  12.188990          happy   \n",
       "2            1147  Sess09_script01_User018M_030   4.147000        neutral   \n",
       "3            6435  Sess14_script02_User028M_002   1.556999        neutral   \n",
       "4            7053  Sess32_script01_User064F_049   5.443000        neutral   \n",
       "...           ...                           ...        ...            ...   \n",
       "10764        5191  Sess03_script03_User005M_032   3.847000        neutral   \n",
       "10765       13418  Sess22_script06_User043F_006   5.639000          angry   \n",
       "10766        5390  Sess12_script01_User081F_018   3.157000  happy;neutral   \n",
       "10767         860  Sess19_script02_User037M_002   6.590000        neutral   \n",
       "10768        7270  Sess32_script04_User064F_015   4.884996        neutral   \n",
       "\n",
       "       Valence  Arousal      mfcc_1      mfcc_2     mfcc_3     mfcc_4  ...  \\\n",
       "0          3.0      3.5 -379.964142  109.689522   8.409287  14.947894  ...   \n",
       "1          3.4      3.6 -316.165100   96.198570 -13.874292  23.989883  ...   \n",
       "2          2.6      3.1 -354.932220  124.622856  13.342740  26.263084  ...   \n",
       "3          3.6      3.1 -385.279846  118.912277  13.351930  38.183296  ...   \n",
       "4          3.0      3.5 -351.163513  109.502029   4.125424  35.244442  ...   \n",
       "...        ...      ...         ...         ...        ...        ...  ...   \n",
       "10764      3.2      3.3 -307.079376  110.748199   3.071705  26.098019  ...   \n",
       "10765      2.6      3.7 -239.924500   72.984070 -18.580250  -6.845965  ...   \n",
       "10766      3.7      3.6 -333.780945   95.989861 -14.832805  15.677377  ...   \n",
       "10767      2.6      3.3 -339.677338  118.469772  10.663605  37.051121  ...   \n",
       "10768      3.1      3.5 -347.986877  119.509628  -8.292249  15.593624  ...   \n",
       "\n",
       "        mfcc_23    mfcc_24   mfcc_25    mfcc_26   mfcc_27    mfcc_28  \\\n",
       "0     -0.390324   2.626545  0.437545   2.089825  0.171152   5.063381   \n",
       "1      2.732685   5.995924  5.040682   8.586355  6.141405   8.720468   \n",
       "2     -0.242917   0.397124  4.878927   5.264405  2.994150   2.870259   \n",
       "3      2.936629  -0.579203 -1.418086   2.701938 -0.128480  -0.650981   \n",
       "4      3.420357   6.290679  3.681709   5.164492  3.685917   7.506263   \n",
       "...         ...        ...       ...        ...       ...        ...   \n",
       "10764 -4.210361  -1.421704 -2.658003   1.054526 -3.188072   0.515963   \n",
       "10765  6.675857  10.649000  7.589522   8.607011  2.216211   7.668085   \n",
       "10766 -3.285286   1.564119  6.515084  10.374402  8.091890  14.184700   \n",
       "10767 -0.823455   1.826434 -0.563071   3.543955 -0.973314   3.040696   \n",
       "10768  5.915211   5.233806  6.230196   8.562304  2.252800   5.576434   \n",
       "\n",
       "         mfcc_29    mfcc_30   mfcc_31   mfcc_32  \n",
       "0       3.834727   1.973997 -3.659752 -1.247434  \n",
       "1       8.573661  10.320451  7.674820  4.114799  \n",
       "2       0.314687   2.809367 -0.628893  3.489642  \n",
       "3       0.222955   1.449542 -1.601669 -0.025312  \n",
       "4       7.469996   7.370858  6.551222  5.917849  \n",
       "...          ...        ...       ...       ...  \n",
       "10764   0.066960   0.752482 -1.321549 -3.410846  \n",
       "10765  -0.767406   2.716651 -3.422506  4.849663  \n",
       "10766  12.203178  19.104069  6.390728  4.591437  \n",
       "10767   3.090068   4.243995  0.834076  1.139718  \n",
       "10768   2.432891   6.054037  9.382090  3.946812  \n",
       "\n",
       "[10769 rows x 38 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SegmentId</th>\n",
       "      <th>time</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_23</th>\n",
       "      <th>mfcc_24</th>\n",
       "      <th>mfcc_25</th>\n",
       "      <th>mfcc_26</th>\n",
       "      <th>mfcc_27</th>\n",
       "      <th>mfcc_28</th>\n",
       "      <th>mfcc_29</th>\n",
       "      <th>mfcc_30</th>\n",
       "      <th>mfcc_31</th>\n",
       "      <th>mfcc_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11164</td>\n",
       "      <td>Sess26_script04_User052F_010</td>\n",
       "      <td>7.785990</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-379.964142</td>\n",
       "      <td>109.689522</td>\n",
       "      <td>8.409287</td>\n",
       "      <td>14.947894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390324</td>\n",
       "      <td>2.626545</td>\n",
       "      <td>0.437545</td>\n",
       "      <td>2.089825</td>\n",
       "      <td>0.171152</td>\n",
       "      <td>5.063381</td>\n",
       "      <td>3.834727</td>\n",
       "      <td>1.973997</td>\n",
       "      <td>-3.659752</td>\n",
       "      <td>-1.247434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13031</td>\n",
       "      <td>Sess24_script03_User047F_009</td>\n",
       "      <td>12.188990</td>\n",
       "      <td>happy</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-316.165100</td>\n",
       "      <td>96.198570</td>\n",
       "      <td>-13.874292</td>\n",
       "      <td>23.989883</td>\n",
       "      <td>...</td>\n",
       "      <td>2.732685</td>\n",
       "      <td>5.995924</td>\n",
       "      <td>5.040682</td>\n",
       "      <td>8.586355</td>\n",
       "      <td>6.141405</td>\n",
       "      <td>8.720468</td>\n",
       "      <td>8.573661</td>\n",
       "      <td>10.320451</td>\n",
       "      <td>7.674820</td>\n",
       "      <td>4.114799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1147</td>\n",
       "      <td>Sess09_script01_User018M_030</td>\n",
       "      <td>4.147000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-354.932220</td>\n",
       "      <td>124.622856</td>\n",
       "      <td>13.342740</td>\n",
       "      <td>26.263084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242917</td>\n",
       "      <td>0.397124</td>\n",
       "      <td>4.878927</td>\n",
       "      <td>5.264405</td>\n",
       "      <td>2.994150</td>\n",
       "      <td>2.870259</td>\n",
       "      <td>0.314687</td>\n",
       "      <td>2.809367</td>\n",
       "      <td>-0.628893</td>\n",
       "      <td>3.489642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6435</td>\n",
       "      <td>Sess14_script02_User028M_002</td>\n",
       "      <td>1.556999</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-385.279846</td>\n",
       "      <td>118.912277</td>\n",
       "      <td>13.351930</td>\n",
       "      <td>38.183296</td>\n",
       "      <td>...</td>\n",
       "      <td>2.936629</td>\n",
       "      <td>-0.579203</td>\n",
       "      <td>-1.418086</td>\n",
       "      <td>2.701938</td>\n",
       "      <td>-0.128480</td>\n",
       "      <td>-0.650981</td>\n",
       "      <td>0.222955</td>\n",
       "      <td>1.449542</td>\n",
       "      <td>-1.601669</td>\n",
       "      <td>-0.025312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7053</td>\n",
       "      <td>Sess32_script01_User064F_049</td>\n",
       "      <td>5.443000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-351.163513</td>\n",
       "      <td>109.502029</td>\n",
       "      <td>4.125424</td>\n",
       "      <td>35.244442</td>\n",
       "      <td>...</td>\n",
       "      <td>3.420357</td>\n",
       "      <td>6.290679</td>\n",
       "      <td>3.681709</td>\n",
       "      <td>5.164492</td>\n",
       "      <td>3.685917</td>\n",
       "      <td>7.506263</td>\n",
       "      <td>7.469996</td>\n",
       "      <td>7.370858</td>\n",
       "      <td>6.551222</td>\n",
       "      <td>5.917849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10764</th>\n",
       "      <td>5191</td>\n",
       "      <td>Sess03_script03_User005M_032</td>\n",
       "      <td>3.847000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-307.079376</td>\n",
       "      <td>110.748199</td>\n",
       "      <td>3.071705</td>\n",
       "      <td>26.098019</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.210361</td>\n",
       "      <td>-1.421704</td>\n",
       "      <td>-2.658003</td>\n",
       "      <td>1.054526</td>\n",
       "      <td>-3.188072</td>\n",
       "      <td>0.515963</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.752482</td>\n",
       "      <td>-1.321549</td>\n",
       "      <td>-3.410846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10765</th>\n",
       "      <td>13418</td>\n",
       "      <td>Sess22_script06_User043F_006</td>\n",
       "      <td>5.639000</td>\n",
       "      <td>angry</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>-239.924500</td>\n",
       "      <td>72.984070</td>\n",
       "      <td>-18.580250</td>\n",
       "      <td>-6.845965</td>\n",
       "      <td>...</td>\n",
       "      <td>6.675857</td>\n",
       "      <td>10.649000</td>\n",
       "      <td>7.589522</td>\n",
       "      <td>8.607011</td>\n",
       "      <td>2.216211</td>\n",
       "      <td>7.668085</td>\n",
       "      <td>-0.767406</td>\n",
       "      <td>2.716651</td>\n",
       "      <td>-3.422506</td>\n",
       "      <td>4.849663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10766</th>\n",
       "      <td>5390</td>\n",
       "      <td>Sess12_script01_User081F_018</td>\n",
       "      <td>3.157000</td>\n",
       "      <td>happy;neutral</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-333.780945</td>\n",
       "      <td>95.989861</td>\n",
       "      <td>-14.832805</td>\n",
       "      <td>15.677377</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.285286</td>\n",
       "      <td>1.564119</td>\n",
       "      <td>6.515084</td>\n",
       "      <td>10.374402</td>\n",
       "      <td>8.091890</td>\n",
       "      <td>14.184700</td>\n",
       "      <td>12.203178</td>\n",
       "      <td>19.104069</td>\n",
       "      <td>6.390728</td>\n",
       "      <td>4.591437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10767</th>\n",
       "      <td>860</td>\n",
       "      <td>Sess19_script02_User037M_002</td>\n",
       "      <td>6.590000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-339.677338</td>\n",
       "      <td>118.469772</td>\n",
       "      <td>10.663605</td>\n",
       "      <td>37.051121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.823455</td>\n",
       "      <td>1.826434</td>\n",
       "      <td>-0.563071</td>\n",
       "      <td>3.543955</td>\n",
       "      <td>-0.973314</td>\n",
       "      <td>3.040696</td>\n",
       "      <td>3.090068</td>\n",
       "      <td>4.243995</td>\n",
       "      <td>0.834076</td>\n",
       "      <td>1.139718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10768</th>\n",
       "      <td>7270</td>\n",
       "      <td>Sess32_script04_User064F_015</td>\n",
       "      <td>4.884996</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-347.986877</td>\n",
       "      <td>119.509628</td>\n",
       "      <td>-8.292249</td>\n",
       "      <td>15.593624</td>\n",
       "      <td>...</td>\n",
       "      <td>5.915211</td>\n",
       "      <td>5.233806</td>\n",
       "      <td>6.230196</td>\n",
       "      <td>8.562304</td>\n",
       "      <td>2.252800</td>\n",
       "      <td>5.576434</td>\n",
       "      <td>2.432891</td>\n",
       "      <td>6.054037</td>\n",
       "      <td>9.382090</td>\n",
       "      <td>3.946812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10769 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                     SegmentId       time        Emotion  \\\n",
       "0           11164  Sess26_script04_User052F_010   7.785990        neutral   \n",
       "1           13031  Sess24_script03_User047F_009  12.188990          happy   \n",
       "2            1147  Sess09_script01_User018M_030   4.147000        neutral   \n",
       "3            6435  Sess14_script02_User028M_002   1.556999        neutral   \n",
       "4            7053  Sess32_script01_User064F_049   5.443000        neutral   \n",
       "...           ...                           ...        ...            ...   \n",
       "10764        5191  Sess03_script03_User005M_032   3.847000        neutral   \n",
       "10765       13418  Sess22_script06_User043F_006   5.639000          angry   \n",
       "10766        5390  Sess12_script01_User081F_018   3.157000  happy;neutral   \n",
       "10767         860  Sess19_script02_User037M_002   6.590000        neutral   \n",
       "10768        7270  Sess32_script04_User064F_015   4.884996        neutral   \n",
       "\n",
       "       Valence  Arousal      mfcc_1      mfcc_2     mfcc_3     mfcc_4  ...  \\\n",
       "0          3.0      3.5 -379.964142  109.689522   8.409287  14.947894  ...   \n",
       "1          3.4      3.6 -316.165100   96.198570 -13.874292  23.989883  ...   \n",
       "2          2.6      3.1 -354.932220  124.622856  13.342740  26.263084  ...   \n",
       "3          3.6      3.1 -385.279846  118.912277  13.351930  38.183296  ...   \n",
       "4          3.0      3.5 -351.163513  109.502029   4.125424  35.244442  ...   \n",
       "...        ...      ...         ...         ...        ...        ...  ...   \n",
       "10764      3.2      3.3 -307.079376  110.748199   3.071705  26.098019  ...   \n",
       "10765      2.6      3.7 -239.924500   72.984070 -18.580250  -6.845965  ...   \n",
       "10766      3.7      3.6 -333.780945   95.989861 -14.832805  15.677377  ...   \n",
       "10767      2.6      3.3 -339.677338  118.469772  10.663605  37.051121  ...   \n",
       "10768      3.1      3.5 -347.986877  119.509628  -8.292249  15.593624  ...   \n",
       "\n",
       "        mfcc_23    mfcc_24   mfcc_25    mfcc_26   mfcc_27    mfcc_28  \\\n",
       "0     -0.390324   2.626545  0.437545   2.089825  0.171152   5.063381   \n",
       "1      2.732685   5.995924  5.040682   8.586355  6.141405   8.720468   \n",
       "2     -0.242917   0.397124  4.878927   5.264405  2.994150   2.870259   \n",
       "3      2.936629  -0.579203 -1.418086   2.701938 -0.128480  -0.650981   \n",
       "4      3.420357   6.290679  3.681709   5.164492  3.685917   7.506263   \n",
       "...         ...        ...       ...        ...       ...        ...   \n",
       "10764 -4.210361  -1.421704 -2.658003   1.054526 -3.188072   0.515963   \n",
       "10765  6.675857  10.649000  7.589522   8.607011  2.216211   7.668085   \n",
       "10766 -3.285286   1.564119  6.515084  10.374402  8.091890  14.184700   \n",
       "10767 -0.823455   1.826434 -0.563071   3.543955 -0.973314   3.040696   \n",
       "10768  5.915211   5.233806  6.230196   8.562304  2.252800   5.576434   \n",
       "\n",
       "         mfcc_29    mfcc_30   mfcc_31   mfcc_32  \n",
       "0       3.834727   1.973997 -3.659752 -1.247434  \n",
       "1       8.573661  10.320451  7.674820  4.114799  \n",
       "2       0.314687   2.809367 -0.628893  3.489642  \n",
       "3       0.222955   1.449542 -1.601669 -0.025312  \n",
       "4       7.469996   7.370858  6.551222  5.917849  \n",
       "...          ...        ...       ...       ...  \n",
       "10764   0.066960   0.752482 -1.321549 -3.410846  \n",
       "10765  -0.767406   2.716651 -3.422506  4.849663  \n",
       "10766  12.203178  19.104069  6.390728  4.591437  \n",
       "10767   3.090068   4.243995  0.834076  1.139718  \n",
       "10768   2.432891   6.054037  9.382090  3.946812  \n",
       "\n",
       "[10769 rows x 38 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터를 모델의 input으로 들어갈 x와 label로 사용할 y로 분할\n",
    "train_x_df = train_df.drop(columns=['Unnamed: 0', 'SegmentId','time','Valence','Arousal','Emotion'])\n",
    "train_y_df = train_df['Valence']\n",
    "test_x_df = test_df.drop(columns=['Unnamed: 0', 'SegmentId','time','Valence','Arousal','Emotion'])\n",
    "test_y_df = test_df['Valence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 X\n",
    "\n",
    "train_x = torch.from_numpy(train_x_df.to_numpy()).float()\n",
    "train_y = torch.from_numpy(train_y_df.to_numpy()).float()\n",
    "\n",
    "test_x = torch.from_numpy(test_x_df.to_numpy()).float()\n",
    "test_y = torch.from_numpy(test_y_df.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7807, 0.6002, 0.5256,  ..., 0.3406, 0.2676, 0.2650],\n",
       "        [0.8470, 0.5264, 0.3587,  ..., 0.5966, 0.6359, 0.4704],\n",
       "        [0.8067, 0.6820, 0.5625,  ..., 0.3662, 0.3661, 0.4465],\n",
       "        ...,\n",
       "        [0.8287, 0.5253, 0.3515,  ..., 0.8661, 0.5941, 0.4887],\n",
       "        [0.8225, 0.6483, 0.5424,  ..., 0.4102, 0.4136, 0.3564],\n",
       "        [0.8139, 0.6540, 0.4005,  ..., 0.4658, 0.6913, 0.4640]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화 O\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(np.array(train_x_df))\n",
    "# # ### 다 0으로 초기화됨\n",
    "normalized_train_data = scaler.transform(np.array(train_x_df))\n",
    "normalized_test_data = torch.tensor( scaler.transform(np.array(test_x_df))).float()\n",
    "\n",
    "\n",
    "norm_train_x = torch.from_numpy(normalized_train_data).float()\n",
    "train_y = torch.from_numpy(train_y_df.to_numpy()).float()\n",
    "\n",
    "norm_test_x = torch.from_numpy(normalized_train_data).float()\n",
    "test_y = torch.from_numpy(test_y_df.to_numpy()).float()\n",
    "norm_train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "        out, (hidden_state, cell_state) = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(out[:,-1,:])\n",
    "        hidden = self.fc(hidden_state[-1])\n",
    "        return output, hidden,  out[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to calculate CCC\n",
    "def compute_ccc(y_true, y_pred):\n",
    "    mu_true = np.mean(y_true)\n",
    "    mu_pred = np.mean(y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    std_true = np.std(y_true)\n",
    "    std_pred = np.std(y_pred)\n",
    "\n",
    "    rho = np.corrcoef(y_true, y_pred)[0,1]\n",
    "\n",
    "    num = 2 * rho * std_true * std_pred\n",
    "    denom = var_true + var_pred + (mu_true - mu_pred)**2\n",
    "\n",
    "    ccc = num / denom\n",
    "    return ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import KFold\\n# from sklearn.metrics import average_precision_score\\n\\nhidden_sizes = [32, 64, 128]\\nnum_layerss = [2, 3, 4]\\nnum_epochss = [100, 150, 200, 250, 300]\\n\\nnum_folds = 3\\nkf = KFold(n_splits=num_folds)\\n\\nresults = []\\nfor hidden_size in hidden_sizes:\\n    for num_layers in num_layerss:\\n        for num_epochs in num_epochss:\\n            # initialize model and optimizer\\n            model = LSTM(input_size, hidden_size, num_layers, output_size)\\n            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\\n            # train and evaluate using cross-validation\\n            fold_results = []\\n            for train_idx, val_idx in kf.split(train_x):\\n                train_x_fold, train_y_fold = train_x[train_idx], train_y[train_idx]\\n                \\n                for epoch in range(num_epochs):\\n                    model.train()\\n                    outputs = model(train_x_fold.unsqueeze(1))\\n                    loss = criterion(outputs, train_y_fold)\\n                    optimizer.zero_grad()\\n                    loss.backward()\\n                    optimizer.step()\\n                model.eval()\\n                with torch.no_grad():\\n                    val_outputs = model(val_x_fold.unsqueeze(1))\\n                    val_loss = criterion(val_outputs, val_y_fold)\\n                    val_predictions = val_outputs.detach().numpy().flatten()\\n                    val_targets = val_y_fold.detach().numpy().flatten()\\n                    ccc = compute_ccc(val_targets, val_predictions)\\n                    fold_results.append((val_loss.item(), ccc))\\n            # compute average performance over folds\\n            avg_val_loss = np.mean([x[0] for x in fold_results])\\n            avg_ccc = np.mean([x[1] for x in fold_results])\\n            results.append((hidden_size, num_layers, num_epochs, avg_val_loss, avg_ccc))\\n            # print results for current hyperparameter combination\\n            print(f\"hidden_size={hidden_size}, num_layers={num_layers}, num_epochs={num_epochs}: \"\\n                  f\"val_loss={avg_val_loss:.4f}, ccc={avg_ccc:.4f}\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "hidden_sizes = [32, 64, 128]\n",
    "num_layerss = [2, 3, 4]\n",
    "num_epochss = [100, 150, 200, 250, 300]\n",
    "\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds)\n",
    "\n",
    "results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layers in num_layerss:\n",
    "        for num_epochs in num_epochss:\n",
    "            # initialize model and optimizer\n",
    "            model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            # train and evaluate using cross-validation\n",
    "            fold_results = []\n",
    "            for train_idx, val_idx in kf.split(train_x):\n",
    "                train_x_fold, train_y_fold = train_x[train_idx], train_y[train_idx]\n",
    "                \n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    outputs = model(train_x_fold.unsqueeze(1))\n",
    "                    loss = criterion(outputs, train_y_fold)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(val_x_fold.unsqueeze(1))\n",
    "                    val_loss = criterion(val_outputs, val_y_fold)\n",
    "                    val_predictions = val_outputs.detach().numpy().flatten()\n",
    "                    val_targets = val_y_fold.detach().numpy().flatten()\n",
    "                    ccc = compute_ccc(val_targets, val_predictions)\n",
    "                    fold_results.append((val_loss.item(), ccc))\n",
    "            # compute average performance over folds\n",
    "            avg_val_loss = np.mean([x[0] for x in fold_results])\n",
    "            avg_ccc = np.mean([x[1] for x in fold_results])\n",
    "            results.append((hidden_size, num_layers, num_epochs, avg_val_loss, avg_ccc))\n",
    "            # print results for current hyperparameter combination\n",
    "            print(f\"hidden_size={hidden_size}, num_layers={num_layers}, num_epochs={num_epochs}: \"\n",
    "                  f\"val_loss={avg_val_loss:.4f}, ccc={avg_ccc:.4f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbest_results = sorted(results, key=lambda x: x[4], reverse=True)\\nbest_hidden_size, best_num_layers, best_num_epochs, _, best_ccc= best_results[0]\\nprint(f\"Best hyperparameters: hidden_size={best_hidden_size}, num_layers={best_num_layers}, \"\\n      f\"num_epochs={best_num_epochs}, ccc={best_ccc:.4f}\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find hyperparameters with best performance\n",
    "'''\n",
    "best_results = sorted(results, key=lambda x: x[4], reverse=True)\n",
    "best_hidden_size, best_num_layers, best_num_epochs, _, best_ccc= best_results[0]\n",
    "print(f\"Best hyperparameters: hidden_size={best_hidden_size}, num_layers={best_num_layers}, \"\n",
    "      f\"num_epochs={best_num_epochs}, ccc={best_ccc:.4f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32\n",
    "hidden_size = 32 # 32\n",
    "num_layers = 5 # 2\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500 # 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767f8dc4977143b9ba99803d8c0196d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 8.8400\n",
      "min_ccc: 9.637538618753532e-08\n",
      "Epoch [2/500], Loss: 8.8161\n",
      "Epoch [3/500], Loss: 8.7923\n",
      "Epoch [4/500], Loss: 8.7686\n",
      "Epoch [5/500], Loss: 8.7450\n",
      "Epoch [6/500], Loss: 8.7215\n",
      "Epoch [7/500], Loss: 8.6980\n",
      "Epoch [8/500], Loss: 8.6745\n",
      "Epoch [9/500], Loss: 8.6510\n",
      "Epoch [10/500], Loss: 8.6274\n",
      "Epoch [11/500], Loss: 8.6038\n",
      "Epoch [12/500], Loss: 8.5802\n",
      "Epoch [13/500], Loss: 8.5564\n",
      "min_ccc: 9.994097951849036e-08\n",
      "Epoch [14/500], Loss: 8.5325\n",
      "min_ccc: 1.1033397714873902e-07\n",
      "Epoch [15/500], Loss: 8.5084\n",
      "min_ccc: 1.2332601799683918e-07\n",
      "Epoch [16/500], Loss: 8.4841\n",
      "min_ccc: 1.388411007883169e-07\n",
      "Epoch [17/500], Loss: 8.4595\n",
      "min_ccc: 1.564361354676366e-07\n",
      "Epoch [18/500], Loss: 8.4347\n",
      "min_ccc: 1.7614928212996155e-07\n",
      "Epoch [19/500], Loss: 8.4095\n",
      "min_ccc: 1.9793395913041376e-07\n",
      "Epoch [20/500], Loss: 8.3839\n",
      "min_ccc: 2.2157602067695005e-07\n",
      "Epoch [21/500], Loss: 8.3578\n",
      "min_ccc: 2.4660281203315094e-07\n",
      "Epoch [22/500], Loss: 8.3312\n",
      "min_ccc: 2.723031622787951e-07\n",
      "Epoch [23/500], Loss: 8.3039\n",
      "min_ccc: 2.975823943725611e-07\n",
      "Epoch [24/500], Loss: 8.2760\n",
      "min_ccc: 3.2083445548319297e-07\n",
      "Epoch [25/500], Loss: 8.2473\n",
      "min_ccc: 3.3934177740709325e-07\n",
      "Epoch [26/500], Loss: 8.2177\n",
      "min_ccc: 3.4957024472564774e-07\n",
      "Epoch [27/500], Loss: 8.1871\n",
      "Epoch [28/500], Loss: 8.1553\n",
      "Epoch [29/500], Loss: 8.1223\n",
      "Epoch [30/500], Loss: 8.0878\n",
      "Epoch [31/500], Loss: 8.0517\n",
      "Epoch [32/500], Loss: 8.0137\n",
      "Epoch [33/500], Loss: 7.9737\n",
      "Epoch [34/500], Loss: 7.9313\n",
      "Epoch [35/500], Loss: 7.8863\n",
      "Epoch [36/500], Loss: 7.8383\n",
      "Epoch [37/500], Loss: 7.7870\n",
      "Epoch [38/500], Loss: 7.7318\n",
      "Epoch [39/500], Loss: 7.6723\n",
      "Epoch [40/500], Loss: 7.6080\n",
      "Epoch [41/500], Loss: 7.5383\n",
      "Epoch [42/500], Loss: 7.4626\n",
      "Epoch [43/500], Loss: 7.3802\n",
      "Epoch [44/500], Loss: 7.2902\n",
      "Epoch [45/500], Loss: 7.1921\n",
      "Epoch [46/500], Loss: 7.0849\n",
      "Epoch [47/500], Loss: 6.9678\n",
      "Epoch [48/500], Loss: 6.8403\n",
      "Epoch [49/500], Loss: 6.7014\n",
      "Epoch [50/500], Loss: 6.5507\n",
      "Epoch [51/500], Loss: 6.3877\n",
      "Epoch [52/500], Loss: 6.2123\n",
      "Epoch [53/500], Loss: 6.0244\n",
      "Epoch [54/500], Loss: 5.8243\n",
      "Epoch [55/500], Loss: 5.6128\n",
      "Epoch [56/500], Loss: 5.3907\n",
      "Epoch [57/500], Loss: 5.1594\n",
      "Epoch [58/500], Loss: 4.9205\n",
      "Epoch [59/500], Loss: 4.6757\n",
      "Epoch [60/500], Loss: 4.4270\n",
      "Epoch [61/500], Loss: 4.1763\n",
      "Epoch [62/500], Loss: 3.9256\n",
      "Epoch [63/500], Loss: 3.6769\n",
      "Epoch [64/500], Loss: 3.4319\n",
      "Epoch [65/500], Loss: 3.1922\n",
      "Epoch [66/500], Loss: 2.9592\n",
      "Epoch [67/500], Loss: 2.7341\n",
      "Epoch [68/500], Loss: 2.5178\n",
      "Epoch [69/500], Loss: 2.3111\n",
      "Epoch [70/500], Loss: 2.1146\n",
      "min_ccc: 6.206589279532468e-07\n",
      "Epoch [71/500], Loss: 1.9287\n",
      "min_ccc: 4.608617613566438e-06\n",
      "Epoch [72/500], Loss: 1.7537\n",
      "Epoch [73/500], Loss: 1.5898\n",
      "Epoch [74/500], Loss: 1.4370\n",
      "Epoch [75/500], Loss: 1.2952\n",
      "Epoch [76/500], Loss: 1.1643\n",
      "Epoch [77/500], Loss: 1.0441\n",
      "Epoch [78/500], Loss: 0.9344\n",
      "Epoch [79/500], Loss: 0.8347\n",
      "Epoch [80/500], Loss: 0.7448\n",
      "Epoch [81/500], Loss: 0.6642\n",
      "Epoch [82/500], Loss: 0.5924\n",
      "Epoch [83/500], Loss: 0.5291\n",
      "Epoch [84/500], Loss: 0.4736\n",
      "Epoch [85/500], Loss: 0.4255\n",
      "Epoch [86/500], Loss: 0.3843\n",
      "Epoch [87/500], Loss: 0.3494\n",
      "Epoch [88/500], Loss: 0.3202\n",
      "Epoch [89/500], Loss: 0.2961\n",
      "Epoch [90/500], Loss: 0.2768\n",
      "Epoch [91/500], Loss: 0.2615\n",
      "min_ccc: 2.3088208008444467e-05\n",
      "Epoch [92/500], Loss: 0.2499\n",
      "min_ccc: 5.829496207212175e-05\n",
      "Epoch [93/500], Loss: 0.2413\n",
      "min_ccc: 9.58576364054492e-05\n",
      "Epoch [94/500], Loss: 0.2354\n",
      "min_ccc: 0.0001348681381568401\n",
      "Epoch [95/500], Loss: 0.2316\n",
      "min_ccc: 0.00017442519728551016\n",
      "Epoch [96/500], Loss: 0.2297\n",
      "min_ccc: 0.00021368636524323384\n",
      "Epoch [97/500], Loss: 0.2291\n",
      "min_ccc: 0.00025192443768310227\n",
      "Epoch [98/500], Loss: 0.2296\n",
      "min_ccc: 0.0002885337549583148\n",
      "Epoch [99/500], Loss: 0.2308\n",
      "min_ccc: 0.00032304095713788937\n",
      "Epoch [100/500], Loss: 0.2325\n",
      "min_ccc: 0.00035509294030673324\n",
      "Epoch [101/500], Loss: 0.2346\n",
      "min_ccc: 0.0003844540104171955\n",
      "Epoch [102/500], Loss: 0.2367\n",
      "min_ccc: 0.0004109727781194082\n",
      "Epoch [103/500], Loss: 0.2388\n",
      "min_ccc: 0.0004345915229361912\n",
      "Epoch [104/500], Loss: 0.2407\n",
      "min_ccc: 0.00045530979032911155\n",
      "Epoch [105/500], Loss: 0.2424\n",
      "min_ccc: 0.00047318016845638325\n",
      "Epoch [106/500], Loss: 0.2438\n",
      "min_ccc: 0.0004883009834318363\n",
      "Epoch [107/500], Loss: 0.2448\n",
      "min_ccc: 0.0005008101355602642\n",
      "Epoch [108/500], Loss: 0.2456\n",
      "min_ccc: 0.0005108648995355247\n",
      "Epoch [109/500], Loss: 0.2460\n",
      "min_ccc: 0.0005186430793893181\n",
      "Epoch [110/500], Loss: 0.2460\n",
      "min_ccc: 0.0005243304274331249\n",
      "Epoch [111/500], Loss: 0.2458\n",
      "min_ccc: 0.0005281335887207111\n",
      "Epoch [112/500], Loss: 0.2454\n",
      "min_ccc: 0.0005302473649484793\n",
      "Epoch [113/500], Loss: 0.2447\n",
      "min_ccc: 0.0005308732105426557\n",
      "Epoch [114/500], Loss: 0.2438\n",
      "Epoch [115/500], Loss: 0.2428\n",
      "Epoch [116/500], Loss: 0.2417\n",
      "Epoch [117/500], Loss: 0.2406\n",
      "Epoch [118/500], Loss: 0.2394\n",
      "Epoch [119/500], Loss: 0.2382\n",
      "Epoch [120/500], Loss: 0.2371\n",
      "Epoch [121/500], Loss: 0.2360\n",
      "Epoch [122/500], Loss: 0.2349\n",
      "Epoch [123/500], Loss: 0.2340\n",
      "Epoch [124/500], Loss: 0.2331\n",
      "Epoch [125/500], Loss: 0.2324\n",
      "Epoch [126/500], Loss: 0.2317\n",
      "Epoch [127/500], Loss: 0.2311\n",
      "Epoch [128/500], Loss: 0.2306\n",
      "Epoch [129/500], Loss: 0.2302\n",
      "Epoch [130/500], Loss: 0.2299\n",
      "Epoch [131/500], Loss: 0.2296\n",
      "Epoch [132/500], Loss: 0.2294\n",
      "Epoch [133/500], Loss: 0.2293\n",
      "Epoch [134/500], Loss: 0.2292\n",
      "Epoch [135/500], Loss: 0.2291\n",
      "Epoch [136/500], Loss: 0.2291\n",
      "Epoch [137/500], Loss: 0.2291\n",
      "Epoch [138/500], Loss: 0.2291\n",
      "Epoch [139/500], Loss: 0.2291\n",
      "Epoch [140/500], Loss: 0.2292\n",
      "Epoch [141/500], Loss: 0.2292\n",
      "Epoch [142/500], Loss: 0.2292\n",
      "Epoch [143/500], Loss: 0.2293\n",
      "Epoch [144/500], Loss: 0.2293\n",
      "Epoch [145/500], Loss: 0.2293\n",
      "Epoch [146/500], Loss: 0.2294\n",
      "Epoch [147/500], Loss: 0.2294\n",
      "Epoch [148/500], Loss: 0.2294\n",
      "Epoch [149/500], Loss: 0.2294\n",
      "Epoch [150/500], Loss: 0.2294\n",
      "Epoch [151/500], Loss: 0.2294\n",
      "Epoch [152/500], Loss: 0.2294\n",
      "Epoch [153/500], Loss: 0.2293\n",
      "Epoch [154/500], Loss: 0.2293\n",
      "Epoch [155/500], Loss: 0.2293\n",
      "Epoch [156/500], Loss: 0.2293\n",
      "Epoch [157/500], Loss: 0.2292\n",
      "Epoch [158/500], Loss: 0.2292\n",
      "Epoch [159/500], Loss: 0.2292\n",
      "Epoch [160/500], Loss: 0.2292\n",
      "Epoch [161/500], Loss: 0.2292\n",
      "Epoch [162/500], Loss: 0.2291\n",
      "Epoch [163/500], Loss: 0.2291\n",
      "Epoch [164/500], Loss: 0.2291\n",
      "Epoch [165/500], Loss: 0.2291\n",
      "Epoch [166/500], Loss: 0.2291\n",
      "Epoch [167/500], Loss: 0.2291\n",
      "Epoch [168/500], Loss: 0.2291\n",
      "Epoch [169/500], Loss: 0.2291\n",
      "Epoch [170/500], Loss: 0.2291\n",
      "Epoch [171/500], Loss: 0.2291\n",
      "Epoch [172/500], Loss: 0.2291\n",
      "Epoch [173/500], Loss: 0.2291\n",
      "Epoch [174/500], Loss: 0.2291\n",
      "Epoch [175/500], Loss: 0.2291\n",
      "Epoch [176/500], Loss: 0.2291\n",
      "Epoch [177/500], Loss: 0.2291\n",
      "Epoch [178/500], Loss: 0.2291\n",
      "Epoch [179/500], Loss: 0.2291\n",
      "Epoch [180/500], Loss: 0.2291\n",
      "Epoch [181/500], Loss: 0.2291\n",
      "Epoch [182/500], Loss: 0.2291\n",
      "Epoch [183/500], Loss: 0.2291\n",
      "Epoch [184/500], Loss: 0.2291\n",
      "Epoch [185/500], Loss: 0.2291\n",
      "Epoch [186/500], Loss: 0.2291\n",
      "Epoch [187/500], Loss: 0.2291\n",
      "Epoch [188/500], Loss: 0.2291\n",
      "Epoch [189/500], Loss: 0.2291\n",
      "Epoch [190/500], Loss: 0.2291\n",
      "Epoch [191/500], Loss: 0.2291\n",
      "Epoch [192/500], Loss: 0.2291\n",
      "Epoch [193/500], Loss: 0.2291\n",
      "Epoch [194/500], Loss: 0.2291\n",
      "Epoch [195/500], Loss: 0.2291\n",
      "Epoch [196/500], Loss: 0.2291\n",
      "Epoch [197/500], Loss: 0.2291\n",
      "Epoch [198/500], Loss: 0.2291\n",
      "Epoch [199/500], Loss: 0.2291\n",
      "Epoch [200/500], Loss: 0.2291\n",
      "Epoch [201/500], Loss: 0.2291\n",
      "Epoch [202/500], Loss: 0.2291\n",
      "Epoch [203/500], Loss: 0.2291\n",
      "Epoch [204/500], Loss: 0.2291\n",
      "Epoch [205/500], Loss: 0.2291\n",
      "Epoch [206/500], Loss: 0.2291\n",
      "Epoch [207/500], Loss: 0.2291\n",
      "Epoch [208/500], Loss: 0.2291\n",
      "Epoch [209/500], Loss: 0.2291\n",
      "Epoch [210/500], Loss: 0.2291\n",
      "Epoch [211/500], Loss: 0.2291\n",
      "Epoch [212/500], Loss: 0.2291\n",
      "Epoch [213/500], Loss: 0.2291\n",
      "Epoch [214/500], Loss: 0.2291\n",
      "Epoch [215/500], Loss: 0.2291\n",
      "Epoch [216/500], Loss: 0.2291\n",
      "Epoch [217/500], Loss: 0.2291\n",
      "Epoch [218/500], Loss: 0.2291\n",
      "Epoch [219/500], Loss: 0.2291\n",
      "Epoch [220/500], Loss: 0.2291\n",
      "Epoch [221/500], Loss: 0.2291\n",
      "Epoch [222/500], Loss: 0.2291\n",
      "Epoch [223/500], Loss: 0.2291\n",
      "Epoch [224/500], Loss: 0.2291\n",
      "Epoch [225/500], Loss: 0.2291\n",
      "Epoch [226/500], Loss: 0.2291\n",
      "Epoch [227/500], Loss: 0.2291\n",
      "Epoch [228/500], Loss: 0.2291\n",
      "Epoch [229/500], Loss: 0.2291\n",
      "Epoch [230/500], Loss: 0.2291\n",
      "Epoch [231/500], Loss: 0.2291\n",
      "Epoch [232/500], Loss: 0.2291\n",
      "Epoch [233/500], Loss: 0.2291\n",
      "Epoch [234/500], Loss: 0.2291\n",
      "Epoch [235/500], Loss: 0.2291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [236/500], Loss: 0.2291\n",
      "Epoch [237/500], Loss: 0.2291\n",
      "Epoch [238/500], Loss: 0.2291\n",
      "Epoch [239/500], Loss: 0.2291\n",
      "Epoch [240/500], Loss: 0.2291\n",
      "Epoch [241/500], Loss: 0.2291\n",
      "Epoch [242/500], Loss: 0.2291\n",
      "Epoch [243/500], Loss: 0.2291\n",
      "Epoch [244/500], Loss: 0.2291\n",
      "Epoch [245/500], Loss: 0.2291\n",
      "Epoch [246/500], Loss: 0.2291\n",
      "Epoch [247/500], Loss: 0.2291\n",
      "Epoch [248/500], Loss: 0.2291\n",
      "Epoch [249/500], Loss: 0.2291\n",
      "Epoch [250/500], Loss: 0.2291\n",
      "Epoch [251/500], Loss: 0.2291\n",
      "Epoch [252/500], Loss: 0.2291\n",
      "Epoch [253/500], Loss: 0.2291\n",
      "Epoch [254/500], Loss: 0.2291\n",
      "Epoch [255/500], Loss: 0.2291\n",
      "Epoch [256/500], Loss: 0.2291\n",
      "Epoch [257/500], Loss: 0.2291\n",
      "Epoch [258/500], Loss: 0.2291\n",
      "Epoch [259/500], Loss: 0.2291\n",
      "Epoch [260/500], Loss: 0.2291\n",
      "Epoch [261/500], Loss: 0.2291\n",
      "Epoch [262/500], Loss: 0.2291\n",
      "Epoch [263/500], Loss: 0.2291\n",
      "Epoch [264/500], Loss: 0.2291\n",
      "Epoch [265/500], Loss: 0.2291\n",
      "Epoch [266/500], Loss: 0.2291\n",
      "Epoch [267/500], Loss: 0.2291\n",
      "Epoch [268/500], Loss: 0.2291\n",
      "Epoch [269/500], Loss: 0.2291\n",
      "Epoch [270/500], Loss: 0.2291\n",
      "Epoch [271/500], Loss: 0.2291\n",
      "Epoch [272/500], Loss: 0.2291\n",
      "Epoch [273/500], Loss: 0.2291\n",
      "Epoch [274/500], Loss: 0.2291\n",
      "Epoch [275/500], Loss: 0.2291\n",
      "Epoch [276/500], Loss: 0.2291\n",
      "Epoch [277/500], Loss: 0.2291\n",
      "Epoch [278/500], Loss: 0.2291\n",
      "Epoch [279/500], Loss: 0.2291\n",
      "Epoch [280/500], Loss: 0.2291\n",
      "Epoch [281/500], Loss: 0.2291\n",
      "Epoch [282/500], Loss: 0.2291\n",
      "Epoch [283/500], Loss: 0.2291\n",
      "Epoch [284/500], Loss: 0.2291\n",
      "Epoch [285/500], Loss: 0.2291\n",
      "Epoch [286/500], Loss: 0.2291\n",
      "Epoch [287/500], Loss: 0.2291\n",
      "Epoch [288/500], Loss: 0.2291\n",
      "Epoch [289/500], Loss: 0.2291\n",
      "Epoch [290/500], Loss: 0.2291\n",
      "Epoch [291/500], Loss: 0.2291\n",
      "Epoch [292/500], Loss: 0.2291\n",
      "Epoch [293/500], Loss: 0.2291\n",
      "Epoch [294/500], Loss: 0.2291\n",
      "Epoch [295/500], Loss: 0.2291\n",
      "Epoch [296/500], Loss: 0.2291\n",
      "Epoch [297/500], Loss: 0.2291\n",
      "Epoch [298/500], Loss: 0.2291\n",
      "Epoch [299/500], Loss: 0.2291\n",
      "Epoch [300/500], Loss: 0.2291\n",
      "Epoch [301/500], Loss: 0.2291\n",
      "Epoch [302/500], Loss: 0.2291\n",
      "Epoch [303/500], Loss: 0.2291\n",
      "Epoch [304/500], Loss: 0.2291\n",
      "Epoch [305/500], Loss: 0.2291\n",
      "Epoch [306/500], Loss: 0.2291\n",
      "Epoch [307/500], Loss: 0.2291\n",
      "Epoch [308/500], Loss: 0.2291\n",
      "Epoch [309/500], Loss: 0.2291\n",
      "Epoch [310/500], Loss: 0.2291\n",
      "Epoch [311/500], Loss: 0.2291\n",
      "Epoch [312/500], Loss: 0.2291\n",
      "Epoch [313/500], Loss: 0.2291\n",
      "Epoch [314/500], Loss: 0.2291\n",
      "Epoch [315/500], Loss: 0.2291\n",
      "Epoch [316/500], Loss: 0.2291\n",
      "Epoch [317/500], Loss: 0.2291\n",
      "Epoch [318/500], Loss: 0.2291\n",
      "Epoch [319/500], Loss: 0.2291\n",
      "Epoch [320/500], Loss: 0.2291\n",
      "Epoch [321/500], Loss: 0.2291\n",
      "Epoch [322/500], Loss: 0.2291\n",
      "Epoch [323/500], Loss: 0.2291\n",
      "Epoch [324/500], Loss: 0.2291\n",
      "Epoch [325/500], Loss: 0.2291\n",
      "Epoch [326/500], Loss: 0.2291\n",
      "Epoch [327/500], Loss: 0.2291\n",
      "Epoch [328/500], Loss: 0.2291\n",
      "Epoch [329/500], Loss: 0.2291\n",
      "Epoch [330/500], Loss: 0.2291\n",
      "Epoch [331/500], Loss: 0.2291\n",
      "Epoch [332/500], Loss: 0.2291\n",
      "Epoch [333/500], Loss: 0.2291\n",
      "Epoch [334/500], Loss: 0.2291\n",
      "Epoch [335/500], Loss: 0.2291\n",
      "Epoch [336/500], Loss: 0.2291\n",
      "Epoch [337/500], Loss: 0.2291\n",
      "Epoch [338/500], Loss: 0.2291\n",
      "Epoch [339/500], Loss: 0.2291\n",
      "Epoch [340/500], Loss: 0.2291\n",
      "Epoch [341/500], Loss: 0.2291\n",
      "Epoch [342/500], Loss: 0.2291\n",
      "Epoch [343/500], Loss: 0.2291\n",
      "Epoch [344/500], Loss: 0.2291\n",
      "Epoch [345/500], Loss: 0.2291\n",
      "Epoch [346/500], Loss: 0.2291\n",
      "Epoch [347/500], Loss: 0.2291\n",
      "Epoch [348/500], Loss: 0.2291\n",
      "Epoch [349/500], Loss: 0.2291\n",
      "Epoch [350/500], Loss: 0.2291\n",
      "Epoch [351/500], Loss: 0.2291\n",
      "Epoch [352/500], Loss: 0.2291\n",
      "Epoch [353/500], Loss: 0.2291\n",
      "Epoch [354/500], Loss: 0.2291\n",
      "Epoch [355/500], Loss: 0.2291\n",
      "Epoch [356/500], Loss: 0.2291\n",
      "Epoch [357/500], Loss: 0.2291\n",
      "Epoch [358/500], Loss: 0.2291\n",
      "Epoch [359/500], Loss: 0.2291\n",
      "Epoch [360/500], Loss: 0.2291\n",
      "Epoch [361/500], Loss: 0.2291\n",
      "Epoch [362/500], Loss: 0.2291\n",
      "Epoch [363/500], Loss: 0.2291\n",
      "Epoch [364/500], Loss: 0.2291\n",
      "Epoch [365/500], Loss: 0.2291\n",
      "Epoch [366/500], Loss: 0.2291\n",
      "Epoch [367/500], Loss: 0.2291\n",
      "Epoch [368/500], Loss: 0.2291\n",
      "Epoch [369/500], Loss: 0.2291\n",
      "Epoch [370/500], Loss: 0.2291\n",
      "Epoch [371/500], Loss: 0.2291\n",
      "Epoch [372/500], Loss: 0.2291\n",
      "Epoch [373/500], Loss: 0.2291\n",
      "Epoch [374/500], Loss: 0.2291\n",
      "Epoch [375/500], Loss: 0.2291\n",
      "Epoch [376/500], Loss: 0.2291\n",
      "Epoch [377/500], Loss: 0.2291\n",
      "Epoch [378/500], Loss: 0.2291\n",
      "Epoch [379/500], Loss: 0.2291\n",
      "Epoch [380/500], Loss: 0.2291\n",
      "Epoch [381/500], Loss: 0.2291\n",
      "Epoch [382/500], Loss: 0.2291\n",
      "Epoch [383/500], Loss: 0.2291\n",
      "Epoch [384/500], Loss: 0.2291\n",
      "Epoch [385/500], Loss: 0.2291\n",
      "Epoch [386/500], Loss: 0.2291\n",
      "Epoch [387/500], Loss: 0.2291\n",
      "Epoch [388/500], Loss: 0.2291\n",
      "Epoch [389/500], Loss: 0.2291\n",
      "Epoch [390/500], Loss: 0.2291\n",
      "Epoch [391/500], Loss: 0.2291\n",
      "Epoch [392/500], Loss: 0.2291\n",
      "Epoch [393/500], Loss: 0.2291\n",
      "Epoch [394/500], Loss: 0.2291\n",
      "Epoch [395/500], Loss: 0.2291\n",
      "Epoch [396/500], Loss: 0.2291\n",
      "Epoch [397/500], Loss: 0.2291\n",
      "Epoch [398/500], Loss: 0.2291\n",
      "Epoch [399/500], Loss: 0.2291\n",
      "Epoch [400/500], Loss: 0.2291\n",
      "Epoch [401/500], Loss: 0.2291\n",
      "Epoch [402/500], Loss: 0.2291\n",
      "Epoch [403/500], Loss: 0.2291\n",
      "Epoch [404/500], Loss: 0.2291\n",
      "Epoch [405/500], Loss: 0.2291\n",
      "Epoch [406/500], Loss: 0.2291\n",
      "Epoch [407/500], Loss: 0.2291\n",
      "Epoch [408/500], Loss: 0.2291\n",
      "Epoch [409/500], Loss: 0.2291\n",
      "Epoch [410/500], Loss: 0.2291\n",
      "Epoch [411/500], Loss: 0.2291\n",
      "Epoch [412/500], Loss: 0.2291\n",
      "Epoch [413/500], Loss: 0.2291\n",
      "Epoch [414/500], Loss: 0.2291\n",
      "Epoch [415/500], Loss: 0.2291\n",
      "Epoch [416/500], Loss: 0.2291\n",
      "Epoch [417/500], Loss: 0.2291\n",
      "Epoch [418/500], Loss: 0.2291\n",
      "Epoch [419/500], Loss: 0.2291\n",
      "Epoch [420/500], Loss: 0.2291\n",
      "Epoch [421/500], Loss: 0.2291\n",
      "Epoch [422/500], Loss: 0.2291\n",
      "Epoch [423/500], Loss: 0.2291\n",
      "Epoch [424/500], Loss: 0.2291\n",
      "Epoch [425/500], Loss: 0.2291\n",
      "Epoch [426/500], Loss: 0.2291\n",
      "Epoch [427/500], Loss: 0.2291\n",
      "Epoch [428/500], Loss: 0.2291\n",
      "Epoch [429/500], Loss: 0.2291\n",
      "Epoch [430/500], Loss: 0.2291\n",
      "Epoch [431/500], Loss: 0.2291\n",
      "Epoch [432/500], Loss: 0.2291\n",
      "Epoch [433/500], Loss: 0.2291\n",
      "Epoch [434/500], Loss: 0.2291\n",
      "Epoch [435/500], Loss: 0.2291\n",
      "Epoch [436/500], Loss: 0.2291\n",
      "Epoch [437/500], Loss: 0.2291\n",
      "Epoch [438/500], Loss: 0.2291\n",
      "Epoch [439/500], Loss: 0.2291\n",
      "Epoch [440/500], Loss: 0.2291\n",
      "Epoch [441/500], Loss: 0.2291\n",
      "Epoch [442/500], Loss: 0.2291\n",
      "Epoch [443/500], Loss: 0.2291\n",
      "Epoch [444/500], Loss: 0.2291\n",
      "Epoch [445/500], Loss: 0.2291\n",
      "Epoch [446/500], Loss: 0.2291\n",
      "Epoch [447/500], Loss: 0.2291\n",
      "Epoch [448/500], Loss: 0.2291\n",
      "Epoch [449/500], Loss: 0.2291\n",
      "Epoch [450/500], Loss: 0.2291\n",
      "Epoch [451/500], Loss: 0.2291\n",
      "Epoch [452/500], Loss: 0.2291\n",
      "Epoch [453/500], Loss: 0.2291\n",
      "Epoch [454/500], Loss: 0.2291\n",
      "Epoch [455/500], Loss: 0.2291\n",
      "Epoch [456/500], Loss: 0.2291\n",
      "Epoch [457/500], Loss: 0.2291\n",
      "Epoch [458/500], Loss: 0.2291\n",
      "Epoch [459/500], Loss: 0.2291\n",
      "Epoch [460/500], Loss: 0.2291\n",
      "Epoch [461/500], Loss: 0.2291\n",
      "Epoch [462/500], Loss: 0.2291\n",
      "Epoch [463/500], Loss: 0.2291\n",
      "Epoch [464/500], Loss: 0.2291\n",
      "Epoch [465/500], Loss: 0.2291\n",
      "Epoch [466/500], Loss: 0.2291\n",
      "Epoch [467/500], Loss: 0.2291\n",
      "Epoch [468/500], Loss: 0.2291\n",
      "Epoch [469/500], Loss: 0.2291\n",
      "Epoch [470/500], Loss: 0.2291\n",
      "Epoch [471/500], Loss: 0.2291\n",
      "Epoch [472/500], Loss: 0.2291\n",
      "Epoch [473/500], Loss: 0.2291\n",
      "Epoch [474/500], Loss: 0.2291\n",
      "Epoch [475/500], Loss: 0.2291\n",
      "Epoch [476/500], Loss: 0.2291\n",
      "Epoch [477/500], Loss: 0.2291\n",
      "Epoch [478/500], Loss: 0.2291\n",
      "Epoch [479/500], Loss: 0.2291\n",
      "Epoch [480/500], Loss: 0.2291\n",
      "Epoch [481/500], Loss: 0.2291\n",
      "Epoch [482/500], Loss: 0.2291\n",
      "Epoch [483/500], Loss: 0.2291\n",
      "Epoch [484/500], Loss: 0.2291\n",
      "Epoch [485/500], Loss: 0.2291\n",
      "Epoch [486/500], Loss: 0.2291\n",
      "Epoch [487/500], Loss: 0.2291\n",
      "Epoch [488/500], Loss: 0.2291\n",
      "Epoch [489/500], Loss: 0.2291\n",
      "Epoch [490/500], Loss: 0.2291\n",
      "Epoch [491/500], Loss: 0.2291\n",
      "Epoch [492/500], Loss: 0.2291\n",
      "Epoch [493/500], Loss: 0.2291\n",
      "Epoch [494/500], Loss: 0.2291\n",
      "Epoch [495/500], Loss: 0.2291\n",
      "Epoch [496/500], Loss: 0.2291\n",
      "Epoch [497/500], Loss: 0.2291\n",
      "Epoch [498/500], Loss: 0.2291\n",
      "Epoch [499/500], Loss: 0.2291\n",
      "Epoch [500/500], Loss: 0.2291\n"
     ]
    }
   ],
   "source": [
    "### 정규화 \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "min_ccc = 0\n",
    "for epoch in tqdm(range(num_epochs), desc='Training', unit='epoch'):\n",
    "    model.train()\n",
    "    outputs,_,_ = model(norm_train_x.unsqueeze(1))\n",
    "    loss = criterion(outputs, train_y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    tqdm.write('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predict, test_hidden, test_out = model(test_x.unsqueeze(1))\n",
    "        ccc = compute_ccc(test_y.numpy(),test_predict.squeeze(1).numpy())\n",
    "        if ccc > min_ccc :\n",
    "                min_ccc = ccc\n",
    "                print(f'min_ccc: {min_ccc}') \n",
    "                torch.save(model.state_dict(), f'./model/lstm_valence_model_best.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "valence_model.load_state_dict(torch.load('./model/lstm_valence_model_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2312\n",
      "Test Loss: 0.2312\n",
      "ccc: 0.0016\n",
      "Extracting 32 sized features: tensor([[-0.4746, -0.5534,  0.6373,  ...,  0.6132,  0.6742, -0.4740],\n",
      "        [-0.4790, -0.5580,  0.6417,  ...,  0.6175,  0.6781, -0.4790],\n",
      "        [-0.4784, -0.5574,  0.6411,  ...,  0.6169,  0.6775, -0.4783],\n",
      "        ...,\n",
      "        [-0.4772, -0.5561,  0.6399,  ...,  0.6157,  0.6765, -0.4769],\n",
      "        [-0.4782, -0.5572,  0.6409,  ...,  0.6167,  0.6774, -0.4781],\n",
      "        [-0.4797, -0.5587,  0.6423,  ...,  0.6182,  0.6787, -0.4798]])\n",
      "shape: torch.Size([10769, 32])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_predict,_,_ = model(norm_train_x.unsqueeze(1))\n",
    "    test_predict, test_hidden, test_out  = model(norm_test_x.unsqueeze(1))\n",
    "    train_loss = criterion(train_predict, train_y)\n",
    "    test_loss = criterion(test_predict, test_y)\n",
    "    ccc = compute_ccc(test_y.numpy(),test_predict.squeeze(1).numpy())\n",
    "\n",
    "#     val_predictions = test_predict.detach().numpy().flatten()\n",
    "#     val_targets = test_y.detach().numpy().flatten()\n",
    "#     ccc = compute_ccc(val_targets, val_predictions)\n",
    "    print('Train Loss: {:.4f}'.format(train_loss.item()))\n",
    "    print('Test Loss: {:.4f}'.format(test_loss.item()))\n",
    "    print('ccc: {:.4f}'.format(ccc))\n",
    "    print(f'Extracting 32 sized features: {test_out}') # 32 sized feature 뽑아냄\n",
    "    print(f'shape: {test_out.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# filename = './model/lstm_valence_model.pkl'\n",
    "# pickle.dump(model, open(filename,'wb'))\n",
    "\n",
    "# Save model\n",
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), './model/lstm_valence_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
