{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import optim\n",
    "# from torchmetrics import F1\n",
    "from transformers import ViTModel\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViTImageProcessor\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "# from torchmetrics import F1\n",
    "from transformers import ViTModel\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, video_dir, directory, video_features_file, max_seq_len=250):\n",
    "        self.directory = directory\n",
    "        self.video_names = self.get_video_names(video_dir)\n",
    "        self.video_features_file = video_features_file\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Because we can't use DDP with IterableDataset,\n",
    "        # data must be pre-chunked to combat OOM.\n",
    "        self.label_files = self.prefetch_label_files()\n",
    "        self.data_size, self.index_to_chunk, self.labels = self.prefetch_and_index()\n",
    "    \n",
    "    def get_video_names(self, mp4_dir):\n",
    "        # mp4 파일이 있는 디렉토리 경로\n",
    "        # mp4_dir = \"/workspace/EmotionShortForm/aihub/2.Validation/Video_data/VS_유튜브_04\"\n",
    "\n",
    "        video_names = []\n",
    "        for filename in os.listdir(mp4_dir):\n",
    "            if filename.endswith('.mp4'):\n",
    "                name = os.path.splitext(filename)[0]\n",
    "                video_names.append(name)\n",
    "        return video_names\n",
    "    \n",
    "    def prefetch_label_files(self):\n",
    "\n",
    "        name_set = set(self.video_names)\n",
    "\n",
    "        label_files = defaultdict(list)\n",
    "\n",
    "        for label_file in Path(self.directory).glob(f\"**/*.json\"):\n",
    "\n",
    "            file_name = label_file.stem\n",
    "\n",
    "            # 예시: [KBS]kim370_대법원 업무 과부하…상고 법원이 대안_18567498.json\n",
    "            # annotator id 제거하면 비디오 이름 추출.\n",
    "            # 파일 이름 reverse ([::-1]) 후 \"_\" 찾음.\n",
    "            annotator_id_index = len(file_name) - file_name[::-1].find(\"_\") - 1\n",
    "            video_name = file_name[:annotator_id_index]\n",
    "\n",
    "            if video_name in name_set:\n",
    "                label_files[video_name].append(label_file)\n",
    "        \n",
    "        \n",
    "        return label_files\n",
    "\n",
    "    def prefetch_and_index(self):\n",
    "        index = 0\n",
    "        index_to_chunk = {}\n",
    "        all_labels = {}\n",
    "\n",
    "        for video_name in self.video_names:\n",
    "\n",
    "#            if video_name == m\"news_footage_1710\":\n",
    "#                continue\n",
    "\n",
    "            labels = self.extract_label(video_name)\n",
    "#             print(len(labels)) # 3\n",
    "            all_labels[video_name] = labels\n",
    "        \n",
    "            chunk_count = math.ceil(len(labels[0]) / self.max_seq_len)\n",
    "            for chunk_index in range(0, chunk_count):\n",
    "                index_to_chunk[index + chunk_index] = (video_name, chunk_index)\n",
    "\n",
    "            index += chunk_count\n",
    "\n",
    "        return index, index_to_chunk, all_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        video_name, chunk_index = self.index_to_chunk[index]\n",
    "        start = chunk_index * self.max_seq_len \n",
    "\n",
    "        end = start + self.max_seq_len\n",
    "#         print(f'start: {start}, end: {end}')\n",
    "        with h5py.File(self.video_features_file, \"r\") as rf:\n",
    "\n",
    "            labels = self.labels[video_name][:, start:end]\n",
    "             # Convert labels to 1D array\n",
    "            \n",
    "#             video_features = rf[video_name][:][: len(labels[0])][start:end]\n",
    "            \n",
    "            video_features = rf[video_name][start:end]\n",
    "#             print(f'{len(video_features)}, {len(video_features[0])}')\n",
    "#             print(f'video_name: {video_name}, label len: {len(labels[0])}, video_features_len: {len(video_features)}')\n",
    "            labels = torch.from_numpy(labels)\n",
    "            # majority voting\n",
    "            labels = labels.squeeze(0)\n",
    "            labels = torch.sum(labels, dim=0) \n",
    "            labels = torch.min(\n",
    "                labels,\n",
    "                torch.ones(\n",
    "                    labels.shape[0],\n",
    "                ).to(labels.device),\n",
    "            )\n",
    "            return video_name, video_features, labels\n",
    "\n",
    "    def extract_label(self, video_name):\n",
    "\n",
    "        label_files = self.label_files[video_name]\n",
    "        labels = []\n",
    "\n",
    "        for label_file in label_files:\n",
    "\n",
    "            with open(label_file, \"r\") as rf:\n",
    "                data = json.load(rf)\n",
    "\n",
    "            metadata = data[\"metadata\"]\n",
    "            video_length = math.ceil(metadata[\"length\"])\n",
    "            annotator_label = np.zeros((video_length,))\n",
    "\n",
    "            for timeline in data[\"timelines\"]:\n",
    "                for time_index in range(timeline[\"start\"], timeline[\"end\"] + 1):\n",
    "                    # annotator_label[time_index] += 1\n",
    "                    if time_index < video_length:\n",
    "                        annotator_label[time_index] = 1\n",
    "\n",
    "            labels.append(annotator_label)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp4 파일이 있는 디렉토리 경로\n",
    "mp4_dir = \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Video_data/VS_유튜브_01\"\n",
    "# video_name = '유튜브_기타_21516'\n",
    "# video_path = f'{mp4_dir}/{video_name}.mp4'\n",
    "args = argparse.Namespace(\n",
    "    videos = \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Video_data/VS_유튜브_01\",\n",
    "    data_directory =  \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Labeling_data/VL_youtube\",\n",
    "    video_features_file = \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Video_data/VS_유튜브_01.h5\"\n",
    ")\n",
    "\n",
    "val_label_path = \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Labeling_data/VL_youtube\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryModel(LightningModule):\n",
    "    def __init__(self, hidden_dim=768, individual_logs=None):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.scorer = nn.Linear(hidden_dim, 1)\n",
    "        self.feature_extractor = nn.Identity() # Feature extraction layer\n",
    "      \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        # self.train_f1 = F1()\n",
    "        # self.val_f1 = F1()\n",
    "        # self.test_f1 = F1()\n",
    "        self.individual_logs = individual_logs\n",
    "        self.tta_logs = defaultdict(list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x).pooler_output\n",
    "        score = self.scorer(x)\n",
    "        feature = self.feature_extractor(x) # extract features\n",
    "        # x = self.sigmoid(x)\n",
    "        return score, feature\n",
    "\n",
    "    def run_batch(self, batch, batch_idx, metric, training=False):\n",
    "        video_name, image_features, labels = batch\n",
    "        video_name = video_name[0]\n",
    "        image_features = image_features.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "\n",
    "        # Score - aggregated labels.\n",
    "        score = torch.sum(labels, dim=0)\n",
    "        score = torch.min(\n",
    "            score,\n",
    "            torch.ones(\n",
    "                score.shape[0],\n",
    "            ).to(score.device),\n",
    "        )\n",
    "        out = self(image_features).squeeze(1)\n",
    "        try:\n",
    "            loss = self.loss(out.double(), score)\n",
    "            preds = (torch.sigmoid(out) > 0.7).int()\n",
    "            metric.update(preds, score.int())\n",
    "            f1 = metric.compute()\n",
    "            tp, fp, tn, fn = metric._get_final_stats()\n",
    "            self.tta_logs[video_name].append((tp.item(), fp.item(), fn.item()))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            loss = 0\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.run_batch(batch, batch_idx, self.train_f1, training=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        self.log(\"train_f1\", self.train_f1.compute())\n",
    "        self.train_f1.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.run_batch(batch, batch_idx, self.val_f1)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        self.log(\"val_f1\", self.val_f1.compute())\n",
    "        self.val_f1.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.run_batch(batch, batch_idx, self.test_f1)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        f1 = self.test_f1.compute()\n",
    "        self.log(\"test_f1\", f1)\n",
    "        tp, fp, tn, fn = self.test_f1._get_final_stats()\n",
    "        print(f\"\\nTest f1: {f1}, TP: {tp}, FP: {fp}, TN: {tn}, fn: {fn}\")\n",
    "        self.test_f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-a\", \"--argument\", help=\"Example argument\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "preprocessor = ViTImageProcessor.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\", size=224, device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SummaryModel()\n",
    "model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../vit/summary.ckpt'\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "d = SummaryDataset(args.videos, args.data_directory, args.video_features_file)\n",
    "dl = DataLoader(d, batch_size=1)\n",
    "\n",
    "vit_features = []\n",
    "y_pred=[]\n",
    "\n",
    "for video_name, inputs, labels in tqdm_notebook(dl,total=len(dl), desc='Processing dataset' ):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_pred_list = []\n",
    "        vit_features_list=[]\n",
    "        print(f'video_name: {video_name}, inputs.shape: {inputs.shape}, labels.shape: {labels.shape}')\n",
    "        for f in tqdm(inputs.squeeze(0)):\n",
    "#             print(f.unsqueeze(0).shape)\n",
    "        #     print(frame.unsqueeze(0).shape)\n",
    "            y_p, y_f = model(f.cuda().unsqueeze(0))\n",
    "#            print(y_f.shape)\n",
    "#             print(y_f.cpu().detach().numpy().shape)\n",
    "            y_p = torch.sigmoid(y_p)\n",
    "            y_pred_list.append(y_p.cpu().detach().numpy().squeeze())    \n",
    "            vit_features_list.append(y_f.cpu().detach().numpy().squeeze())        \n",
    "        y_pred.append(np.array(y_pred_list))    \n",
    "        vit_features.append(np.array(vit_features_list)) \n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_features = [torch.tensor(a) for a in vit_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import librosa\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, video_dir, wav_dir, directory, max_seq_len=250):\n",
    "        self.directory = directory\n",
    "        self.video_names = self.get_video_names(video_dir)\n",
    "        self.wav_dir = wav_dir\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Because we can't use DDP with IterableDataset,\n",
    "        # data must be pre-chunked to combat OOM.\n",
    "        self.label_files = self.prefetch_label_files()\n",
    "        self.data_size, self.index_to_chunk, self.labels = self.prefetch_and_index()\n",
    "\n",
    "    def get_video_names(self, mp4_dir):\n",
    "        # mp4 파일이 있는 디렉토리 경로\n",
    "        # mp4_dir = \"/workspace/EmotionShortForm/aihub/2.Validation/Video_data/VS_유튜브_04\"\n",
    "\n",
    "        video_names = []\n",
    "        for filename in os.listdir(mp4_dir):\n",
    "            if filename.endswith('.mp4'):\n",
    "                name = os.path.splitext(filename)[0]\n",
    "                video_names.append(name)\n",
    "        return video_names\n",
    "\n",
    "    def prefetch_label_files(self):\n",
    "        # video_names 는 이름만 들어있을것 .mp4 제거\n",
    "        name_set = set(self.video_names)\n",
    "\n",
    "        label_files = defaultdict(list)\n",
    "\n",
    "        for label_file in Path(self.directory).glob(f\"**/*.json\"):\n",
    "\n",
    "            file_name = label_file.stem\n",
    "\n",
    "            # 예시: [KBS]kim370_대법원 업무 과부하…상고 법원이 대안_18567498.json\n",
    "            # annotator id 제거하면 비디오 이름 추출.\n",
    "            # 파일 이름 reverse ([::-1]) 후 \"_\" 찾음.\n",
    "            annotator_id_index = len(file_name) - file_name[::-1].find(\"_\") - 1\n",
    "            video_name = file_name[:annotator_id_index]\n",
    "\n",
    "            if video_name in name_set:\n",
    "                label_files[video_name].append(label_file)\n",
    "\n",
    "        return label_files\n",
    "\n",
    "    def prefetch_and_index(self):\n",
    "        index = 0\n",
    "        index_to_chunk = {}\n",
    "        all_labels = {}\n",
    "\n",
    "        for video_name in self.video_names:\n",
    "\n",
    "            labels = self.extract_label(video_name)\n",
    "\n",
    "            all_labels[video_name] = labels\n",
    "\n",
    "            chunk_count = math.ceil(len(labels[0]) / self.max_seq_len)\n",
    "            for chunk_index in range(0, chunk_count):\n",
    "                index_to_chunk[index + chunk_index] = (video_name, chunk_index)\n",
    "\n",
    "            index += chunk_count\n",
    "\n",
    "        return index, index_to_chunk, all_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        video_name, chunk_index = self.index_to_chunk[index]\n",
    "        start = chunk_index * self.max_seq_len\n",
    "        end = start + self.max_seq_len\n",
    "        \n",
    "        labels = self.labels[video_name][:,start:end]\n",
    "\n",
    "        # audio_data: 음성 데이터, sr: sampling rate, max_seq_len: chunk 단위 길이\n",
    "        audio_data, sr = librosa.load(f\"{self.wav_dir}/{video_name}.wav\", sr=None)\n",
    "        # print(f'audio_data 길이: {len(audio_data)/sr}')\n",
    "        # print(f'sr: {sr}')\n",
    "        \n",
    "        audio_data = audio_data[start*sr:end*sr]\n",
    "        # print(f'audio_data 길이: {len(audio_data)/sr}')\n",
    "        \n",
    "        max_seq_len = labels.shape[-1]\n",
    "\n",
    "        \n",
    "        # 1초 단위로 MFCC 추출하여 리스트에 추가\n",
    "        sec = 1\n",
    "        mfcc_list = []\n",
    "        for i in range(0, len(audio_data), sec*sr):\n",
    "            audio_segment = audio_data[i : i + sec*sr]\n",
    "                \n",
    "            mfcc = librosa.feature.mfcc(y=audio_segment, sr=sr, n_mfcc=32).T\n",
    "            mfcc_mean = np.mean(mfcc, axis=0)\n",
    "            mfcc_list.append(mfcc_mean)\n",
    "\n",
    "        # 리스트를 배열로 변환\n",
    "        mfcc_array = np.vstack(mfcc_list)\n",
    "        \n",
    "        # Convert labels to 1D array\n",
    "        labels = torch.from_numpy(labels)\n",
    "        # majority voting\n",
    "        labels = labels.squeeze(0)\n",
    "        labels = torch.sum(labels, dim=0) \n",
    "        labels = torch.min(\n",
    "            labels,\n",
    "            torch.ones(\n",
    "                labels.shape[0],\n",
    "            ).to(labels.device),\n",
    "        )\n",
    "        return video_name, mfcc_array, labels\n",
    "\n",
    "    def extract_label(self, video_name):\n",
    "\n",
    "        label_files = self.label_files[video_name]\n",
    "        labels = []\n",
    "\n",
    "        for label_file in label_files:\n",
    "\n",
    "            with open(label_file, \"r\") as rf:\n",
    "                data = json.load(rf)\n",
    "\n",
    "            metadata = data[\"metadata\"]\n",
    "            video_length = math.ceil(metadata[\"length\"])\n",
    "            annotator_label = np.zeros((video_length,))\n",
    "\n",
    "            for timeline in data[\"timelines\"]:\n",
    "                for time_index in range(timeline[\"start\"], timeline[\"end\"] + 1):\n",
    "                    # annotator_label[time_index] += 1\n",
    "                    if time_index < video_length:\n",
    "                        annotator_label[time_index] = 1\n",
    "\n",
    "            labels.append(annotator_label)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링 디렉토리 경로\n",
    "train_label_path = \"/workspace/EmotionShortForm/data_AIHub/1.Training/Labeling_data/TL_youtube\"\n",
    "val_label_path = \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Labeling_data/VL_youtube\"\n",
    "\n",
    "# mp4 파일이 있는 디렉토리 경로\n",
    "mp4_dir = \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Video_data/VS_유튜브_01\"\n",
    "\n",
    "# wav 파일이 있는 디렉토리 경로\n",
    "wav_dir = \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Audio_data/VS_유튜브_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SummaryDataset(mp4_dir, wav_dir, val_label_path)\n",
    "\n",
    "dl = DataLoader(sd,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, (hidden_state, cell_state) = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(out[:,-1,:])\n",
    "        hidden = self.fc(hidden_state[-1])\n",
    "        return output, hidden, out[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "input_size = 32\n",
    "hidden_size = 128\n",
    "num_layers=3\n",
    "output_size=7\n",
    "\n",
    "emotion_model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "emotion_model.load_state_dict(torch.load('../model/lstm_emotion_classification_model.pt'))\n",
    "\n",
    "input_size = 32\n",
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "output_size = 1\n",
    "\n",
    "arousal_model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "arousal_model.load_state_dict(torch.load('../model/lstm_arousal_model_best.pt'))\n",
    "\n",
    "input_size = 32\n",
    "hidden_size = 32\n",
    "num_layers=4\n",
    "output_size=1\n",
    "\n",
    "valence_model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "valence_model.load_state_dict(torch.load('../model/lstm_valence_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "emotion_lstm_features = []\n",
    "label = []\n",
    "for video_name, inputs, labels in tqdm_notebook(dl,total=len(dl), desc='Extracting emotion features'):\n",
    "    label.append(labels)\n",
    "    with torch.no_grad():\n",
    "        emotion_model.eval()\n",
    "        outputs, hidden, out = emotion_model(inputs.reshape(-1,1,32))\n",
    "        emotion_lstm_features.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "arousal_lstm_features = []\n",
    "\n",
    "for video_name, inputs, labels in tqdm_notebook(dl,total=len(dl), desc='Extracting arousal features'):\n",
    "    with torch.no_grad():\n",
    "        arousal_model.eval()\n",
    "        outputs, hidden, out = arousal_model(inputs.reshape(-1,1,32))\n",
    "        arousal_lstm_features.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "valence_lstm_features = []\n",
    "\n",
    "for video_name, inputs, labels in tqdm_notebook(dl,total=len(dl), desc='Extracting valence features'):\n",
    "    with torch.no_grad():\n",
    "        valence_model.eval()\n",
    "        outputs, hidden, out = valence_model(inputs.reshape(-1,1,32))\n",
    "        valence_lstm_features.append(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_lstm_features = []\n",
    "for e, a, v in zip(emotion_lstm_features, arousal_lstm_features, valence_lstm_features):\n",
    "    concatenate_lstm_features.append(torch.cat((e,a,v),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concatenate_lstm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_lstm_features[0].shape # emotion + arousal + valence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio LSTMClassifier, multimodal LSTMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        output = self.fc(out[:, -1, :])\n",
    "        \n",
    "        # Apply sigmoid activation function to output\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output, out[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "output_size = 1\n",
    "audio_concat_model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "audio_concat_model.load_state_dict(torch.load('concatenate_lstm_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_concat_features = []\n",
    "\n",
    "for inputs in tqdm_notebook(concatenate_lstm_features,total=len(concatenate_lstm_features), desc='Extracting audio features'):\n",
    "    with torch.no_grad():\n",
    "        audio_concat_model.eval()\n",
    "        outputs, out = audio_concat_model(inputs.reshape(-1,1,input_size))\n",
    "        audio_concat_features.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(audio_concat_features), audio_concat_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature concatenate w/o lstm\n",
    "+ audio lstm 없이! -> model input size를 바꿔줘야 해서 나중에 비교!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_concat_lstm_feature  =  torch.cat((concatenate_lstm_feature, video_features_torch), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_concat_lstm_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_concat_lstm_features = []\n",
    "for v, a in zip(vit_features, audio_concat_features):\n",
    "    final_concat_lstm_features.append(torch.cat((v,a),dim=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_concat_lstm_features), final_concat_lstm_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 896\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "output_size = 1\n",
    "multimodal_model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "multimodal_model.load_state_dict(torch.load('multimodal_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in multimodal_model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f'Layer: {name} | Size: {param.size()} | Values: {param}')\n",
    "    if 'bias' in name:\n",
    "        print(f'Layer: {name} | Size: {param.size()} | Values: {param}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "for inputs in tqdm_notebook(final_concat_lstm_features,total=len(final_concat_lstm_features), desc='Evaluation'):\n",
    "    with torch.no_grad():\n",
    "        multimodal_model.eval()\n",
    "        outputs, out = multimodal_model(inputs.reshape(-1,1,input_size))\n",
    "        y_pred.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred),y_pred[0].shape,y_pred[0].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label), label[0].shape, label[0].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_flat = [y.item() for x in y_pred for y in x]\n",
    "\n",
    "label = [t.squeeze().tolist() for t in label]\n",
    "label = sum(label, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred_flat), len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred_flat)\n",
    "y_true = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Threshold 하나 일때만\n",
    "THRES = 0.4\n",
    "\n",
    "# calculate scores\n",
    "f1 = f1_score(y_true, np.where(y_pred > THRES, 1, 0), average='binary')\n",
    "acc = accuracy_score(y_true, np.where(y_pred > THRES, 1, 0))\n",
    "prec = precision_score(y_true, np.where(y_pred > THRES, 1, 0))\n",
    "rec = recall_score(y_true, np.where(y_pred > THRES, 1, 0))\n",
    "\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# initialize lists for storing results\n",
    "f1_scores = []\n",
    "acc_scores = []\n",
    "prec_scores = []\n",
    "rec_scores = []\n",
    "\n",
    "# vary threshold and calculate scores\n",
    "for THRES in np.arange(0, 1.05, 0.05):\n",
    "    # calculate scores\n",
    "    f1 = f1_score(y_true, np.where(y_pred > THRES, 1, 0), average='binary')\n",
    "    acc = accuracy_score(y_true, np.where(y_pred > THRES, 1, 0))\n",
    "    prec = precision_score(y_true, np.where(y_pred > THRES, 1, 0))\n",
    "    rec = recall_score(y_true, np.where(y_pred > THRES, 1, 0))\n",
    "    \n",
    "    # store scores in lists\n",
    "    f1_scores.append(f1)\n",
    "    acc_scores.append(acc)\n",
    "    prec_scores.append(prec)\n",
    "    rec_scores.append(rec)\n",
    "\n",
    "# plot results\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(np.arange(0, 1.05, 0.05), f1_scores, label='F1 Score')\n",
    "plt.plot(np.arange(0, 1.05, 0.05), acc_scores, label='Accuracy')\n",
    "plt.plot(np.arange(0, 1.05, 0.05), prec_scores, label='Precision')\n",
    "plt.plot(np.arange(0, 1.05, 0.05), rec_scores, label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# calculate fpr and tpr for different thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "\n",
    "# calculate AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(outputs.detach().numpy())\n",
    "plt.title(\"Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(outputs.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "total_secs = 0\n",
    "SAMPLE_EVERY_SEC = 2\n",
    "\n",
    "for i, y_p in enumerate(outputs.detach().numpy()):\n",
    "    #print(i, y_p)\n",
    "    if y_p >= THRESHOLD:\n",
    "        print(i * SAMPLE_EVERY_SEC)\n",
    "        total_secs += SAMPLE_EVERY_SEC\n",
    "\n",
    "total_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/workspace/EmotionShortForm/data_AIHub/2.Validation/Video_data/VS_유튜브_01/유튜브_여행_7640.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = VideoFileClip(video_path)\n",
    "\n",
    "subclips = []\n",
    "\n",
    "for i, y_p in enumerate(outputs.detach().numpy()):\n",
    "    sec = i * SAMPLE_EVERY_SEC\n",
    "\n",
    "    if y_p >= THRESHOLD:\n",
    "        subclip = clip.subclip(sec, sec + SAMPLE_EVERY_SEC)\n",
    "        subclips.append(subclip)\n",
    "\n",
    "result = concatenate_videoclips(subclips)\n",
    "\n",
    "result.write_videofile(\"videos/유튜브_여행_7640_result.mp4\")\n",
    "\n",
    "result.ipython_display(width=640, maxduration=240)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
